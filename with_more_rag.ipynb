{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_1_fast=LitellmModel(model=\"openrouter/x-ai/grok-4.1-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf96427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent2 = Agent(\n",
    "#     name=\"Assistant\",\n",
    "#     model=grok_4_1_fast,\n",
    "#     instructions=\"You are helpful.\",\n",
    "#     model_settings=ModelSettings(\n",
    "#         extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# with trace(\"Telling a joke\"):\n",
    "#     result = await Runner.run(agent2, \"Tell a joke about Autonomous AI Agents\")\n",
    "#     print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added deletion analysis, chain-of-thought, and better semantic understanding\n",
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed: What code was added? What was removed? What was modified?\n",
    "2. Then, identify potential issues in the changes\n",
    "3. Consider the inverse: What functionality might be LOST from deletions?\n",
    "\n",
    "DELETION ANALYSIS (CRITICAL):\n",
    "- When you see removed code (lines starting with -), pay special attention to:\n",
    "  * Entire functions/classes being deleted - flag if they're called elsewhere\n",
    "  * Helper functions removed - check if remaining code still works without them\n",
    "  * Error handling removed - flag if this makes code less safe\n",
    "  * Imports removed - verify they're truly unused\n",
    "- If 10+ consecutive lines are deleted, describe what functionality is being removed\n",
    "\n",
    "BUG PATTERNS TO IDENTIFY:\n",
    "- Logic errors, unhandled edge cases, null/undefined access, type mismatches\n",
    "- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\n",
    "- Infinite loops, missing error handling (no try-except blocks)\n",
    "- Code duplication, overly complex functions\n",
    "- Removed functionality that breaks remaining code\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed from a security perspective\n",
    "2. Identify what security controls or validations were added or removed\n",
    "3. Consider: Does this change introduce new attack surface?\n",
    "\n",
    "SECURITY PATTERNS:\n",
    "- SQL injection, command injection, XSS vulnerabilities\n",
    "- Hardcoded secrets/credentials, insecure authentication\n",
    "- Path traversal, insecure deserialization\n",
    "- Improper input validation\n",
    "- Missing error handling that could expose sensitive information\n",
    "- Removed security checks or validation code\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\n",
    "- Consider what protections are LOST, not just what bugs are added\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Describe what changed in terms of code quality\n",
    "2. Identify violations of best practices in the new/modified code\n",
    "3. Consider: Does this change make the code harder to maintain?\n",
    "\n",
    "CODE QUALITY ISSUES:\n",
    "- Unclear variable names, functions exceeding 50 lines\n",
    "- Nested complexity over 3 levels, missing docstrings\n",
    "- Inconsistent formatting, magic numbers without explanation\n",
    "- Violations of DRY principle\n",
    "- Unclosed resources (files, database cursors, connections)\n",
    "- Missing try-except blocks for error-prone operations\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If helpful comments, docstrings, or error handling are removed, flag it\n",
    "- If code is simplified but loses clarity, mention it\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Identify what functions/methods are new or modified\n",
    "2. For each, list what test scenarios are needed\n",
    "3. Consider edge cases and error conditions\n",
    "\n",
    "For each new or modified function, suggest test cases covering:\n",
    "- Normal input cases\n",
    "- Edge cases (empty, null, boundary values)\n",
    "- Error conditions (exceptions, failures, timeouts)\n",
    "- Integration scenarios\n",
    "\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    # model_settings=ModelSettings(\n",
    "    #     temperature=0.5,\n",
    "        # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "    # ),\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    # model_settings=ModelSettings(\n",
    "    #     temperature=0.5,\n",
    "        # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "    # ),\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    # model_settings=ModelSettings(\n",
    "    #     temperature=0.5,\n",
    "        # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "    # ),\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    # model_settings=ModelSettings(\n",
    "    #     temperature=0.5,\n",
    "        # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "    # ),\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc44cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_python_gotchas(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant Python gotchas patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    python_gotchas_collection = chroma_client.get_collection(name=\"python_gotchas_patterns\")\n",
    "    results = python_gotchas_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_code_review_patterns(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant code review patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    code_review_collection = chroma_client.get_collection(name=\"code_review_patterns\")\n",
    "    results = code_review_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_refactoring_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant refactoring patterns from ChromaDB (multi-file changes, shotgun surgery, etc.)\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    refactoring_collection = chroma_client.get_collection(name=\"refactoring_patterns\")\n",
    "    results = refactoring_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all_agents(diff):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent, diff),\n",
    "        Runner.run(best_practices_agent, diff),\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# async def run_all_agents(diff):\n",
    "#     # Get RAG context for all agents\n",
    "#     security_patterns = get_relevant_security_patterns(diff, n_results=5)\n",
    "#     best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "#     python_gotchas = get_relevant_python_gotchas(diff, n_results=3)\n",
    "#     code_review_patterns = get_relevant_code_review_patterns(diff, n_results=3)\n",
    "#     refactoring_patterns = get_relevant_refactoring_patterns(diff, n_results=5)  # NEW: Multi-file refactoring patterns\n",
    "    \n",
    "#     # Create RAG-enhanced Code Analyzer agent (UPDATED: with all patterns including refactoring)\n",
    "#     enhanced_code_analyzer_instructions = f\"\"\"{code_analyzer_instructions}\n",
    "\n",
    "# RELEVANT PYTHON GOTCHAS TO CHECK:\n",
    "# {python_gotchas}\n",
    "\n",
    "# RELEVANT CODE REVIEW PATTERNS TO CHECK:\n",
    "# {code_review_patterns}\n",
    "\n",
    "# RELEVANT REFACTORING PATTERNS TO CHECK (Multi-File Changes):\n",
    "# {refactoring_patterns}\"\"\"\n",
    "    \n",
    "#     # Create RAG-enhanced security agent\n",
    "#     enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "# RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "# {security_patterns}\"\"\"\n",
    "    \n",
    "#     # Create RAG-enhanced best practices agent\n",
    "#     enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "# RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "# {best_practices_patterns}\"\"\"\n",
    "    \n",
    "#     code_analyzer_rag = Agent(\n",
    "#         name=\"Code Analyzer\",\n",
    "#         instructions=enhanced_code_analyzer_instructions,\n",
    "#         model=\"gpt-4.1-mini\",\n",
    "#         model_settings=ModelSettings(\n",
    "#             temperature=0.6,\n",
    "#             # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "#         ),\n",
    "#         output_type=CodeAnalyzerOutput\n",
    "#     )\n",
    "    \n",
    "#     security_agent_rag = Agent(\n",
    "#         name=\"Security Agent\",\n",
    "#         instructions=enhanced_security_instructions,\n",
    "#         model=\"gpt-4.1-mini\",\n",
    "#         model_settings=ModelSettings(\n",
    "#             temperature=0.6,\n",
    "#             # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "#         ),\n",
    "#         output_type=SecurityOutput\n",
    "#     )\n",
    "    \n",
    "#     best_practices_agent_rag = Agent(\n",
    "#         name=\"Best Practices Agent\",\n",
    "#         instructions=enhanced_best_practices_instructions,\n",
    "#         model=\"gpt-4.1-mini\",\n",
    "#         model_settings=ModelSettings(\n",
    "#             temperature=0.6,\n",
    "#             # extra_args={\"reasoning\": {\"enabled\": False}}\n",
    "#         ),\n",
    "#         output_type=BestPracticesOutput\n",
    "#     )\n",
    "    \n",
    "#     # Run all agents in parallel\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer_rag, diff),  # Now uses RAG with refactoring patterns!\n",
    "#         Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "#         Runner.run(best_practices_agent_rag, diff),  # Uses RAG\n",
    "#         Runner.run(test_coverage_agent, diff)  # No RAG needed for test coverage\n",
    "#     )\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added multi-file awareness and cross-file dependency detection\n",
    "\n",
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "CRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "AGGREGATION GUIDELINES:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. MULTI-FILE AWARENESS (CRITICAL):\n",
    "   - If findings span multiple files, check for cross-file dependencies\n",
    "   - Flag if changes in one file might break APIs/contracts in another file\n",
    "   - Look for patterns like: \"File A removes function X, but does File B call it?\"\n",
    "   - Consider the bigger picture: Do these changes work together?\n",
    "\n",
    "3. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "4. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "5. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_1_fast,\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    with trace(\"Multi-Agent Code Review\"):\n",
    "        results = await run_all_agents(diff)\n",
    "        code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "        \n",
    "        organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALLING AGGREGATOR...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report = await aggregator_agent(organized)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AGGREGATOR OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(report)\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        if save_output:\n",
    "            os.makedirs(\"user-data\", exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {filepath}\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation (Option 4): Models and Utilities\n",
    "import re\n",
    "\n",
    "class HybridEvaluation(BaseModel):\n",
    "    \"\"\"Hybrid evaluation combining location metrics + LLM relevance.\"\"\"\n",
    "    file_recall: float = Field(description=\"Recall at file level (0.0-1.0)\")\n",
    "    line_precision: float = Field(description=\"Precision at line level (0.0-1.0)\")\n",
    "    line_recall: float = Field(description=\"Recall at line level (0.0-1.0)\")\n",
    "    llm_relevance: float = Field(description=\"LLM-judged relevance score (0.0-1.0)\")\n",
    "    composite_score: float = Field(description=\"Combined score: (line_recall + llm_relevance) / 2\")\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ri2gmqdlr5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation Function\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid(report: str, bug_patch: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation: Location metrics (automated) + LLM relevance (semantic).\n",
    "    \n",
    "    Stage 1: Calculate automated location overlap\n",
    "    Stage 2: If file_recall > 0, use LLM to judge semantic relevance\n",
    "    \n",
    "    Returns:\n",
    "        dict with file_recall, line_precision, line_recall, llm_relevance, composite_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(bug_patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance (only if there's file overlap)\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_instructions = \"\"\"You are evaluating the semantic relevance of code review findings to an actual bug fix.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. ACTUAL FIX PATCH: The changes that were made to fix bugs\n",
    "2. CODE REVIEW REPORT: What the review system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the bugs that were fixed\n",
    "- 0.7-0.9: Findings flag related issues that would lead to discovering the bugs\n",
    "- 0.4-0.6: Findings flag the general area but miss specific bugs\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Be objective and strict in your assessment.\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"Relevance Judge\",\n",
    "            instructions=llm_judge_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{bug_patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to the actual fix.\n",
    "\"\"\"\n",
    "        with trace(\"LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e63ligp8p4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hybrid Batch Testing\n",
    "\n",
    "# async def test_bugsinpy_hybrid(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "#     \"\"\"\n",
    "#     Test multiple BugsInPy bugs with hybrid evaluation.\n",
    "    \n",
    "#     Args:\n",
    "#         bugs_to_test: List of (project, bug_id) tuples\n",
    "        \n",
    "#     Returns:\n",
    "#         List of evaluation results with hybrid metrics\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "    \n",
    "#     for project, bug_id in bugs_to_test:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"TESTING: {project} bug {bug_id}\")\n",
    "#         print('='*60)\n",
    "        \n",
    "#         try:\n",
    "#             # Load bug patch\n",
    "#             bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "#             bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "#             # Reverse diff to show bug introduction\n",
    "#             reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "#             # Run code review on buggy code\n",
    "#             report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "#             # Hybrid evaluation\n",
    "#             eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "#             # Store result\n",
    "#             result = {\n",
    "#                 'project': project,\n",
    "#                 'bug_id': bug_id,\n",
    "#                 'file_recall': eval_result['file_recall'],\n",
    "#                 'line_precision': eval_result['line_precision'],\n",
    "#                 'line_recall': eval_result['line_recall'],\n",
    "#                 'llm_relevance': eval_result['llm_relevance'],\n",
    "#                 'composite_score': eval_result['composite_score'],\n",
    "#                 'passed': eval_result['composite_score'] >= 0.60  # 60% threshold for hybrid\n",
    "#             }\n",
    "#             results.append(result)\n",
    "            \n",
    "#             # Print metrics\n",
    "#             print(f\"\\nðŸ“ LOCATION METRICS (Automated):\")\n",
    "#             print(f\"  File Recall: {eval_result['file_recall']:.2%}\")\n",
    "#             print(f\"  Line Precision: {eval_result['line_precision']:.2%}\")\n",
    "#             print(f\"  Line Recall: {eval_result['line_recall']:.2%}\")\n",
    "#             print(f\"\\nðŸ¤– LLM RELEVANCE (Semantic):\")\n",
    "#             print(f\"  Relevance Score: {eval_result['llm_relevance']:.2%}\")\n",
    "#             print(f\"\\nðŸŽ¯ COMPOSITE SCORE:\")\n",
    "#             print(f\"  Score: {eval_result['composite_score']:.2%}\")\n",
    "#             print(f\"  Status: {'âœ“ PASSED' if result['passed'] else 'âœ— FAILED'}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             results.append({\n",
    "#                 'project': project,\n",
    "#                 'bug_id': bug_id,\n",
    "#                 'error': str(e),\n",
    "#                 'passed': False\n",
    "#             })\n",
    "    \n",
    "#     # Print overall summary\n",
    "#     print(f\"\\n\\n{'='*60}\")\n",
    "#     print(\"OVERALL SUMMARY\")\n",
    "#     print('='*60)\n",
    "#     for result in results:\n",
    "#         if 'error' in result:\n",
    "#             print(f\"âœ— {result['project']}/{result['bug_id']}: ERROR - {result['error']}\")\n",
    "#         else:\n",
    "#             status = 'âœ“' if result['passed'] else 'âœ—'\n",
    "#             print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "#                   f\"FileRec={result['file_recall']:.0%}, \"\n",
    "#                   f\"LineRec={result['line_recall']:.0%}, \"\n",
    "#                   f\"LLM={result['llm_relevance']:.0%}, \"\n",
    "#                   f\"Composite={result['composite_score']:.0%}\")\n",
    "    \n",
    "#     passed = sum(1 for r in results if r.get('passed', False))\n",
    "#     print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "#     print(f\"Success Rate: {passed/len(results):.0%}\")\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wu6y1lr2hy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 4 bugs (commented out - see 10-bug test below)\n",
    "# bugs_to_test = [\n",
    "#     (\"luigi\", 2),\n",
    "#     (\"black\", 4),\n",
    "#     (\"keras\", 1),\n",
    "#     (\"pandas\", 1),\n",
    "# ]\n",
    "# \n",
    "# results = await test_bugsinpy_hybrid(bugs_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bgahnfexbho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test with 10 bugs - showing what agents missed\n",
    "\n",
    "async def test_bugsinpy_with_miss_analysis(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with detailed miss analysis.\n",
    "    Shows what the agents caught vs. what they missed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            print(\"\\nACTUAL FIX (first 500 chars):\")\n",
    "            print(bug_patch[:500])\n",
    "            print(\"...\" if len(bug_patch) > 500 else \"\")\n",
    "            \n",
    "            # Reverse diff\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Parse locations to show what was missed\n",
    "            actual_locations = parse_changed_locations(bug_patch)\n",
    "            flagged_locations = parse_flagged_locations(report)\n",
    "            \n",
    "            # Find missed files\n",
    "            missed_files = actual_locations['files'] - flagged_locations['files']\n",
    "            \n",
    "            # Find missed line ranges\n",
    "            missed_lines = {}\n",
    "            for file in actual_locations['files']:\n",
    "                actual_lines = actual_locations['lines'].get(file, set())\n",
    "                flagged_lines_in_file = flagged_locations['lines'].get(file, set())\n",
    "                \n",
    "                # Lines that weren't caught (no flagged line within 5 lines)\n",
    "                uncaught = []\n",
    "                for actual_line in actual_lines:\n",
    "                    if not any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                        uncaught.append(actual_line)\n",
    "                \n",
    "                if uncaught:\n",
    "                    missed_lines[file] = sorted(uncaught)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60,\n",
    "                'missed_files': list(missed_files),\n",
    "                'missed_lines': missed_lines\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nðŸ“ LOCATION METRICS:\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.0%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.0%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.0%}\")\n",
    "            print(f\"\\nðŸ¤– LLM RELEVANCE: {eval_result['llm_relevance']:.0%}\")\n",
    "            print(f\"ðŸŽ¯ COMPOSITE: {eval_result['composite_score']:.0%}\")\n",
    "            \n",
    "            # Show what was missed\n",
    "            if missed_files:\n",
    "                print(f\"\\nâŒ MISSED FILES: {', '.join(missed_files)}\")\n",
    "            \n",
    "            if missed_lines:\n",
    "                print(f\"\\nâŒ MISSED LINES:\")\n",
    "                for file, lines in missed_lines.items():\n",
    "                    line_ranges = []\n",
    "                    start = lines[0]\n",
    "                    end = start\n",
    "                    for i in range(1, len(lines)):\n",
    "                        if lines[i] == end + 1:\n",
    "                            end = lines[i]\n",
    "                        else:\n",
    "                            line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                            start = lines[i]\n",
    "                            end = start\n",
    "                    line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                    print(f\"  {file}: lines {', '.join(line_ranges)}\")\n",
    "            \n",
    "            print(f\"\\n{'âœ“ PASSED' if result['passed'] else 'âœ— FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"âœ— {result['project']}/{result['bug_id']}: ERROR\")\n",
    "        else:\n",
    "            status = 'âœ“' if result['passed'] else 'âœ—'\n",
    "            missed_info = \"\"\n",
    "            if result['missed_files']:\n",
    "                missed_info += f\" | Missed files: {len(result['missed_files'])}\"\n",
    "            if result['missed_lines']:\n",
    "                total_missed = sum(len(lines) for lines in result['missed_lines'].values())\n",
    "                missed_info += f\" | Missed lines: {total_missed}\"\n",
    "            \n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"Composite={result['composite_score']:.0%} \"\n",
    "                  f\"(LineRec={result['line_recall']:.0%}, LLM={result['llm_relevance']:.0%})\"\n",
    "                  f\"{missed_info}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)} ({passed/len(results):.0%})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13030d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING: scrapy bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/scrapy/utils/datatypes.py b/scrapy/utils/datatypes.py\n",
      "index df2b99c2..f7e3240c 100644\n",
      "--- a/scrapy/utils/datatypes.py\n",
      "+++ b/scrapy/utils/datatypes.py\n",
      "@@ -315,8 +315,9 @@ class LocalCache(collections.OrderedDict):\n",
      "         self.limit = limit\n",
      " \n",
      "     def __setitem__(self, key, value):\n",
      "-        while len(self) >= self.limit:\n",
      "-            self.popitem(last=False)\n",
      "+        if self.limit:\n",
      "+            while len(self) >= self.limit:\n",
      "+                self.popitem(last=False)\n",
      "         super(L\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue in `scrapy/utils/datatypes.py` is the removal of a conditional check for `self.limit` in `LocalCache.__setitem__`, which merges a potential bug (unintended behavior if limit is None/zero) with inefficiency and reduced code clarity from unconditional eviction logic. This change alters original behavior and risks unnecessary pops or errors. Comprehensive test coverage is missing for this method across normal, edge, and error scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed conditional check for self.limit in LocalCache.__setitem__ eviction loop, risking bugs, inefficiency, and unclear behavior if limit is None/zero | scrapy/utils/datatypes.py | 315-322 | 5 | Bug | Restore conditional check for self.limit > 0 before eviction loop; ensure self.limit is always positive integer; add docstring/comment explaining eviction logic and assumptions | Code Analyzer, Best Practices |\n",
      "| Missing tests for LocalCache.__setitem__ | scrapy/utils/datatypes.py | 317,323 | 7 | Test Gap | Add unit tests for: normal insertion below limit; insertion at limit (triggers popitem); multiple insertions triggering evictions; limit=0; limit=1; popping empty cache; integration with larger code | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: ansible bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/lib/ansible/utils/version.py b/lib/ansible/utils/version.py\n",
      "index 0dc6687ed9..d69723b473 100644\n",
      "--- a/lib/ansible/utils/version.py\n",
      "+++ b/lib/ansible/utils/version.py\n",
      "@@ -72,14 +72,14 @@ class _Alpha:\n",
      " \n",
      "         raise ValueError\n",
      " \n",
      "-    def __gt__(self, other):\n",
      "-        return not self.__lt__(other)\n",
      "-\n",
      "     def __le__(self, other):\n",
      "         return self.__lt__(other) or self.__eq__(other)\n",
      " \n",
      "+    def __gt__(self, other):\n",
      "+        return not self.__le__(other)\n",
      "+\n",
      "     def __ge__(self, othe\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary critical issue is an incorrect implementation of the `__gt__` operator in both `_Alpha` and `_Numeric` classes (severity 8), which incorrectly uses `not self.__lt__(other)` instead of `not self.__le__(other)`, breaking strict greater-than semantics and potentially causing incorrect version comparisons, including impacts on `__ge__`. This logic error is flagged by both code analysis and best practices checks. There are also four distinct test coverage gaps for the affected `__gt__` and `__ge__` methods in these classes (severity 7 each), lacking unit and integration tests for key scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect greater-than (__gt__) operator implementation | lib/ansible/utils/version.py | 73, 78, 119, 127, 132 | 8 | Bug | Revert the __gt__ method changes to the original implementation, i.e., define __gt__ as 'return not self.__le__(other)'. Restore original implementations to ensure consistent comparison behavior per Python's rich comparison guidelines. | Code Analyzer, Best Practices |\n",
      "| Missing tests for _Alpha.__gt__ | lib/ansible/utils/version.py | 75, 76, 77 | 7 | Test Gap | Unit tests covering normal greater-than cases, equal edge cases, error handling for incompatible types, and integration with SemanticVersion | Test Coverage |\n",
      "| Missing tests for _Alpha.__ge__ | lib/ansible/utils/version.py | 82, 83, 84 | 7 | Test Gap | Unit tests covering greater-or-equal cases, equality, less-than negatives, invalid types, and integration with SemanticVersion | Test Coverage |\n",
      "| Missing tests for _Numeric.__gt__ | lib/ansible/utils/version.py | 118, 119, 120 | 7 | Test Gap | Unit tests covering typical greater-than numeric comparisons, equal edges, invalid types, and SemanticVersion integration | Test Coverage |\n",
      "| Missing tests for _Numeric.__ge__ | lib/ansible/utils/version.py | 125, 126, 127 | 7 | Test Gap | Unit tests covering greater-or-equal numeric cases, equality, less-than negatives, invalid types, and SemanticVersion integration | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: pytest bug 2\n",
      "============================================================\n",
      "ERROR: [Errno 2] No such file or directory: 'BugsInPy/projects/pytest/bugs/2/bug_patch.txt'\n",
      "\n",
      "============================================================\n",
      "TESTING: sanic bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/sanic/server.py b/sanic/server.py\n",
      "index 47ebcd9..b9e7219 100644\n",
      "--- a/sanic/server.py\n",
      "+++ b/sanic/server.py\n",
      "@@ -735,6 +735,26 @@ class AsyncioServer:\n",
      "             task = asyncio.ensure_future(coro, loop=self.loop)\n",
      "             return task\n",
      " \n",
      "+    def start_serving(self):\n",
      "+        if self.server:\n",
      "+            try:\n",
      "+                return self.server.start_serving()\n",
      "+            except AttributeError:\n",
      "+                raise NotImplementedError(\n",
      "+                    \"server.start_servin\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/c3/gnqkj5hs4yg3rvj1_rjf3rrr0000gn/T/ipykernel_44022/1088895609.py\", line 18, in test_bugsinpy_with_miss_analysis\n",
      "    bug_patch = bug_patch_path.read_text()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/thomaybalazs/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/pathlib.py\", line 1027, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/thomaybalazs/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/pathlib.py\", line 1013, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'BugsInPy/projects/pytest/bugs/2/bug_patch.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is the removal of `start_serving` and `serve_forever` methods from `AsyncioServer` in `sanic/server.py`, which breaks backward compatibility, removes error handling wrappers, and may cause runtime errors like AttributeError instead of controlled NotImplementedError (severity 6, flagged by Code Analyzer, Security, and Best Practices). No replacement functionality or migration guidance is provided in the diff. Additionally, there are two test coverage gaps for these removed methods, lacking tests for normal operation, missing server, and attribute errors (priority 7 each).\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removal of start_serving and serve_forever methods from AsyncioServer breaking compatibility and error handling | sanic/server.py | 735-766 | 6 | Bug | Re-add methods or provide equivalent functionality/alternatives with error handling; include migration guidance if deprecated; update all usages to avoid calls | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for start_serving (normal operation, server=None, missing server.start_serving) | sanic/server.py | 735-745 | 7 | Test Gap | Add unit tests covering normal operation with server available and start_serving callable, server=None, and AttributeError raising NotImplementedError | Test Coverage |\n",
      "| Missing tests for serve_forever (normal operation, server=None, missing server.serve_forever) | sanic/server.py | 746-756 | 7 | Test Gap | Add unit tests covering normal operation with server available and serve_forever callable, server=None, and AttributeError raising NotImplementedError | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 97%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: spacy bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/spacy/util.py b/spacy/util.py\n",
      "index 609c0b572..d4cdca4e0 100644\n",
      "--- a/spacy/util.py\n",
      "+++ b/spacy/util.py\n",
      "@@ -208,6 +208,7 @@ def load_model_from_path(model_path, meta=False, **overrides):\n",
      "     for name in pipeline:\n",
      "         if name not in disable:\n",
      "             config = meta.get(\"pipeline_args\", {}).get(name, {})\n",
      "+            config.update(overrides)\n",
      "             factory = factories.get(name, name)\n",
      "             component = nlp.create_pipe(factory, config=config)\n",
      "             nlp.add_p\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The highest severity issue (9) is the use of an undefined 'factories' variable in spacy/util.py, causing runtime NameErrors. A related bug (severity 7) involves the removal of 'config.update(overrides)' in load_model_from_path, disabling pipeline component configuration overrides and potentially leading to user confusion or incorrect behavior. Test coverage gaps exist for load_model_from_path across multiple key scenarios; no cross-file dependencies noted.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Overriding pipeline component configuration disabled (config.update(overrides) removed) | spacy/util.py | 211-213 | 7 | Bug | Restore the line 'config.update(overrides)' to enable overriding pipeline component configurations as intended and ensure overrides are applied during model loading. | Code Analyzer, Security |\n",
      "| Use of undefined variable 'factories' | spacy/util.py | 212 | 9 | Bug | Ensure that 'factories' is properly imported or defined before it is used (e.g., add 'from some_module import factories'). | Best Practices |\n",
      "| Missing test coverage for load_model_from_path | spacy/util.py | 208-214 | 7 | Test Gap | Add unit tests focusing on mocking pipeline components and configuration loading for: loading with no/empty pipeline, all components disabled, some disabled, overrides applied, meta=False/None, missing factories entry, and correct component creation/addition. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: youtube-dl bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\n",
      "index 3b79b8cb4..35d427eec 100644\n",
      "--- a/youtube_dl/extractor/common.py\n",
      "+++ b/youtube_dl/extractor/common.py\n",
      "@@ -2007,16 +2007,14 @@ class InfoExtractor(object):\n",
      "                                     f['url'] = initialization_url\n",
      "                                 f['fragments'].append({location_key(initialization_url): initialization_url})\n",
      "                             f['fragments'].extend(representation_ms_info['fragments\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary concern is a high-severity bug (sev 7) in `youtube_dl/extractor/common.py` where the new logic for handling duplicate `format_id` in DASH manifests may merge and lose distinct formats, altering original behavior and potentially causing missing data; this is compounded by a related low-severity security review note and removal of an explanatory comment (sev 5). Significant test coverage gaps exist for the `_extract_mpd_formats` function, missing key scenarios like duplicate format handling and edge cases (priority 7). All issues are confined to a single file with no cross-file dependencies observed.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Potential Loss of Handling for Non-Unique Representation IDs and Format Merge Logic Change in DASH Manifest Parsing | youtube_dl/extractor/common.py | 2011-2021 | 7 | Bug | Reconsider handling of formats with duplicate format_id to preserve all distinct formats as the original code did; possibly collect all duplicates or uniquely identify beyond format_id. Also review merging to ensure no improper skipping of metadata, especially if used in security decisions. | Code Analyzer, Security |\n",
      "| Removal of Commented-Out Code Explaining Non-Unique Format IDs | youtube_dl/extractor/common.py | 2015-2022 | 5 | Style | Retain the comment explaining why formats with the same 'format_id' can exist and why append logic is necessary, to preserve rationale and clarity for maintainers. | Best Practices |\n",
      "| Missing Tests for _extract_mpd_formats Function | youtube_dl/extractor/common.py | 2007-2021 | 7 | Test Gap | Add unit tests for: existing_format update scenario (merging fragment data), StopIteration exception handling, format merging correctness (dict copying/updating), fragments appending (new vs existing), unknown MIME type handling, edge cases (empty formats list, multiple same format_id). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: thefuck bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/thefuck/utils.py b/thefuck/utils.py\n",
      "index bd8028e..6112c01 100644\n",
      "--- a/thefuck/utils.py\n",
      "+++ b/thefuck/utils.py\n",
      "@@ -118,7 +118,7 @@ def get_all_executables():\n",
      "     tf_entry_points = ['thefuck', 'fuck']\n",
      " \n",
      "     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n",
      "-            for path in os.environ.get('PATH', '').split(':')\n",
      "+            for path in os.environ.get('PATH', '').split(os.pathsep)\n",
      "             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n",
      "             if no\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a high-severity bug (8/10) in `thefuck/utils.py` where the PATH environment variable splitting was changed from platform-independent `os.pathsep` to a hardcoded colon `:`, breaking compatibility on Windows and potentially leading to incorrect executable searches or security misconfigurations. This root problem is flagged by Code Analyzer (Bug), Security, and Best Practices agents across lines 120-121. Additionally, there is a test coverage gap (priority 7) for the `get_all_executables` function spanning lines 118-124, lacking tests for key scenarios like empty PATH, permission errors, and non-UTF8 names.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect Path Separator in Splitting PATH Environment Variable (hardcoded ':' breaks Windows compatibility, incorrect parsing, increased attack surface) | thefuck/utils.py | 120-121 | 8 | Bug | Use `os.pathsep` to split the PATH environment variable for cross-platform compatibility, e.g., `os.environ.get('PATH', '').split(os.pathsep)` | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for get_all_executables function | thefuck/utils.py | 118-124 | 7 | Test Gap | Add unit tests covering: Normal case (valid PATH/executables), Edge cases (empty PATH, non-existent dirs, empty dirs, non-UTF8 names, missing PATH/None), Error case (permission denied), Integration (env PATH and _safe behavior) | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: luigi bug 4\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py\n",
      "index 5901685c..0ab50dc6 100644\n",
      "--- a/luigi/contrib/redshift.py\n",
      "+++ b/luigi/contrib/redshift.py\n",
      "@@ -353,7 +353,7 @@ class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n",
      "         \"\"\"\n",
      "         logger.info(\"Inserting file: %s\", f)\n",
      "         colnames = ''\n",
      "-        if len(self.columns) > 0:\n",
      "+        if self.columns and len(self.columns) > 0:\n",
      "             colnames = \",\".join([x[0] for x in self.columns])\n",
      "             colnames = '(\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is a potential runtime TypeError in `luigi/contrib/redshift.py` at lines 354-355, where the safety check for `self.columns` being non-None was removed, flagged as both a bug and best practice violation. This change risks exceptions if `self.columns` is None and lacks test coverage for key scenarios including None/empty lists. No cross-file dependencies observed as changes are isolated to this file.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed safety check for self.columns potentially causing TypeError | luigi/contrib/redshift.py | 354-355 | 6 | Bug | Revert to original condition `if self.columns and len(self.columns) > 0` or ensure `self.columns` is always defined as a list/iterable before this check to prevent AttributeError/TypeError | Code Analyzer, Best Practices |\n",
      "| Missing unit tests for S3CopyToTable.run (normal/empty/None columns, special chars/SQL keywords) | luigi/contrib/redshift.py | 353-360 | 5 | Test Gap | Add unit tests covering: normal case with non-empty columns list, empty columns list, None/missing columns attribute, columns with special characters/SQL keywords | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: black bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/black.py b/black.py\n",
      "index 2df03f7..e55e4fe 100644\n",
      "--- a/black.py\n",
      "+++ b/black.py\n",
      "@@ -3116,18 +3116,49 @@ def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n",
      "     \"\"\"\n",
      "     container: Optional[LN] = container_of(leaf)\n",
      "     while container is not None and container.type != token.ENDMARKER:\n",
      "-        is_fmt_on = False\n",
      "-        for comment in list_comments(container.prefix, is_endmarker=False):\n",
      "-            if comment.value in FMT_ON:\n",
      "-                is_fmt_on = True\n",
      "-            elif\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a high-severity (7) bug in `black.py`'s `generate_ignored_nodes` function, where simplified inline logic for FMT_ON/FMT_OFF comments replaces nuanced helper functions, likely breaking handling of child nodes, column positions, and control flow, with potential behavior changes noted across agents. Additional style concerns include leftover commented-out code and reduced clarity from repeated inline checks. Three test coverage gaps exist for the function and its former helpers, with high-priority scenarios missing for normal/edge cases and integration testing.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Logic and functionality issue in generate_ignored_nodes with FMT_ON/FMT_OFF handling (lost nuanced child/column checks, altered control flow) | black.py | 3116-3186 | 7 | Bug | Restore helper functions `fmt_on`, `contains_fmt_on_at_column`, and `first_leaf_column`; reintegrate into `generate_ignored_nodes` for correct detection and child iteration; verify equivalence for all edge cases including nested nodes and comment sequences; add comments explaining logic. | Code Analyzer, Security, Best Practices |\n",
      "| Redundant commented-out code and mixed inline logic reducing clarity/reuse | black.py | 3116-3133 | 6 | Style | Remove commented-out old code; reintroduce/refactor helper functions for 'fmt on/off' checks to encapsulate logic, improve readability, and enable reuse. | Best Practices |\n",
      "| Test Gap: generate_ignored_nodes (missing normal/edge/error/integration scenarios) | black.py | 3116-3148 | 7 | Test Gap | Add unit tests for: normal case no comments; fmt:on stops early; fmt:off resets flag; multiple toggling comments; empty prefix; list_comments exceptions; downstream integration. | Test Coverage |\n",
      "| Test Gap: fmt_on (remodeled inline logic) (missing edge/error scenarios) | black.py | 3116-3148 | 5 | Test Gap | Add unit tests for: multiple comments toggling in prefix; malformed comments or unexpected values (consider isolating if externalized). | Test Coverage |\n",
      "| Test Gap: contains_fmt_on_at_column (logic removed in diff) | black.py | 3116-3148 | 3 | Test Gap | Adapt any existing tests dependent on this logic (function no longer applicable). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 76%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: pandas bug 3\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/pandas/core/series.py b/pandas/core/series.py\n",
      "index 4ba9a0c92..e107b66d3 100644\n",
      "--- a/pandas/core/series.py\n",
      "+++ b/pandas/core/series.py\n",
      "@@ -4684,7 +4684,8 @@ Name: Max Speed, dtype: float64\n",
      "         if copy:\n",
      "             new_values = new_values.copy()\n",
      " \n",
      "-        assert isinstance(self.index, PeriodIndex)\n",
      "+        if not isinstance(self.index, PeriodIndex):\n",
      "+            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n",
      "         new_index = self.index.to_timestamp(freq=\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is the replacement of explicit type checks (with TypeError raises) with assert statements in `pandas/core/series.py` for `to_timestamp` (lines ~4683-4690) and `to_period` (lines ~4710-4717), which alters runtime behavior, reduces error clarity, introduces inconsistency, and creates a vulnerability as asserts can be disabled via Python's -O flag. This merged finding spans Bug, Security, and Best Practices categories with highest severity 7. Comprehensive unit tests are also missing for both functions to cover normal, edge, error, and integration scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Use of assert instead of explicit type check and exception (behavior change in optimized runs, reduces clarity, inconsistent handling) | pandas/core/series.py | 4683-4690,4710-4717 | 7 | Bug | Revert to explicit type checking with raising TypeError instead of using assert, to ensure that type validation always occurs regardless of optimization mode. Restore original if isinstance(self.index, (PeriodIndex, DatetimeIndex)) checks with detailed TypeError messages for both functions to maintain consistency and clarity. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for to_timestamp (normal/empty PeriodIndex, non-PeriodIndex error handling, integration) | pandas/core/series.py | 4683-4690 | 8 | Test Gap | Add unit tests covering normal case (Series with PeriodIndex converts correctly), edge case (empty PeriodIndex), error condition (non-PeriodIndex triggers TypeError/assert behavior), and integration (preserves data/metadata). | Test Coverage |\n",
      "| Missing tests for to_period (normal/empty DatetimeIndex, non-DatetimeIndex error handling, integration) | pandas/core/series.py | 4710-4717 | 8 | Test Gap | Add unit tests covering normal case (Series with DatetimeIndex converts correctly), edge case (empty DatetimeIndex), error condition (non-DatetimeIndex triggers TypeError/assert behavior), and integration (preserves data/metadata). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: keras bug 3\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/keras/models.py b/keras/models.py\n",
      "index a03b09ab..03c487dd 100644\n",
      "--- a/keras/models.py\n",
      "+++ b/keras/models.py\n",
      "@@ -137,9 +137,12 @@ def _clone_functional_model(model, input_tensors=None):\n",
      "                             kwargs['mask'] = computed_mask\n",
      "                     output_tensors = to_list(\n",
      "                         layer(computed_tensor, **kwargs))\n",
      "-                    output_masks = to_list(\n",
      "-                        layer.compute_mask(computed_tensor,\n",
      "-                           \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is the removal of a conditional check for `layer.supports_masking` in `_clone_functional_model` (keras/models.py), which unconditionally calls `layer.compute_mask` and risks runtime errors or incorrect masking behavior when cloning models with unsupported layers. This bug is flagged by multiple agents with high severity (up to 7/10) and requires reinstating the check for backward compatibility and stability. Additionally, critical test gaps exist for validating masking logic across various layer scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed conditional fallback for `layer.supports_masking` before calling `layer.compute_mask` in `_clone_functional_model`, risking errors, exceptions, or inconsistent masking on unsupported layers | keras/models.py | [138, 140, 141, 142, 144, 151, 152, 153, 154, 157] | 7 | Bug | Reinstate the conditional check for `layer.supports_masking` before calling `layer.compute_mask`; if False, set `output_masks` to `[None] * len(output_tensors)` to preserve prior behavior, prevent errors, and handle unsupported cases gracefully | Code Analyzer, Security, Best Practices |\n",
      "| Missing comprehensive tests for `_clone_functional_model` masking logic, including `supports_masking` variations, `compute_mask` outputs/edges/exceptions, multi-output layers, and length validation | keras/models.py | [137, 157] | 7 | Test Gap | Add unit tests with mocked layers (supports_masking=True/False, compute_mask returning None/unexpected/raising exceptions); validate `output_masks` computation, length matching `output_tensors`, and error handling across scenarios | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 92%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 96%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  keras/models.py: lines 163-164\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: matplotlib bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py\n",
      "index f1a16cbd6..81f8aa4ee 100644\n",
      "--- a/lib/matplotlib/axes/_axes.py\n",
      "+++ b/lib/matplotlib/axes/_axes.py\n",
      "@@ -4393,8 +4393,9 @@ default: :rc:`scatter.edgecolors`\n",
      "             - 'none': No patch boundary will be drawn.\n",
      "             - A color or sequence of colors.\n",
      " \n",
      "-            For non-filled markers, the *edgecolors* kwarg is ignored and\n",
      "-            forced to 'face' internally.\n",
      "+            For non-filled markers, *edgecolo\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "Critical bugs in `lib/matplotlib/axes/_axes.py` include an indentation error making `edgecolors='face'` unconditional (severity 7) and altered conditional logic for `facecolors`/`edgecolors` in `PathCollection` constructor (severity 8), potentially breaking marker rendering for filled/non-filled cases. A minor style issue affects docstring clarity (severity 3). High-priority test gaps exist for `Axes.scatter` across multiple scenarios, including non-filled markers and edge cases.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect indentation causes `edgecolors='face'` assignment to be unconditional, changing logic for non-filled markers | lib/matplotlib/axes/_axes.py | 4476-4477, 4488-4489 | 7 | Bug | Restore indentation of `edgecolors='face'` inside the `if not marker_obj.is_filled():` block to preserve original conditional logic | Code Analyzer, Best Practices |\n",
      "| Switched `facecolors` and `edgecolors` logic in `PathCollection` constructor, losing conditional handling for filled/non-filled markers | lib/matplotlib/axes/_axes.py | 4488-4489 | 8 | Bug | Restore conditional assignments: `facecolors=colors if filled else 'none'`, `edgecolors=edgecolors if filled else colors` (or equivalent) | Code Analyzer, Best Practices |\n",
      "| Redundant or unclear docstring update reduces explanation of `edgecolors` behavior for non-filled markers | lib/matplotlib/axes/_axes.py | 4395-4397 | 3 | Style | Revise docstring for clarity: explicitly state `edgecolors` forced to `'face'` for non-filled markers, with colors from `c`/`colors`/`facecolors` | Best Practices |\n",
      "| Test coverage gaps in `Axes.scatter`: missing scenarios for filled/non-filled markers, linewidths/colors edge cases, `is_filled()=False` behavior, integration/rendering, error handling | lib/matplotlib/axes/_axes.py | 4393, 4489 | 8 | Test Gap | Add unit tests covering: normal filled markers with distinct colors, non-filled ensuring `edgecolors='face'`, linewidths (None/scalar/iterable), colors (single/sequence/empty), `is_filled()=False`, rcParams/rendering integration, invalid input errors | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 96%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 98%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  lib/matplotlib/axes/_axes.py: lines 4495\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: tornado bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tornado/http1connection.py b/tornado/http1connection.py\n",
      "index 30f2a172..fd63be62 100644\n",
      "--- a/tornado/http1connection.py\n",
      "+++ b/tornado/http1connection.py\n",
      "@@ -390,7 +390,10 @@ class HTTP1Connection(httputil.HTTPConnection):\n",
      "             self._chunking_output = (\n",
      "                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n",
      "                 and \"Content-Length\" not in headers\n",
      "-                and \"Transfer-Encoding\" not in headers\n",
      "+                and (\n",
      "+                    \"Transfer\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "A critical bug (severity 7) in `tornado/http1connection.py` modifies the chunked transfer encoding check around lines 390-404, potentially breaking support for legitimate chunked requests/responses that were previously handled correctly. This change also lacks explanation (best practices issue, sev 4) and has been assessed for security implications (sev 3), with no new vulnerabilities but possible improper request handling. Major test coverage gaps exist for the `HTTP1Connection.__init__` logic (priority 8), requiring new unit tests for various header/method combinations.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect chunked transfer encoding check after modification | tornado/http1connection.py | 390-404 | 7 | Bug | Revert to the original condition or adjust the new condition to specifically allow 'Transfer-Encoding' if its value is 'chunked'. For example, include 'and (\"Transfer-Encoding\" not in headers or headers[\"Transfer-Encoding\"] == \"chunked\")' condition to maintain previous behavior. Add a comment explaining why the check for 'Transfer-Encoding' being 'chunked' was removed. Review the intended handling of 'Transfer-Encoding' headers to ensure legitimate chunked requests are properly handled or explicitly rejected; add explicit logging and error handling for unsupported transfer encodings. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for HTTP1Connection.__init__ / related constructor logic | tornado/http1connection.py | 390,397 | 8 | Test Gap | Unit tests targeting HTTP1Connection initialization or request parsing with various header and method combinations to verify the chunking_output flag logic. Cover: POST/PUT/PATCH with/without Content-Length, Transfer-Encoding (chunked/other/absent), non-POST methods, empty headers, and response start line handling. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: tqdm bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tqdm/std.py b/tqdm/std.py\n",
      "index 0b57f31..14ab11a 100644\n",
      "--- a/tqdm/std.py\n",
      "+++ b/tqdm/std.py\n",
      "@@ -485,8 +485,7 @@ class tqdm(Comparable):\n",
      "             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n",
      "                 bar_format = _unicode(bar_format)\n",
      "             res = bar_format.format(bar=full_bar, **format_dict)\n",
      "-            if ncols:\n",
      "-                return disp_trim(res, ncols)\n",
      "+            return disp_trim(res, ncols) if ncols else res\n",
      " \n",
      "         elif bar_format:\n",
      "  \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "A critical bug in `tqdm/std.py` (severity 7) alters the progress bar display logic, causing an implicit `None` return when `ncols` is falsy instead of the untrimmed string, potentially breaking output. In `tqdm/utils.py`, changes to `disp_trim` introduce inefficient repeated regex matches and potential redundant ANSI reset sequences (severity 4), which could slightly impact performance and ANSI handling used by the std.py display logic. Multiple test coverage gaps exist for both functions, and while no cross-file API breaks are evident, utils.py changes may amplify display issues in std.py under edge cases like ANSI-heavy outputs.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Missing else clause in display trimming logic causing implicit None return when ncols falsy | tqdm/std.py | 486-504 | 7 | Bug | Restore the original logic or add an else clause returning res when ncols is falsy, e.g. use 'return disp_trim(res, ncols) if ncols else res' or 'if ncols: return disp_trim(res, ncols) else: return res'. Confirm that returning only when ncols is set does not alter functional behavior and does not inadvertently truncate or expose output inconsistently. Maintain a single line conditional expression: return disp_trim(res, ncols) if ncols else res to keep functionality concise and correct. | Code Analyzer, Security, Best Practices |\n",
      "| Inefficient repeated regex matches and potential redundant ANSI reset in disp_trim | tqdm/utils.py | 360-375 | 4 | Performance | Reintroduce 'ansi_present = bool(RE_ANSI.search(data))' variable to avoid repeated regex searches if efficiency is a concern, or leave as is if code clarity is preferred. Ensure that the new implementation correctly preserves the ANSI reset sequence to avoid malformed terminal output, potentially verifying edge cases with terminal escape codes. Store the result of RE_ANSI.search(data) to avoid repeated calls. Check if data ends with \"\\\\033[0m\" before appending to avoid redundant sequences. This reduces regex overhead and improves string correctness. | Code Analyzer, Security, Best Practices |\n",
      "| Test Gap: tqdm.__repr__ missing scenarios for ncols handling, ASCII/non-ASCII, bar_format, total=None | tqdm/std.py | 485-510 | 8 | Test Gap | Unit testing: Verify normal behavior with ncols set (should trim output correctly). Verify behavior with ncols=0 or None (should return untrimmed string). Verify behavior when bar_format includes ASCII and non-ASCII chars. Verify output correctness when total is None (different branch). Test with various bar_format inputs to cover all formatting branches. | Test Coverage |\n",
      "| Test Gap: disp_trim missing scenarios for trimming, ANSI sequences, edge lengths | tqdm/utils.py | 360-370 | 7 | Test Gap | Unit testing: Test output trimming of strings without ANSI codes when data length > length. Test strings with ANSI escape sequences to ensure correct truncation and appending of reset code. Test strings exactly equal to length with/without ANSI codes. Test strings shorter than length are unchanged. Test repeated calls with varying length parameters. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 98%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: httpie bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/httpie/client.py b/httpie/client.py\n",
      "index 1115f4d..482f9dc 100644\n",
      "--- a/httpie/client.py\n",
      "+++ b/httpie/client.py\n",
      "@@ -40,6 +40,7 @@ def get_response(args, config_dir):\n",
      "     \"\"\"Send the request and return a `request.Response`.\"\"\"\n",
      " \n",
      "     requests_session = get_requests_session()\n",
      "+    requests_session.max_redirects = args.max_redirects\n",
      " \n",
      "     if not args.session and not args.session_read_only:\n",
      "         kwargs = get_requests_kwargs(args)\n",
      "diff --git a/httpie/core.py b/httpie/core.py\n",
      "index \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is the removal of the `max_redirects` assignment in `httpie/client.py` (get_response function), flagged by multiple agents as a bug, security vulnerability, and best practice violation, potentially leading to unbounded redirects, DoS risks, and inconsistent behavior. This change may have cross-file implications, as `httpie/core.py` includes exception handling for `TooManyRedirects` that could be affected or underutilized without the limit in place. Additional concerns include a minor commenting style issue in `httpie/core.py` and significant test coverage gaps in both files for redirect and error handling scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removal of max_redirects assignment in get_response | httpie/client.py | 41-44 | 7 | Security | Restore the assignment `requests_session.max_redirects = args.max_redirects` to enforce maximum redirect limits, prevent redirect loops/DoS, and ensure consistent behavior across sessions. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for get_response function | httpie/client.py | 40-46 | 8 | Test Gap | Unit test for get_response with mocked requests session settings and HTTP redirects, covering default values, various max_redirects (incl. zero/high/negative), --max-redirects arg, exceeding limits, and interactions with session flags. | Test Coverage |\n",
      "| Improper Use of Commenting Style Inside Exception Handling | httpie/core.py | 196 | 2 | Style | Remove the '+' character from the comment's start to match normal commenting style and consider expanding into a clearer TODO or issue tracker reference. | Best Practices |\n",
      "| Missing tests for main function | httpie/core.py | 192-199 | 7 | Test Gap | Integration tests simulating command line inputs and error conditions; unit tests for exception handling in main, including TooManyRedirects logging, generic exceptions (with/without traceback), error formatting/logging, and integration with Environment/args. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: cookiecutter bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/cookiecutter/hooks.py b/cookiecutter/hooks.py\n",
      "index 20ccae2..3c73f74 100644\n",
      "--- a/cookiecutter/hooks.py\n",
      "+++ b/cookiecutter/hooks.py\n",
      "@@ -54,11 +54,14 @@ def find_hook(hook_name, hooks_dir='hooks'):\n",
      "         logger.debug('No hooks/dir in template_dir')\n",
      "         return None\n",
      " \n",
      "+    scripts = []\n",
      "     for hook_file in os.listdir(hooks_dir):\n",
      "         if valid_hook(hook_file, hook_name):\n",
      "-            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n",
      "+            scripts.append(os.p\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical findings revolve around a breaking change in `cookiecutter/hooks.py` where `find_hook` now returns only the first matching hook script instead of all valid ones, potentially breaking templates relying on multiple sequential hooks (sev 7 Bug), introducing security risks by skipping additional validation hooks (sev 5), and causing best practices violations in efficiency and consistency (sev 6-7). No cross-file dependencies are impacted as all issues are isolated to this single file. High-priority test gaps (sev 7-8) exist for both `find_hook` and `run_hook`, requiring unit tests to validate the new single-hook behavior.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Changed find_hook from returning all hooks to returning only the first hook | cookiecutter/hooks.py | 54-64, 119-128 | 7 | Bug | Confirm intended behavior is to run only one hook script. If multiple hook scripts per hook name must be supported, restore the original code that collects and returns all matching scripts as a list, and update run_hook accordingly. | Code Analyzer |\n",
      "| Reduced Hook Execution Scope May Introduce Security Risks | cookiecutter/hooks.py | 54-55, 119-125 | 5 | Security | Ensure that all necessary security-related hooks are either combined into the single hook script returned by 'find_hook' or revise the function to support running all valid hooks as before, preserving the security checks implemented in multiple scripts. | Security |\n",
      "| Inefficient and inconsistent hook file handling | cookiecutter/hooks.py | 54, 57-58 | 7 | Style | If the intention is to run all valid hooks, revert 'find_hook' to return a list of all matching hooks, or rename the function to clarify it returns only one hook file. | Best Practices |\n",
      "| Code inconsistency and readability degradation in run_hook | cookiecutter/hooks.py | 119-124, 129-130 | 6 | Style | Clarify the expected return type of 'find_hook'. If only one hook is to be run, rename accordingly and update documentation. Otherwise, revert to handling multiple hook scripts to maintain previous behavior. | Best Practices |\n",
      "| Missing unit tests for find_hook | cookiecutter/hooks.py | 54, 64 | 7 | Test Gap | Add unit tests covering: 'hook_name matches file in hooks_dir and returns absolute path', 'hooks_dir does not exist or is empty returns None', 'no valid hook files returns None', 'multiple valid hooks only first is returned (behavior)'. | Test Coverage |\n",
      "| Missing unit tests for run_hook | cookiecutter/hooks.py | 119, 130 | 8 | Test Gap | Add unit tests covering: 'hook exists and script is run with correct context and project_dir', 'no hook found logs debug and returns without error', 'run_script_with_context is called exactly once with correct parameters', 'handle invalid hook_name gracefully'. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: fastapi bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/fastapi/routing.py b/fastapi/routing.py\n",
      "index b90935e..1ec0b69 100644\n",
      "--- a/fastapi/routing.py\n",
      "+++ b/fastapi/routing.py\n",
      "@@ -498,7 +498,12 @@ class APIRouter(routing.Router):\n",
      "     def add_api_websocket_route(\n",
      "         self, path: str, endpoint: Callable, name: str = None\n",
      "     ) -> None:\n",
      "-        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n",
      "+        route = APIWebSocketRoute(\n",
      "+            path,\n",
      "+            endpoint=endpoint,\n",
      "+            name=name,\n",
      "+            depen\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is the removal of `dependency_overrides_provider` from `APIWebSocketRoute` instantiation in `add_api_websocket_route` (fastapi/routing.py), which breaks dependency injection for WebSocket routes and may introduce unexpected behavior or reduced security controls (severity 8, flagged by Code Analyzer, Security, and Best Practices agents). This change affects lines around 498-509. Additionally, there are critical test coverage gaps for this function, lacking tests for normal cases, edges, errors, and integration scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed dependency_overrides_provider from APIWebSocketRoute instantiation in add_api_websocket_route, breaking dependency injection for WebSocket routes and potentially reducing security customizations | fastapi/routing.py | 498-509 | 8 | Bug | Reintroduce the dependency_overrides_provider parameter when instantiating APIWebSocketRoute to preserve the original functionality, like this:<br>route = APIWebSocketRoute(<br>    path,<br>    endpoint=endpoint,<br>    name=name,<br>    dependency_overrides_provider=self.dependency_overrides_provider,<br>) | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for add_api_websocket_route | fastapi/routing.py | 498-509 | 8 | Test Gap | unit test with some integration test coverage for route addition and websocket handling | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: scrapy bug 3\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py\n",
      "index 49468a2e..b73f864d 100644\n",
      "--- a/scrapy/downloadermiddlewares/redirect.py\n",
      "+++ b/scrapy/downloadermiddlewares/redirect.py\n",
      "@@ -1,5 +1,5 @@\n",
      " import logging\n",
      "-from six.moves.urllib.parse import urljoin\n",
      "+from six.moves.urllib.parse import urljoin, urlparse\n",
      " \n",
      " from w3lib.url import safe_url_string\n",
      " \n",
      "@@ -70,7 +70,10 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n",
      "         if 'Location' not in respo\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary critical issue is in `scrapy/downloadermiddlewares/redirect.py` where changes to Location header access (case-sensitive mismatch from 'Location' to 'location') and removal of protocol-relative URL handling (starting with '//') introduce bugs, potential security risks like improper redirects or SSRF, and inconsistent behavior (severity 7). An additional minor style issue exists with unnecessary import changes reducing clarity. Comprehensive test gaps are identified for the `process_response` function, lacking coverage for key redirect scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect casing for Location header access and removal of protocol-relative URL handling leading to bugs, potential security vulnerabilities (e.g., SSRF/open redirects), and incorrect URL resolution | scrapy/downloadermiddlewares/redirect.py | 70-83 | 7 | Bug | Ensure consistent case-insensitive header access (e.g., normalize keys or use 'Location' as stored). Restore logic for handling scheme-relative URLs starting with '//' by prefixing with request scheme. Validate/sanitize redirect URLs to prevent unsafe resolutions. | Code Analyzer, Security, Best Practices |\n",
      "| Unnecessary Import Change Reducing Clarity (removal of urlparse import despite prior usage) | scrapy/downloadermiddlewares/redirect.py | 2-3 | 3 | Style | Reconsider removal of urlparse import and related code; preserve if needed for URL parsing to maintain clarity and support restored protocol-relative handling. | Best Practices |\n",
      "| Missing test coverage for process_response function (normal redirects, protocol-relative '//', relative URLs, no/malformed Location, non-redirect status, unsafe/malformed URLs, empty header, casing variations, chained redirects, integration) | scrapy/downloadermiddlewares/redirect.py | 70,80 | 8 | Test Gap | Add unit tests for all listed scenarios (valid/invalid Location headers, status codes, relative/protocol-relative/malformed URLs, casing, no header) plus integration tests with real HTTP redirects and chained scenarios. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: ansible bug 3\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/lib/ansible/module_utils/facts/system/distribution.py b/lib/ansible/module_utils/facts/system/distribution.py\n",
      "index 606dbc19bd..528b0e2b9e 100644\n",
      "--- a/lib/ansible/module_utils/facts/system/distribution.py\n",
      "+++ b/lib/ansible/module_utils/facts/system/distribution.py\n",
      "@@ -320,7 +320,8 @@ class DistributionFiles:\n",
      "         elif 'SteamOS' in data:\n",
      "             debian_facts['distribution'] = 'SteamOS'\n",
      "             # nothing else to do, SteamOS gets correct info from python functions\n",
      "-     \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is a syntax error (severity 8) in the Kali distribution detection logic due to improper elif indentation in `lib/ansible/module_utils/facts/system/distribution.py`, which will prevent the code from running. This change also introduces a security vulnerability (severity 4) and best practices violation (severity 5) by narrowing detection to only `/etc/lsb-release`, potentially missing Kali systems identified via `/etc/os-release` and leading to misconfigurations. Comprehensive test coverage is lacking (priority 7) for this critical distribution detection method, with multiple edge cases untested.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect elif statement without block indentation | lib/ansible/module_utils/facts/system/distribution.py | 323-324 | 8 | Bug | Ensure the new elif line replaces the removed elif line cleanly and preserves the indented block underneath. For example, change the existing block header line instead of adding a separate elif without a block, or align the code properly under the new condition. | Code Analyzer |\n",
      "| Potential Distribution Detection Bypass | lib/ansible/module_utils/facts/system/distribution.py | 321-326 | 4 | Security | Restore the original check to include both '/etc/lsb-release' and '/etc/os-release' files when checking for 'Kali' to maintain comprehensive detection of the distribution. | Security |\n",
      "| Incorrect ordering and unreachable code in conditional branches | lib/ansible/module_utils/facts/system/distribution.py | 320-327 | 5 | Style | Restore combined condition for path in ('/etc/lsb-release', '/etc/os-release') when checking for 'Kali' in data as in the original to ensure proper detection. Add comments to clarify why specific paths are checked separately to avoid confusion. Ensure ordering of elif branches does not block valid conditions. | Best Practices |\n",
      "| Test Gap for DistributionFiles.<some_method> | lib/ansible/module_utils/facts/system/distribution.py | 320-327 | 7 | Test Gap | Unit test: Normal input: file path is '/etc/lsb-release' containing 'Kali' string; Negative case: file path is other than '/etc/lsb-release' or data missing 'Kali'; Check no detection if 'Kali' appears in other files; Edge case: data is empty or None; Edge case: path is None or empty string; Verify regex on DISTRIB_RELEASE when present and absent; Error handling: malformed release line (e.g. DISTRIB_RELEASE without value); Integration: interaction with other distribution detection logic | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: pytest bug 3\n",
      "============================================================\n",
      "ERROR: [Errno 2] No such file or directory: 'BugsInPy/projects/pytest/bugs/3/bug_patch.txt'\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "âœ“ scrapy/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ ansible/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ— pytest/2: ERROR\n",
      "âœ“ sanic/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ spacy/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ youtube-dl/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ thefuck/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ luigi/4: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ black/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ pandas/3: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ keras/3: Composite=96% (LineRec=92%, LLM=100%) | Missed lines: 2\n",
      "âœ“ matplotlib/2: Composite=98% (LineRec=96%, LLM=100%) | Missed lines: 1\n",
      "âœ“ tornado/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ tqdm/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ httpie/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ cookiecutter/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ fastapi/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ scrapy/3: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ ansible/3: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ— pytest/3: ERROR\n",
      "\n",
      "Passed: 18/20 (90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/c3/gnqkj5hs4yg3rvj1_rjf3rrr0000gn/T/ipykernel_44022/1088895609.py\", line 18, in test_bugsinpy_with_miss_analysis\n",
      "    bug_patch = bug_patch_path.read_text()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/thomaybalazs/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/pathlib.py\", line 1027, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/thomaybalazs/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/pathlib.py\", line 1013, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'BugsInPy/projects/pytest/bugs/3/bug_patch.txt'\n"
     ]
    }
   ],
   "source": [
    "# Test 20 diverse bugs - different projects not yet tested\n",
    "bugs_to_test_20_diverse = [\n",
    "    (\"scrapy\", 2),        # Web scraping framework\n",
    "    (\"ansible\", 2),       # Automation tool\n",
    "    (\"pytest\", 2),        # Testing framework\n",
    "    (\"sanic\", 2),         # Async web framework\n",
    "    (\"spacy\", 2),         # NLP library\n",
    "    (\"youtube-dl\", 2),    # Video downloader\n",
    "    (\"thefuck\", 2),       # Command corrector\n",
    "    (\"luigi\", 4),         # Pipeline framework\n",
    "    (\"black\", 2),         # Code formatter\n",
    "    (\"pandas\", 3),        # Data analysis\n",
    "    (\"keras\", 3),         # ML framework\n",
    "    (\"matplotlib\", 2),    # Plotting library\n",
    "    (\"tornado\", 2),       # Async networking\n",
    "    (\"tqdm\", 2),          # Progress bar\n",
    "    (\"httpie\", 2),        # HTTP client\n",
    "    (\"cookiecutter\", 2),  # Project templating\n",
    "    (\"fastapi\", 2),       # API framework\n",
    "    (\"scrapy\", 3),        # More scrapy\n",
    "    (\"ansible\", 3),       # More ansible\n",
    "    (\"pytest\", 3),        # More pytest\n",
    "]\n",
    "\n",
    "results = await test_bugsinpy_with_miss_analysis(bugs_to_test_20_diverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
