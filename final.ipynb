{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_1_fast=LitellmModel(model=\"openrouter/x-ai/grok-4.1-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780d499",
   "metadata": {},
   "source": [
    "## 4 Agents + Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6bf01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code (max 20 lines)\", max_length=20)\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b269e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added deletion analysis, chain-of-thought, and better semantic understanding\n",
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed: What code was added? What was removed? What was modified?\n",
    "2. Then, identify potential issues in the changes\n",
    "3. Consider the inverse: What functionality might be LOST from deletions?\n",
    "\n",
    "CRITICAL: Only create findings for actual bugs, logic errors, or antipatterns. If the code is clean and correct, return an empty findings list.\n",
    "\n",
    "DELETION ANALYSIS (CRITICAL):\n",
    "- When you see removed code (lines starting with -), pay special attention to:\n",
    "  * Entire functions/classes being deleted - flag if they're called elsewhere\n",
    "  * Helper functions removed - check if remaining code still works without them\n",
    "  * Error handling removed - flag if this makes code less safe\n",
    "  * Imports removed - verify they're truly unused\n",
    "- If 10+ consecutive lines are deleted, describe what functionality is being removed\n",
    "\n",
    "BUG PATTERNS TO IDENTIFY:\n",
    "- Logic errors, unhandled edge cases, null/undefined access, type mismatches\n",
    "- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\n",
    "- Infinite loops, missing error handling (no try-except blocks)\n",
    "- Code duplication, overly complex functions\n",
    "- Removed functionality that breaks remaining code\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines where the issue occurs (max 20 lines per finding). \n",
    "Do NOT list entire files or large ranges. Be precise and focused.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed from a security perspective\n",
    "2. Identify what security controls or validations were added or removed\n",
    "3. Consider: Does this change introduce new attack surface?\n",
    "\n",
    "CRITICAL: Only create findings for actual security vulnerabilities or risks. If the code is secure and follows security best practices, return an empty findings list.\n",
    "\n",
    "SECURITY PATTERNS:\n",
    "- SQL injection, command injection, XSS vulnerabilities\n",
    "- Hardcoded secrets/credentials, insecure authentication\n",
    "- Path traversal, insecure deserialization\n",
    "- Improper input validation\n",
    "- Missing error handling that could expose sensitive information\n",
    "- Removed security checks or validation code\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\n",
    "- Consider what protections are LOST, not just what bugs are added\n",
    "\n",
    "IMPORTANT: For each vulnerability, specify ONLY the specific lines where the vulnerability exists (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Focus on the exact vulnerable code location.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Describe what changed in terms of code quality\n",
    "2. Identify violations of best practices in the new/modified code\n",
    "3. Consider: Does this change make the code harder to maintain?\n",
    "\n",
    "CRITICAL: Only create findings for actual violations of coding standards and best practices. If the code follows PEP 8, has proper docstrings, and is well-structured, return an empty findings list.\n",
    "\n",
    "CODE QUALITY ISSUES:\n",
    "- Unclear variable names, functions exceeding 50 lines\n",
    "- Nested complexity over 3 levels, missing docstrings\n",
    "- Inconsistent formatting, magic numbers without explanation\n",
    "- Violations of DRY principle\n",
    "- Unclosed resources (files, database cursors, connections)\n",
    "- Missing try-except blocks for error-prone operations\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If helpful comments, docstrings, or error handling are removed, flag it\n",
    "- If code is simplified but loses clarity, mention it\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines with the violation (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Be specific and targeted.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Identify what functions/methods are new or modified\n",
    "2. For each, assess criticality and risk\n",
    "3. Only flag missing tests for high-risk code\n",
    "\n",
    "CRITICAL: Only create test gap findings for functions that are genuinely risky if untested. Use priority 7-8 for critical code, priority 4-5 for nice-to-have tests.\n",
    "\n",
    "PRIORITY GUIDELINES:\n",
    "- Priority 8-10: Functions handling user input, authentication, authorization, financial transactions, data persistence, security controls, or external API calls\n",
    "- Priority 7: Functions with complex logic, multiple conditional branches, error-prone operations (file I/O, parsing, calculations)\n",
    "- Priority 4-6: Simple utility functions, formatters, getters/setters, straightforward data transformations\n",
    "- Priority 1-3: Trivial helpers (one-liners, simple wrappers, obvious logic)\n",
    "\n",
    "DO NOT FLAG: Trivial helper functions, simple string formatters, obvious getters/setters, or functions with self-evident correctness.\n",
    "\n",
    "For each flagged function, suggest test cases covering:\n",
    "- Normal input cases\n",
    "- Edge cases (empty, null, boundary values)\n",
    "- Error conditions (exceptions, failures, timeouts)\n",
    "- Integration scenarios\n",
    "\n",
    "IMPORTANT: For each gap, specify ONLY the specific lines of the function needing tests (max 20 lines per gap).\n",
    "Do NOT list entire files. Focus on the specific untested function location.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9520f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_python_gotchas(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant Python gotchas patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    python_gotchas_collection = chroma_client.get_collection(name=\"python_gotchas_patterns\")\n",
    "    results = python_gotchas_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_code_review_patterns(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant code review patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    code_review_collection = chroma_client.get_collection(name=\"code_review_patterns\")\n",
    "    results = code_review_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_refactoring_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant refactoring patterns from ChromaDB (multi-file changes, shotgun surgery, etc.)\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    refactoring_collection = chroma_client.get_collection(name=\"refactoring_patterns\")\n",
    "    results = refactoring_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa22fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_all_agents(diff):\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer, diff),\n",
    "#         Runner.run(security_agent, diff),\n",
    "#         Runner.run(best_practices_agent, diff),\n",
    "#         Runner.run(test_coverage_agent, diff)\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "async def run_all_agents(diff):\n",
    "    # Get RAG context for all agents\n",
    "    # INCREASED n_results from 5 to 15 for security patterns to capture more injection patterns\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=15)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    python_gotchas = get_relevant_python_gotchas(diff, n_results=3)\n",
    "    code_review_patterns = get_relevant_code_review_patterns(diff, n_results=3)\n",
    "    refactoring_patterns = get_relevant_refactoring_patterns(diff, n_results=5)  # NEW: Multi-file refactoring patterns\n",
    "    \n",
    "    # Create RAG-enhanced Code Analyzer agent (UPDATED: with all patterns including refactoring)\n",
    "    enhanced_code_analyzer_instructions = f\"\"\"{code_analyzer_instructions}\n",
    "\n",
    "RELEVANT PYTHON GOTCHAS TO CHECK:\n",
    "{python_gotchas}\n",
    "\n",
    "RELEVANT CODE REVIEW PATTERNS TO CHECK:\n",
    "{code_review_patterns}\n",
    "\n",
    "RELEVANT REFACTORING PATTERNS TO CHECK (Multi-File Changes):\n",
    "{refactoring_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    code_analyzer_rag = Agent(\n",
    "        name=\"Code Analyzer\",\n",
    "        instructions=enhanced_code_analyzer_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=CodeAnalyzerOutput\n",
    "    )\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer_rag, diff),  # Now uses RAG with refactoring patterns!\n",
    "        Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(best_practices_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(test_coverage_agent, diff)  # No RAG needed for test coverage\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af692459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12ae4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added multi-file awareness and cross-file dependency detection\n",
    "\n",
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "CRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "AGGREGATION GUIDELINES:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. MULTI-FILE AWARENESS (CRITICAL):\n",
    "   - If findings span multiple files, check for cross-file dependencies\n",
    "   - Flag if changes in one file might break APIs/contracts in another file\n",
    "   - Look for patterns like: \"File A removes function X, but does File B call it?\"\n",
    "   - Consider the bigger picture: Do these changes work together?\n",
    "\n",
    "3. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "4. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "5. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_1_fast,\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "594ab3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72c85fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True, min_severity: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        min_severity: Minimum severity threshold (1-10). Findings below this are filtered out. (default: 1)\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    with trace(\"Multi-Agent Code Review\"):\n",
    "        results = await run_all_agents(diff)\n",
    "        code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "        \n",
    "        # Filter findings by severity threshold\n",
    "        def filter_by_severity(result):\n",
    "            filtered_findings = [\n",
    "                finding for finding in result.final_output.findings\n",
    "                if getattr(finding, 'severity', getattr(finding, 'priority', 0)) >= min_severity\n",
    "            ]\n",
    "            result.final_output.findings = filtered_findings\n",
    "            return result\n",
    "        \n",
    "        code_result = filter_by_severity(code_result)\n",
    "        security_result = filter_by_severity(security_result)\n",
    "        best_practices_result = filter_by_severity(best_practices_result)\n",
    "        test_coverage_result = filter_by_severity(test_coverage_result)\n",
    "        \n",
    "        organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "        \n",
    "        # If all findings were filtered out, return early with a clean report\n",
    "        if not any(organized.values()):\n",
    "            clean_report = \"# Code Review Report\\n\\nNo issues found meeting severity threshold.\\n\"\n",
    "            print(clean_report)\n",
    "            return clean_report\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALLING AGGREGATOR...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report = await aggregator_agent(organized)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AGGREGATOR OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(report)\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        if save_output:\n",
    "            os.makedirs(\"user-data\", exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {filepath}\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83b1444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Code Review Report\n",
      "\n",
      "No issues found meeting severity threshold.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_diff = '''\n",
    "diff --git a/utils.py b/utils.py\n",
    "index abc123..def456 100644\n",
    "--- a/utils.py\n",
    "+++ b/utils.py\n",
    "@@ -1,3 +1,8 @@\n",
    "+def greet(name):\n",
    "+    \"\"\"Return a greeting message.\"\"\"\n",
    "+    return f\"Hello, {name}!\"\n",
    "+\n",
    "+\n",
    " def add(a, b):\n",
    "     \"\"\"Add two numbers.\"\"\"\n",
    "     return a + b\n",
    "'''\n",
    "\n",
    "report = await review_code(sample_diff, save_output=False, min_severity=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acccdd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "A critical SQL injection vulnerability exists in the `authenticate` method of `user_auth.py` due to direct concatenation of user inputs into SQL queries, flagged by multiple agents as a high-severity security risk that could allow unauthorized access or data leakage. Comprehensive test coverage is missing for this function, including normal, edge, error, and security scenarios. No cross-file dependencies observed in this single-file review.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate Method | user_auth.py | 7-12 | 9 | Security | Use parameterized queries with placeholders to safely pass user inputs to the SQL query, e.g., `cursor.execute(\"SELECT * FROM users WHERE username=? AND password=?\", (username, password))`. Example: `query = \"SELECT * FROM users WHERE username=? AND password=?\"; cursor.execute(query, (username, password))` | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for authenticate function | user_auth.py | 7-11 | 9 | Test Gap | Unit test with mock database and integration test with real database, covering: normal case (valid username/password), edge cases (empty username/password), error handling (SQL injection attempts, DB failures), integration (actual user records) | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serious_diff = '''\n",
    "diff --git a/user_auth.py b/user_auth.py\n",
    "index abc123..def456 100644\n",
    "--- a/user_auth.py\n",
    "+++ b/user_auth.py\n",
    "@@ -5,6 +5,12 @@ class UserAuth:\n",
    "     def __init__(self):\n",
    "         self.db = sqlite3.connect('users.db')\n",
    "     \n",
    "+    def authenticate(self, username, password):\n",
    "+        query = \"SELECT * FROM users WHERE username='\" + username + \"' AND password='\" + password + \"'\"\n",
    "+        cursor = self.db.cursor()\n",
    "+        result = cursor.execute(query)\n",
    "+        return result.fetchone() is not None\n",
    "+\n",
    "'''\n",
    "\n",
    "report = await review_code(serious_diff, save_output=False, min_severity=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8879d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc06abb",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c495b5",
   "metadata": {},
   "source": [
    "## Synthetic Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c351b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"You are an evaluation judge for code review systems comparing expected findings (ground truth) against actual findings.\n",
    "\n",
    "CRITICAL MATCHING RULES:\n",
    "1. Each actual finding can match AT MOST ONE expected finding\n",
    "2. Each expected finding can match AT MOST ONE actual finding\n",
    "3. Once an actual finding is matched, it CANNOT be used again\n",
    "4. Only match within same category (bugs ≠ test gaps)\n",
    "\n",
    "PROCESS:\n",
    "1. Count total_actual from \"Total Distinct Issues: X\" in report\n",
    "2. For EACH expected finding:\n",
    "   - Find the BEST matching actual finding that hasn't been used yet\n",
    "   - If good match exists: mark as matched=True, record which actual finding\n",
    "   - If no match: mark as matched=False\n",
    "   - NEVER reuse an actual finding for multiple expected findings\n",
    "\n",
    "A match means the same type of issue was identified, even if worded differently.\n",
    "\"\"\"\n",
    "\n",
    "class MatchedFinding(BaseModel):\n",
    "    expected: str = Field(description=\"the expected finding text\")\n",
    "    matched: bool = Field(description=\"true if the expected finding is present, else false\")\n",
    "    actual_finding: Optional[str] = Field(default=None, description=\"the matching text from report (if matched)\")\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    matched_findings: list[MatchedFinding]\n",
    "    total_expected: int = Field(description=\"Total number of expected findings from ground truth\")\n",
    "    total_actual: int = Field(description=\"Count of distinct issues in the report's summary section\")\n",
    "    # matches: int = Field(description=\"Number of expected findings successfully matched\")\n",
    "    \n",
    "    def model_post_init(self, __context):\n",
    "        # Calculate matches from the list\n",
    "        matches = sum(1 for mf in self.matched_findings if mf.matched)\n",
    "        \n",
    "        # Check for duplicate actual findings\n",
    "        actual_findings_used = [\n",
    "            mf.actual_finding for mf in self.matched_findings \n",
    "            if mf.matched and mf.actual_finding\n",
    "        ]\n",
    "        unique_actuals = len(set(actual_findings_used))\n",
    "        \n",
    "        if matches > unique_actuals:\n",
    "            print(f\"ERROR: {matches} matches but only {unique_actuals} unique actual findings used!\")\n",
    "            print(\"The judge matched the same actual finding multiple times.\")\n",
    "        \n",
    "        if matches > self.total_actual:\n",
    "            print(f\"WARNING: Matches ({matches}) > Total Actual ({self.total_actual})\")\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_report(report: str, ground_truth_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixed evaluation function with proper counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        name=\"Evaluation Judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"gpt-5.1\",\n",
    "        output_type=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "GROUND TRUTH (expected findings):\n",
    "{ground_truth_content}\n",
    "\n",
    "ACTUAL REPORT (what the system found):\n",
    "{report}\n",
    "\n",
    "For each expected finding, determine if it matches any actual finding.\n",
    "Output matched_findings list, total_expected, and total_actual.\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(judge_agent, prompt)\n",
    "    eval_result = result.final_output\n",
    "    \n",
    "    # Calculate matches from the actual data - don't trust LLM counting\n",
    "    matches = sum(1 for mf in eval_result.matched_findings if mf.matched)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = matches / eval_result.total_expected if eval_result.total_expected > 0 else 0\n",
    "    precision = matches / eval_result.total_actual if eval_result.total_actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"matches\": matches,\n",
    "        \"total_expected\": eval_result.total_expected,\n",
    "        \"total_actual\": eval_result.total_actual,\n",
    "        \"details\": eval_result.matched_findings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test 2\n",
    "test_dir = Path(\"test-cases\")\n",
    "diff_file = test_dir / \"01_sql_injection.diff\"\n",
    "\n",
    "# Load files\n",
    "diff_content = diff_file.read_text()\n",
    "expected_file = diff_file.with_name(\"01_sql_injection_expected.json\")\n",
    "ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# Run review WITH saving\n",
    "report = await review_code(diff_content, save_output=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_result = await evaluate_report(report, ground_truth_content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JUDGE OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "print(f\"matches: {eval_result['matches']}\")\n",
    "print(f\"\\nmatched_findings:\")\n",
    "for mf in eval_result['details']:\n",
    "    print(f\"\\n  Expected: {mf.expected}\")\n",
    "    print(f\"  Matched: {mf.matched}\")\n",
    "    if mf.actual_finding:\n",
    "        print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATED METRICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all test cases\n",
    "\n",
    "test_cases = [\n",
    "    \"01_sql_injection\",\n",
    "    \"02_logic_bug\",\n",
    "    \"03_code_quality\",\n",
    "    \"04_multi_file_security\",\n",
    "    \"05_multi_file_mixed\"\n",
    "]\n",
    "\n",
    "async def run_all_tests():\n",
    "    test_dir = Path(\"test-cases\")\n",
    "    results = []\n",
    "    \n",
    "    for test_name in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {test_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load files\n",
    "        diff_file = test_dir / f\"{test_name}.diff\"\n",
    "        diff_content = diff_file.read_text()\n",
    "        expected_file = test_dir / f\"{test_name}_expected.json\"\n",
    "        ground_truth_content = expected_file.read_text()\n",
    "        \n",
    "        # Run review\n",
    "        report = await review_code(diff_content, save_output=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = await evaluate_report(report, ground_truth_content)\n",
    "        \n",
    "        # Print detailed judge output\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"JUDGE OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "        print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "        print(f\"matches: {eval_result['matches']}\")\n",
    "        print(f\"\\nmatched_findings:\")\n",
    "        for mf in eval_result['details']:\n",
    "            print(f\"\\n  Expected: {mf.expected}\")\n",
    "            print(f\"  Matched: {mf.matched}\")\n",
    "            if mf.actual_finding:\n",
    "                print(f\"  Actual: {mf.actual_finding[:100]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'recall': eval_result['recall'],\n",
    "            'precision': eval_result['precision'],\n",
    "            'f1': eval_result['f1'],\n",
    "            'passed': eval_result['recall'] >= 0.80 and \n",
    "                     eval_result['precision'] >= 0.85 and \n",
    "                     eval_result['f1'] >= 0.82\n",
    "        })\n",
    "        \n",
    "        # Print calculated metrics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATED METRICS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "        print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "        print(f\"F1 Score: {eval_result['f1']:.2f}\")\n",
    "        print(f\"Status: {'✓ PASSED' if results[-1]['passed'] else '✗ FAILED'}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        status = '✓' if result['passed'] else '✗'\n",
    "        print(f\"{status} {result['test_name']}: R={result['recall']:.2f} P={result['precision']:.2f} F1={result['f1']:.2f}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r['passed'])\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run all tests\n",
    "results = await run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580248",
   "metadata": {},
   "source": [
    "## BugsInPy Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation (Option 4): Models and Utilities\n",
    "import re\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa90b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation Function\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid(report: str, bug_patch: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation: Location metrics (automated) + LLM relevance (semantic).\n",
    "    \n",
    "    Stage 1: Calculate automated location overlap\n",
    "    Stage 2: If file_recall > 0, use LLM to judge semantic relevance\n",
    "    \n",
    "    Returns:\n",
    "        dict with file_recall, line_precision, line_recall, llm_relevance, composite_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(bug_patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance (only if there's file overlap)\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_instructions = \"\"\"You are evaluating the semantic relevance of code review findings to an actual bug fix.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. ACTUAL FIX PATCH: The changes that were made to fix bugs\n",
    "2. CODE REVIEW REPORT: What the review system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the bugs that were fixed\n",
    "- 0.7-0.9: Findings flag related issues that would lead to discovering the bugs\n",
    "- 0.4-0.6: Findings flag the general area but miss specific bugs\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Be objective and strict in your assessment.\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"Relevance Judge\",\n",
    "            instructions=llm_judge_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{bug_patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to the actual fix.\n",
    "\"\"\"\n",
    "        with trace(\"LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test with 10 bugs - showing what agents missed\n",
    "\n",
    "async def test_bugsinpy_with_miss_analysis(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with detailed miss analysis.\n",
    "    Shows what the agents caught vs. what they missed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            print(\"\\nACTUAL FIX (first 500 chars):\")\n",
    "            print(bug_patch[:500])\n",
    "            print(\"...\" if len(bug_patch) > 500 else \"\")\n",
    "            \n",
    "            # Reverse diff\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Parse locations to show what was missed\n",
    "            actual_locations = parse_changed_locations(bug_patch)\n",
    "            flagged_locations = parse_flagged_locations(report)\n",
    "            \n",
    "            # Find missed files\n",
    "            missed_files = actual_locations['files'] - flagged_locations['files']\n",
    "            \n",
    "            # Find missed line ranges\n",
    "            missed_lines = {}\n",
    "            for file in actual_locations['files']:\n",
    "                actual_lines = actual_locations['lines'].get(file, set())\n",
    "                flagged_lines_in_file = flagged_locations['lines'].get(file, set())\n",
    "                \n",
    "                # Lines that weren't caught (no flagged line within 5 lines)\n",
    "                uncaught = []\n",
    "                for actual_line in actual_lines:\n",
    "                    if not any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                        uncaught.append(actual_line)\n",
    "                \n",
    "                if uncaught:\n",
    "                    missed_lines[file] = sorted(uncaught)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60,\n",
    "                'missed_files': list(missed_files),\n",
    "                'missed_lines': missed_lines\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\n📍 LOCATION METRICS:\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.0%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.0%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.0%}\")\n",
    "            print(f\"\\n🤖 LLM RELEVANCE: {eval_result['llm_relevance']:.0%}\")\n",
    "            print(f\"🎯 COMPOSITE: {eval_result['composite_score']:.0%}\")\n",
    "            \n",
    "            # Show what was missed\n",
    "            if missed_files:\n",
    "                print(f\"\\n❌ MISSED FILES: {', '.join(missed_files)}\")\n",
    "            \n",
    "            if missed_lines:\n",
    "                print(f\"\\n❌ MISSED LINES:\")\n",
    "                for file, lines in missed_lines.items():\n",
    "                    line_ranges = []\n",
    "                    start = lines[0]\n",
    "                    end = start\n",
    "                    for i in range(1, len(lines)):\n",
    "                        if lines[i] == end + 1:\n",
    "                            end = lines[i]\n",
    "                        else:\n",
    "                            line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                            start = lines[i]\n",
    "                            end = start\n",
    "                    line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                    print(f\"  {file}: lines {', '.join(line_ranges)}\")\n",
    "            \n",
    "            print(f\"\\n{'✓ PASSED' if result['passed'] else '✗ FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"✗ {result['project']}/{result['bug_id']}: ERROR\")\n",
    "        else:\n",
    "            status = '✓' if result['passed'] else '✗'\n",
    "            missed_info = \"\"\n",
    "            if result['missed_files']:\n",
    "                missed_info += f\" | Missed files: {len(result['missed_files'])}\"\n",
    "            if result['missed_lines']:\n",
    "                total_missed = sum(len(lines) for lines in result['missed_lines'].values())\n",
    "                missed_info += f\" | Missed lines: {total_missed}\"\n",
    "            \n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"Composite={result['composite_score']:.0%} \"\n",
    "                  f\"(LineRec={result['line_recall']:.0%}, LLM={result['llm_relevance']:.0%})\"\n",
    "                  f\"{missed_info}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)} ({passed/len(results):.0%})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 20 diverse bugs - different projects not yet tested\n",
    "bugs_to_test_20_diverse = [\n",
    "    (\"scrapy\", 2),        # Web scraping framework\n",
    "    (\"ansible\", 2),       # Automation tool\n",
    "    (\"pytest\", 2),        # Testing framework\n",
    "    (\"sanic\", 2),         # Async web framework\n",
    "    (\"spacy\", 2),         # NLP library\n",
    "    (\"youtube-dl\", 2),    # Video downloader\n",
    "    (\"thefuck\", 2),       # Command corrector\n",
    "    (\"luigi\", 4),         # Pipeline framework\n",
    "    (\"black\", 2),         # Code formatter\n",
    "    (\"pandas\", 3),        # Data analysis\n",
    "    (\"keras\", 3),         # ML framework\n",
    "    (\"matplotlib\", 2),    # Plotting library\n",
    "    (\"tornado\", 2),       # Async networking\n",
    "    (\"tqdm\", 2),          # Progress bar\n",
    "    (\"httpie\", 2),        # HTTP client\n",
    "    (\"cookiecutter\", 2),  # Project templating\n",
    "    (\"fastapi\", 2),       # API framework\n",
    "    (\"scrapy\", 3),        # More scrapy\n",
    "    (\"ansible\", 3),       # More ansible\n",
    "    (\"pytest\", 3),        # More pytest\n",
    "]\n",
    "\n",
    "results = await test_bugsinpy_with_miss_analysis(bugs_to_test_20_diverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641b91b",
   "metadata": {},
   "source": [
    "## CVE Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Utilities\n",
    "import re\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13030d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE Dataset Loading Functions\n",
    "\n",
    "def load_cve_dataset(json_path: str = \"cve_dataset.json\") -> list[dict]:\n",
    "    \"\"\"Load curated CVE dataset\"\"\"\n",
    "    with open(json_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_cve_patch(cve_id: str, patches_dir: str = \"cve_patches\") -> str:\n",
    "    \"\"\"Load patch file for specific CVE\"\"\"\n",
    "    patch_path = Path(patches_dir) / f\"{cve_id}.patch\"\n",
    "    return patch_path.read_text()\n",
    "\n",
    "# Test loading\n",
    "cve_dataset = load_cve_dataset()\n",
    "print(f\"✓ Loaded {len(cve_dataset)} CVEs\")\n",
    "print(f\"\\nCWE Coverage:\")\n",
    "cwe_counts = {}\n",
    "for cve in cve_dataset:\n",
    "    cwe = cve['cwe_name']\n",
    "    cwe_counts[cwe] = cwe_counts.get(cwe, 0) + 1\n",
    "\n",
    "for cwe, count in sorted(cwe_counts.items()):\n",
    "    print(f\"  {cwe}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h14kceo638o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Helper Functions\n",
    "\n",
    "def check_security_agent_flagged(report: str) -> bool:\n",
    "    \"\"\"Check if Security Agent found anything\"\"\"\n",
    "    return \"Security\" in report and \"Found By\" in report\n",
    "\n",
    "def extract_max_severity(report: str) -> int:\n",
    "    \"\"\"Extract highest severity from report (1-10 scale)\"\"\"\n",
    "    import re\n",
    "    # Parse markdown table for severity column (finds Security findings)\n",
    "    severities = re.findall(r'\\|\\s*(\\d+)\\s*\\|.*\\|\\s*Security\\s*\\|', report, re.IGNORECASE)\n",
    "    return max(map(int, severities)) if severities else 0\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nxhejx209k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE-Specific Evaluation\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid_cve(report: str, patch: str, \n",
    "                               cve_id: str, cwe_id: str, cwe_name: str,\n",
    "                               cvss_score: float, severity: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation for CVEs: Location metrics + LLM relevance + Security detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance with CVE context\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_cve_instructions = f\"\"\"You are evaluating code review findings against a real CVE.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. CVE ID: {cve_id}\n",
    "2. CWE Type: {cwe_name} ({cwe_id})\n",
    "3. CVSS Score: {cvss_score} ({severity})\n",
    "4. ACTUAL FIX PATCH: The changes that fixed the vulnerability\n",
    "5. CODE REVIEW REPORT: What our system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the CVE vulnerability type\n",
    "- 0.7-0.9: Findings flag related security issues that would lead to discovery\n",
    "- 0.4-0.6: Findings flag the general area but miss specific vulnerability\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Special attention:\n",
    "- Did the Security Agent flag this as a security issue?\n",
    "- Is the severity appropriate for the CVE?\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"CVE Relevance Judge\",\n",
    "            instructions=llm_judge_cve_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to this CVE.\n",
    "\"\"\"\n",
    "        with trace(\"CVE LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Stage 3: Security detection check\n",
    "    security_flagged = check_security_agent_flagged(report)\n",
    "    severity_from_report = extract_max_severity(report)\n",
    "    severity_appropriate = abs(severity_from_report - cvss_score) <= 3 if severity_from_report > 0 else False\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score,\n",
    "        'severity_appropriate': severity_appropriate,\n",
    "        'security_finding_present': security_flagged\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "npvow3atsm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: CVE Testing Framework\n",
    "\n",
    "async def test_cve_benchmark(cve_dataset: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test code review system on CVE dataset.\n",
    "    Reuses existing hybrid evaluation with CVE enhancements.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cve in cve_dataset:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {cve['cve_id']} - {cve['cwe_name']}\")\n",
    "        print(f\"CVSS: {cve['cvss_score']} | Project: {cve['project']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load patch\n",
    "            patch = load_cve_patch(cve['cve_id'])\n",
    "            \n",
    "            # Reverse diff (show vulnerability introduction)\n",
    "            reversed_diff = reverse_diff(patch)\n",
    "            \n",
    "            # Run code review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation with CVE context\n",
    "            eval_result = await evaluate_hybrid_cve(\n",
    "                report, patch,\n",
    "                cve['cve_id'], cve['cwe_id'], cve['cwe_name'],\n",
    "                cve['cvss_score'], cve['severity']\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'cwe_id': cve['cwe_id'],\n",
    "                'cwe_name': cve['cwe_name'],\n",
    "                'cvss_score': cve['cvss_score'],\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'security_flagged': eval_result['security_finding_present'],\n",
    "                'severity_appropriate': eval_result['severity_appropriate'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\n📍 Location: FileRec={result['file_recall']:.0%}, LineRec={result['line_recall']:.0%}\")\n",
    "            print(f\"🤖 LLM Relevance: {result['llm_relevance']:.0%}\")\n",
    "            print(f\"🛡️  Security Agent: {'✓ FLAGGED' if result['security_flagged'] else '✗ MISSED'}\")\n",
    "            print(f\"📊 Composite: {result['composite_score']:.0%} - {'✓ PASSED' if result['passed'] else '✗ FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"CVE BENCHMARK SUMMARY\")\n",
    "    print('='*60)\n",
    "    \n",
    "    valid_results = [r for r in results if 'error' not in r]\n",
    "    passed = sum(r['passed'] for r in valid_results)\n",
    "    security_detected = sum(r['security_flagged'] for r in valid_results)\n",
    "    \n",
    "    print(f\"Overall Pass Rate: {passed}/{len(valid_results)} ({passed/len(valid_results):.0%})\")\n",
    "    print(f\"Security Agent Detection: {security_detected}/{len(valid_results)} ({security_detected/len(valid_results):.0%})\")\n",
    "    \n",
    "    # By CWE type\n",
    "    print(f\"\\n📋 Results by CWE Type:\")\n",
    "    cwe_results = {}\n",
    "    for r in valid_results:\n",
    "        cwe = r['cwe_name']\n",
    "        if cwe not in cwe_results:\n",
    "            cwe_results[cwe] = {'total': 0, 'passed': 0}\n",
    "        cwe_results[cwe]['total'] += 1\n",
    "        cwe_results[cwe]['passed'] += r['passed']\n",
    "    \n",
    "    for cwe, stats in sorted(cwe_results.items()):\n",
    "        print(f\"  {cwe}: {stats['passed']}/{stats['total']} passed\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"user-data\", exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"user-data/cve_benchmark_{timestamp}.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\n💾 Results saved to {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ CVE testing framework loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3ikvcg60kl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 6: Run CVE Benchmark\n",
    "\n",
    "# # Load CVE dataset\n",
    "# cve_dataset = load_cve_dataset()\n",
    "# print(f\"Loaded {len(cve_dataset)} CVEs\\n\")\n",
    "\n",
    "# # Run benchmark on all CVEs\n",
    "# results = await test_cve_benchmark(cve_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4sphvor2cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the two problematic CVEs that got stuck\n",
    "\n",
    "problematic_cves = [\"CVE-2024-53908\", \"CVE-2024-23346\"]  # SQL Injection and Code Injection\n",
    "problematic_dataset = [cve for cve in cve_dataset if cve['cve_id'] in problematic_cves]\n",
    "\n",
    "print(f\"Testing {len(problematic_dataset)} problematic CVEs with fixed schema (max 20 lines per finding)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run quick test on just these two\n",
    "results_test = await test_cve_benchmark(problematic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmuyx9qdx57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE Benchmark Comparison Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CVE BENCHMARK COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfiguration changes:\")\n",
    "print(\"  - Security KB: 13 patterns → 43 patterns (OWASP Top 10 2021)\")\n",
    "print(\"  - RAG retrieval: n_results=5 → n_results=15\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RESULTS COMPARISON:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\\n📊 No RAG (Baseline):\")\n",
    "print(\"  Pass Rate: 16/17 (94%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\n🔍 RAG with 13 patterns (n_results=5):\")\n",
    "print(\"  Pass Rate: 15/17 (88%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\n🎯 RAG with 43 patterns (n_results=15):\")\n",
    "print(\"  Run cell above to see results...\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6552020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
