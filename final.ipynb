{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stbdauxgin",
   "metadata": {},
   "source": [
    "## FileSearchTool Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "zd6bdm650vp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Collecting codebase files...\n",
      "   Found 23 files to index\n",
      "üì¶ Creating vector store...\n",
      "   Vector Store ID: vs_6923134311808191a57eb46b25ede7c4\n",
      "üì§ Uploading files...\n",
      "   Batch 1: 23 files uploaded\n",
      "‚úÖ Vector store created successfully!\n",
      "   Total files indexed: 23\n",
      "   Store will auto-expire in 1 day\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "EXCLUDE_DIRS = {\n",
    "    'node_modules', '.venv', 'venv', '__pycache__', \n",
    "    '.git', '.github', 'dist', 'build', '.pytest_cache',\n",
    "    'user-data', 'chroma_db', 'BugsInPy', 'cve_patches',\n",
    "    'test-cases', '.mypy_cache', 'notebooks'\n",
    "}\n",
    "\n",
    "# Only OpenAI-supported formats (removed .yaml, .yml, .toml)\n",
    "VALID_EXTENSIONS = {\n",
    "    '.py', '.js', '.ts', '.tsx', '.jsx', '.md', \n",
    "    '.json', '.txt', '.html', '.css', '.java', \n",
    "    '.cpp', '.c', '.go', '.rb', '.php', '.cs'\n",
    "}\n",
    "\n",
    "def collect_codebase_files(root_dir: str = \".\") -> list[str]:\n",
    "    \"\"\"Collect all relevant code files from the repository\"\"\"\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(root_dir):\n",
    "        # Prune excluded directories\n",
    "        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if Path(filename).suffix in VALID_EXTENSIONS:\n",
    "                filepath = os.path.join(root, filename)\n",
    "                # Skip large files (>100KB)\n",
    "                if os.path.getsize(filepath) < 100_000:\n",
    "                    files.append(filepath)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def create_vector_store_from_codebase() -> str:\n",
    "    \"\"\"Create OpenAI vector store from current codebase\n",
    "    \n",
    "    Returns:\n",
    "        vector_store_id: ID of the created vector store\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    print(\"üìÇ Collecting codebase files...\")\n",
    "    files = collect_codebase_files(\".\")\n",
    "    print(f\"   Found {len(files)} files to index\")\n",
    "    \n",
    "    # Create vector store with auto-expiration\n",
    "    print(\"üì¶ Creating vector store...\")\n",
    "    vs = client.vector_stores.create(\n",
    "        name=\"PR Review Context - Test\",\n",
    "        chunking_strategy={\n",
    "            'type': 'static',\n",
    "            'static': {\n",
    "                'max_chunk_size_tokens': 1600,\n",
    "                'chunk_overlap_tokens': 800\n",
    "            }\n",
    "        },\n",
    "        expires_after={\n",
    "            'anchor': 'last_active_at',\n",
    "            'days': 1  # Auto-delete after 1 day\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"   Vector Store ID: {vs.id}\")\n",
    "    \n",
    "    # Upload files in batches (OpenAI limit: 500 files per batch)\n",
    "    print(\"üì§ Uploading files...\")\n",
    "    batch_size = 500\n",
    "    total_uploaded = 0\n",
    "    \n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        file_streams = []\n",
    "        \n",
    "        for filepath in batch_files:\n",
    "            try:\n",
    "                file_streams.append(open(filepath, 'rb'))\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Skipped {filepath}: {e}\")\n",
    "        \n",
    "        if file_streams:\n",
    "            try:\n",
    "                batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "                    vector_store_id=vs.id,\n",
    "                    files=file_streams\n",
    "                )\n",
    "                total_uploaded += batch.file_counts.completed\n",
    "                print(f\"   Batch {i//batch_size + 1}: {batch.file_counts.completed} files uploaded\")\n",
    "                \n",
    "                if batch.file_counts.failed > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è  Failed: {batch.file_counts.failed} files\")\n",
    "            finally:\n",
    "                # Close file streams\n",
    "                for f in file_streams:\n",
    "                    f.close()\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created successfully!\")\n",
    "    print(f\"   Total files indexed: {total_uploaded}\")\n",
    "    print(f\"   Store will auto-expire in 1 day\")\n",
    "    \n",
    "    return vs.id\n",
    "\n",
    "# Test: Create vector store\n",
    "vector_store_id = create_vector_store_from_codebase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_1_fast=LitellmModel(model=\"openrouter/x-ai/grok-4.1-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780d499",
   "metadata": {},
   "source": [
    "## 4 Agents + Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6bf01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code (max 20 lines)\", max_length=20)\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b269e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added deletion analysis, chain-of-thought, and better semantic understanding\n",
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed: What code was added? What was removed? What was modified?\n",
    "2. Then, identify potential issues in the changes\n",
    "3. Consider the inverse: What functionality might be LOST from deletions?\n",
    "\n",
    "CRITICAL: Only create findings for actual bugs, logic errors, or antipatterns. If the code is clean and correct, return an empty findings list.\n",
    "\n",
    "DELETION ANALYSIS (CRITICAL):\n",
    "- When you see removed code (lines starting with -), pay special attention to:\n",
    "  * Entire functions/classes being deleted - flag if they're called elsewhere\n",
    "  * Helper functions removed - check if remaining code still works without them\n",
    "  * Error handling removed - flag if this makes code less safe\n",
    "  * Imports removed - verify they're truly unused\n",
    "- If 10+ consecutive lines are deleted, describe what functionality is being removed\n",
    "\n",
    "BUG PATTERNS TO IDENTIFY:\n",
    "- Logic errors, unhandled edge cases, null/undefined access, type mismatches\n",
    "- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\n",
    "- Infinite loops, missing error handling (no try-except blocks)\n",
    "- Code duplication, overly complex functions\n",
    "- Removed functionality that breaks remaining code\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines where the issue occurs (max 20 lines per finding). \n",
    "Do NOT list entire files or large ranges. Be precise and focused.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed from a security perspective\n",
    "2. Identify what security controls or validations were added or removed\n",
    "3. Consider: Does this change introduce new attack surface?\n",
    "\n",
    "CRITICAL: Only create findings for actual security vulnerabilities or risks. If the code is secure and follows security best practices, return an empty findings list.\n",
    "\n",
    "SECURITY PATTERNS:\n",
    "- SQL injection, command injection, XSS vulnerabilities\n",
    "- Hardcoded secrets/credentials, insecure authentication\n",
    "- Path traversal, insecure deserialization\n",
    "- Improper input validation\n",
    "- Missing error handling that could expose sensitive information\n",
    "- Removed security checks or validation code\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\n",
    "- Consider what protections are LOST, not just what bugs are added\n",
    "\n",
    "IMPORTANT: For each vulnerability, specify ONLY the specific lines where the vulnerability exists (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Focus on the exact vulnerable code location.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Describe what changed in terms of code quality\n",
    "2. Identify violations of best practices in the new/modified code\n",
    "3. Consider: Does this change make the code harder to maintain?\n",
    "\n",
    "CRITICAL: Only create findings for actual violations of coding standards and best practices. If the code follows PEP 8, has proper docstrings, and is well-structured, return an empty findings list.\n",
    "\n",
    "CODE QUALITY ISSUES:\n",
    "- Unclear variable names, functions exceeding 50 lines\n",
    "- Nested complexity over 3 levels, missing docstrings\n",
    "- Inconsistent formatting, magic numbers without explanation\n",
    "- Violations of DRY principle\n",
    "- Unclosed resources (files, database cursors, connections)\n",
    "- Missing try-except blocks for error-prone operations\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If helpful comments, docstrings, or error handling are removed, flag it\n",
    "- If code is simplified but loses clarity, mention it\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines with the violation (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Be specific and targeted.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Identify what functions/methods are new or modified\n",
    "2. For each, assess criticality and risk\n",
    "3. Only flag missing tests for high-risk code\n",
    "\n",
    "CRITICAL: Only create test gap findings for functions that are genuinely risky if untested. Use priority 7-8 for critical code, priority 4-5 for nice-to-have tests.\n",
    "\n",
    "PRIORITY GUIDELINES:\n",
    "- Priority 8-10: Functions handling user input, authentication, authorization, financial transactions, data persistence, security controls, or external API calls\n",
    "- Priority 7: Functions with complex logic, multiple conditional branches, error-prone operations (file I/O, parsing, calculations)\n",
    "- Priority 4-6: Simple utility functions, formatters, getters/setters, straightforward data transformations\n",
    "- Priority 1-3: Trivial helpers (one-liners, simple wrappers, obvious logic)\n",
    "\n",
    "DO NOT FLAG: Trivial helper functions, simple string formatters, obvious getters/setters, or functions with self-evident correctness.\n",
    "\n",
    "For each flagged function, suggest test cases covering:\n",
    "- Normal input cases\n",
    "- Edge cases (empty, null, boundary values)\n",
    "- Error conditions (exceptions, failures, timeouts)\n",
    "- Integration scenarios\n",
    "\n",
    "IMPORTANT: For each gap, specify ONLY the specific lines of the function needing tests (max 20 lines per gap).\n",
    "Do NOT list entire files. Focus on the specific untested function location.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9520f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_python_gotchas(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant Python gotchas patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    python_gotchas_collection = chroma_client.get_collection(name=\"python_gotchas_patterns\")\n",
    "    results = python_gotchas_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_code_review_patterns(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant code review patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    code_review_collection = chroma_client.get_collection(name=\"code_review_patterns\")\n",
    "    results = code_review_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_refactoring_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant refactoring patterns from ChromaDB (multi-file changes, shotgun surgery, etc.)\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    refactoring_collection = chroma_client.get_collection(name=\"refactoring_patterns\")\n",
    "    results = refactoring_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa22fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import FileSearchTool\n",
    "\n",
    "async def run_all_agents(diff, vector_store_id=None):\n",
    "    # Get RAG context for all agents\n",
    "    # INCREASED n_results from 5 to 15 for security patterns to capture more injection patterns\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=15)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    python_gotchas = get_relevant_python_gotchas(diff, n_results=3)\n",
    "    code_review_patterns = get_relevant_code_review_patterns(diff, n_results=3)\n",
    "    refactoring_patterns = get_relevant_refactoring_patterns(diff, n_results=5)  # Multi-file refactoring patterns\n",
    "    \n",
    "    # Create RAG-enhanced Code Analyzer agent with FileSearchTool\n",
    "    enhanced_code_analyzer_instructions = f\"\"\"{code_analyzer_instructions}\n",
    "\n",
    "RELEVANT PYTHON GOTCHAS TO CHECK:\n",
    "{python_gotchas}\n",
    "\n",
    "RELEVANT CODE REVIEW PATTERNS TO CHECK:\n",
    "{code_review_patterns}\n",
    "\n",
    "RELEVANT REFACTORING PATTERNS TO CHECK (Multi-File Changes):\n",
    "{refactoring_patterns}\n",
    "\n",
    "CODEBASE CONTEXT:\n",
    "You have access to FileSearchTool to search the entire codebase.\n",
    "Use it to:\n",
    "- Find how modified functions are used elsewhere\n",
    "- Check for cross-file dependencies\n",
    "- Understand the broader context of changes\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    # Enhanced Test Coverage instructions\n",
    "    enhanced_test_coverage_instructions = f\"\"\"{test_coverage_instructions}\n",
    "\n",
    "CODEBASE CONTEXT:\n",
    "You have access to FileSearchTool to search the entire codebase.\n",
    "Use it to:\n",
    "- Check if tests already exist for modified functions\n",
    "- Find existing test patterns to suggest\n",
    "- Avoid flagging functions that are already tested\"\"\"\n",
    "    \n",
    "    # Prepare FileSearchTool if vector store ID provided\n",
    "    file_search_tool = None\n",
    "    if vector_store_id:\n",
    "        file_search_tool = FileSearchTool(\n",
    "            max_num_results=5,\n",
    "            vector_store_ids=[vector_store_id]\n",
    "        )\n",
    "    \n",
    "    # Create agents with FileSearchTool\n",
    "    code_analyzer_rag = Agent(\n",
    "        name=\"Code Analyzer\",\n",
    "        instructions=enhanced_code_analyzer_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        tools=[file_search_tool] if file_search_tool else [],\n",
    "        output_type=CodeAnalyzerOutput\n",
    "    )\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    test_coverage_agent_enhanced = Agent(\n",
    "        name=\"Test Coverage Agent\",\n",
    "        instructions=enhanced_test_coverage_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        tools=[file_search_tool] if file_search_tool else [],\n",
    "        output_type=TestCoverageOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer_rag, diff),\n",
    "        Runner.run(security_agent_rag, diff),\n",
    "        Runner.run(best_practices_agent_rag, diff),\n",
    "        Runner.run(test_coverage_agent_enhanced, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af692459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ae4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added multi-file awareness and cross-file dependency detection\n",
    "\n",
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "CRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "AGGREGATION GUIDELINES:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. MULTI-FILE AWARENESS (CRITICAL):\n",
    "   - If findings span multiple files, check for cross-file dependencies\n",
    "   - Flag if changes in one file might break APIs/contracts in another file\n",
    "   - Look for patterns like: \"File A removes function X, but does File B call it?\"\n",
    "   - Consider the bigger picture: Do these changes work together?\n",
    "\n",
    "3. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "4. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "5. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_1_fast,\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "594ab3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c85fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True, min_severity: int = 1, vector_store_id: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        min_severity: Minimum severity threshold (1-10). Findings below this are filtered out. (default: 1)\n",
    "        vector_store_id: Optional OpenAI vector store ID for codebase context (default: None)\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    with trace(\"Multi-Agent Code Review\"):\n",
    "        results = await run_all_agents(diff, vector_store_id=vector_store_id)\n",
    "        code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "        \n",
    "        # Filter findings by severity threshold\n",
    "        def filter_by_severity(result):\n",
    "            filtered_findings = [\n",
    "                finding for finding in result.final_output.findings\n",
    "                if getattr(finding, 'severity', getattr(finding, 'priority', 0)) >= min_severity\n",
    "            ]\n",
    "            result.final_output.findings = filtered_findings\n",
    "            return result\n",
    "        \n",
    "        code_result = filter_by_severity(code_result)\n",
    "        security_result = filter_by_severity(security_result)\n",
    "        best_practices_result = filter_by_severity(best_practices_result)\n",
    "        test_coverage_result = filter_by_severity(test_coverage_result)\n",
    "        \n",
    "        organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "        \n",
    "        # If all findings were filtered out, return early with a clean report\n",
    "        if not any(organized.values()):\n",
    "            clean_report = \"# Code Review Report\\n\\nNo issues found meeting severity threshold.\\n\"\n",
    "            print(clean_report)\n",
    "            return clean_report\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALLING AGGREGATOR...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report = await aggregator_agent(organized)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AGGREGATOR OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(report)\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        if save_output:\n",
    "            os.makedirs(\"user-data\", exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {filepath}\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83b1444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Code Review Report\n",
      "\n",
      "No issues found meeting severity threshold.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_diff = '''\n",
    "diff --git a/utils.py b/utils.py\n",
    "index abc123..def456 100644\n",
    "--- a/utils.py\n",
    "+++ b/utils.py\n",
    "@@ -1,3 +1,8 @@\n",
    "+def greet(name):\n",
    "+    \"\"\"Return a greeting message.\"\"\"\n",
    "+    return f\"Hello, {name}!\"\n",
    "+\n",
    "+\n",
    " def add(a, b):\n",
    "     \"\"\"Add two numbers.\"\"\"\n",
    "     return a + b\n",
    "'''\n",
    "\n",
    "report = await review_code(sample_diff, save_output=False, min_severity=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acccdd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "A critical SQL injection vulnerability exists in the `authenticate` method of `user_auth.py` due to direct concatenation of user inputs into SQL queries, flagged by multiple agents as a high-severity security risk that could allow unauthorized access or data leakage. Comprehensive test coverage is missing for this function, including normal, edge, error, and security scenarios. No cross-file dependencies observed in this single-file review.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate Method | user_auth.py | 7-12 | 9 | Security | Use parameterized queries with placeholders to safely pass user inputs to the SQL query, e.g., `cursor.execute(\"SELECT * FROM users WHERE username=? AND password=?\", (username, password))`. Example: `query = \"SELECT * FROM users WHERE username=? AND password=?\"; cursor.execute(query, (username, password))` | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for authenticate function | user_auth.py | 7-11 | 9 | Test Gap | Unit test with mock database and integration test with real database, covering: normal case (valid username/password), edge cases (empty username/password), error handling (SQL injection attempts, DB failures), integration (actual user records) | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serious_diff = '''\n",
    "diff --git a/user_auth.py b/user_auth.py\n",
    "index abc123..def456 100644\n",
    "--- a/user_auth.py\n",
    "+++ b/user_auth.py\n",
    "@@ -5,6 +5,12 @@ class UserAuth:\n",
    "     def __init__(self):\n",
    "         self.db = sqlite3.connect('users.db')\n",
    "     \n",
    "+    def authenticate(self, username, password):\n",
    "+        query = \"SELECT * FROM users WHERE username='\" + username + \"' AND password='\" + password + \"'\"\n",
    "+        cursor = self.db.cursor()\n",
    "+        result = cursor.execute(query)\n",
    "+        return result.fetchone() is not None\n",
    "+\n",
    "'''\n",
    "\n",
    "report = await review_code(serious_diff, save_output=False, min_severity=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8879d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8tmhhgez4pb",
   "metadata": {},
   "source": [
    "## Test FileSearchTool Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qocwuvs0a0j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with FileSearchTool enabled...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "A critical tuple unpacking mismatch in `code_review/pipeline.py` at lines 142-144 will cause a runtime ValueError, as the `review_code` function unpacks three variables from `run_all_agents(diff)` which returns four results. This bug, flagged by both Code Analyzer and Best Practices agents, lacks error handling and reduces maintainability. No cross-file implications detected in this single-file change.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Tuple unpacking mismatch in review_code function | code_review/pipeline.py | 142-144 | 8 | Bug | Verify the number of results returned by run_all_agents(diff). If it returns four items, unpack all four variables. If it returns three items now, ensure the caller and all dependent code are updated accordingly. Add error handling to manage cases where the number of returned items changes, avoiding mismatched unpacking to prevent runtime errors and improve maintainability. | Code Analyzer, Best Practices |\n",
      "\n",
      "**Total Distinct Issues: 1**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Test completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Review with codebase context\n",
    "# This diff modifies code_review/pipeline.py - agents should find related code\n",
    "\n",
    "test_diff_with_context = '''\n",
    "diff --git a/code_review/pipeline.py b/code_review/pipeline.py\n",
    "index abc123..def456 100644\n",
    "--- a/code_review/pipeline.py\n",
    "+++ b/code_review/pipeline.py\n",
    "@@ -142,7 +142,7 @@ async def review_code(diff: str, save_output: bool = True, min_severity: int =\n",
    "     \"\"\"\n",
    "     with trace(\"Multi-Agent Code Review\"):\n",
    "         results = await run_all_agents(diff)\n",
    "-        code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "+        code_result, security_result, best_practices_result = results\n",
    "         \n",
    "         # Filter findings by severity threshold\n",
    "         def filter_by_severity(result):\n",
    "'''\n",
    "\n",
    "print(\"üß™ Testing with FileSearchTool enabled...\")\n",
    "print(\"=\"*60)\n",
    "report_with_context = await review_code(\n",
    "    test_diff_with_context, \n",
    "    save_output=False, \n",
    "    min_severity=5,\n",
    "    vector_store_id=vector_store_id  # Use vector store\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Test completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5g87nkzs15g",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai.agents:Creating trace Multi-Agent Code Review with id trace_a28492d7edc94a71a2f3a2554eeed7c8\n",
      "DEBUG:openai.agents:Setting current trace: trace_a28492d7edc94a71a2f3a2554eeed7c8\n",
      "DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Test 3: Checking if FileSearchTool is actually called...\n",
      "============================================================\n",
      "This should trigger codebase search for 'process()' method\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: us.i.posthog.com\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x118c27340> with id None\n",
      "DEBUG:openai.agents:Running agent Security Agent (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x118c254a0> with id None\n",
      "DEBUG:openai.agents:Running agent Best Practices Agent (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x118c25630> with id None\n",
      "DEBUG:openai.agents:Running agent Code Analyzer (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x118c27de0> with id None\n",
      "DEBUG:openai.agents:Running agent Test Coverage Agent (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x118c34a10> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x118c615d0> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/responses', 'headers': {'User-Agent': 'Agents/Python 0.5.1'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d3d23002-33a3-4885-b299-84f966292d98', 'json_data': {'include': [], 'input': [{'content': '\\ndiff --git a/code_review/agents.py b/code_review/agents.py\\nindex abc123..def456 100644\\n--- a/code_review/agents.py\\n+++ b/code_review/agents.py\\n@@ -10,6 +10,10 @@ from code_review.rag import get_relevant_patterns\\n\\n+def new_helper_function(data):\\n+    \"\"\"Process data\"\"\"\\n+    return data.process()\\n+\\n code_analyzer = Agent(\\n     name=\"Code Analyzer\",\\n', 'role': 'user'}], 'instructions': 'You are a Security agent reviewing a pull request diff.\\n\\nANALYSIS APPROACH:\\n1. First, describe what changed from a security perspective\\n2. Identify what security controls or validations were added or removed\\n3. Consider: Does this change introduce new attack surface?\\n\\nCRITICAL: Only create findings for actual security vulnerabilities or risks. If the code is secure and follows security best practices, return an empty findings list.\\n\\nSECURITY PATTERNS:\\n- SQL injection, command injection, XSS vulnerabilities\\n- Hardcoded secrets/credentials, insecure authentication\\n- Path traversal, insecure deserialization\\n- Improper input validation\\n- Missing error handling that could expose sensitive information\\n- Removed security checks or validation code\\n\\nDELETION AWARENESS:\\n- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\\n- Consider what protections are LOST, not just what bugs are added\\n\\nIMPORTANT: For each vulnerability, specify ONLY the specific lines where the vulnerability exists (max 20 lines per finding).\\nDo NOT list entire files or large ranges. Focus on the exact vulnerable code location.\\n\\nRELEVANT SECURITY PATTERNS TO CHECK:\\nCode Injection via eval/exec: Using eval(), exec(), or similar functions with user-controlled input enables arbitrary code execution. Dynamic code evaluation is extremely dangerous. eval(user_input), exec(user_string), or using compile() with untrusted code. JonesFaithfulTransformation.from_transformation_str() using eval() on user input. Prevention: Never use eval/exec with user input. Use safe alternatives like ast.literal_eval() for data structures. Validate and sanitize all dynamic inputs. Detection: Look for eval(), exec(), compile(), __import__() with user-controlled input. Check for dynamic code generation from user data.\\n\\nHardcoded Cryptographic Keys or Passwords: Cryptographic keys, passwords, or API tokens hardcoded in source code. Exposed in version control and accessible to anyone with code access. API_KEY = \\'sk_live_abc123xyz\\', password = \\'admin123\\', or hardcoded encryption keys in source code. Prevention: Use environment variables or secret management systems. Never commit credentials to version control. Use .env files in .gitignore. Detection: Look for hardcoded strings that look like passwords, API keys, tokens, or encryption keys. Check for patterns like password=, api_key=, token=.\\n\\nUnpinned Dependency Versions: Not pinning dependency versions allows uncontrolled updates. Using wildcard versions (*, >=) without upper bounds can introduce breaking changes or vulnerabilities. requirements.txt with flask>=2.0 instead of flask==2.0.3, package.json with ^lodash without lock file, allowing any new version. Prevention: Pin exact versions or use lock files. Review updates before applying. Use version ranges cautiously with upper bounds. Detection: Check for wildcard or unbounded version specifiers. Look for missing lock files (package-lock.json, poetry.lock, Pipfile.lock).\\n\\nUnnecessary Features Enabled: Unnecessary features, services, ports, accounts, or privileges enabled. Debug mode enabled in production. DEBUG=True in production Django settings, unnecessary admin interfaces exposed, development endpoints in production. Prevention: Disable debug mode in production. Remove unused features and endpoints. Follow principle of least privilege. Use minimal platform configurations. Detection: Check for debug mode enabled, unused services running, development/test endpoints accessible in production.\\n\\nLog Injection: Logging unsanitized user input allows log injection attacks. Attackers can inject fake log entries or manipulate log files. logger.info(f\\'User {user_input} logged in\\') allows injection of newlines and fake log entries. Prevention: Sanitize user input before logging. Encode special characters (newlines, tabs). Use structured logging (JSON) instead of string concatenation. Detection: Check if user input is sanitized before logging. Look for string formatting or concatenation with user data in log statements.\\n\\nUnused or Unnecessary Dependencies: Including unnecessary dependencies increases attack surface. Unused features or libraries that aren\\'t needed. Transitive dependencies with vulnerabilities. Including full jQuery for one function, unused Flask extensions in requirements.txt, development dependencies in production builds. Prevention: Remove unused dependencies. Use tree-shaking and minification. Separate dev and production dependencies. Audit transitive dependencies. Detection: Audit dependencies for actual usage. Check if all imported libraries are necessary. Look for overlapping functionality from multiple libraries.\\n\\nOutdated or Vulnerable Dependencies: Using libraries, frameworks, or components with known vulnerabilities. Not updating dependencies regularly or not tracking versions. Running unsupported versions. Using Django 2.x with known CVEs, running EOL Python 2.7, using libraries listed in security advisories without patches. Prevention: Regularly update dependencies. Monitor security advisories. Use automated dependency scanning tools (Dependabot, Snyk). Remove unused dependencies. Detection: Check dependency versions against CVE databases. Look for old version numbers in requirements.txt, package.json. Scan for known vulnerable packages.\\n\\nInsecure Deserialization: Deserializing untrusted data without validation enables remote code execution. Using pickle, yaml.load, or eval on untrusted input. pickle.loads(user_data), yaml.load(untrusted_yaml), or yaml.full_load() on untrusted YAML files allowing arbitrary code execution. Prevention: Avoid deserialization of untrusted data. Use yaml.safe_load() instead of yaml.load(). Use JSON instead of pickle when possible. Implement integrity checks. Detection: Look for pickle.loads(), yaml.load(), yaml.full_load() on user-controlled data. Check for deserialization of untrusted JSON/XML into objects.\\n\\nMissing Vulnerability Scanning: Not scanning for vulnerabilities in dependencies. No continuous monitoring for new CVEs. Missing SCA (Software Composition Analysis) in CI/CD. No automated scanning in build pipeline, manual dependency management without vulnerability checks, no awareness of CVEs in used libraries. Prevention: Integrate vulnerability scanning in CI/CD. Use tools like npm audit, pip-audit, Snyk, GitHub Dependabot. Subscribe to security advisories. Detection: Check if project uses tools like npm audit, pip-audit, Snyk, or OWASP Dependency-Check. Look for vulnerability scanning in CI/CD.\\n\\nOS Command Injection: Improper neutralization of special elements used in an OS command. Hostile data is directly used or concatenated into system commands. Executing os.system() or subprocess with unsanitized user input allows attackers to inject shell commands. Prevention: Avoid calling OS commands with user input. If necessary, use subprocess with argument lists (not shell=True), validate input against allowlist, escape special characters. Detection: Look for os.system(), subprocess.call(), eval(), exec() or similar functions that execute system commands with user-controlled input.\\n\\nUnsigned or Unverified Code: Code or packages installed without integrity verification. Missing signature validation for updates or dependencies. Auto-updates without verification. Installing packages without checksum verification, downloading and executing code over HTTP without signature checks. Prevention: Verify digital signatures for all updates. Use package lock files. Enable supply chain security features. Use HTTPS for package downloads. Detection: Check if software updates verify signatures. Look for package installations without integrity checks (missing --verify-hashes).\\n\\nVerbose Error Messages: Detailed error messages or stack traces exposed to users, revealing sensitive information about the application\\'s internals, database structure, or file paths. Showing full stack traces, database error messages, or file paths in production error pages. Prevention: Use generic error messages for users. Log detailed errors server-side only. Disable debug mode in production. Detection: Check if production error handling shows detailed technical information to end users. Look for exposed stack traces.\\n\\nMissing Rate Limiting or DoS Protection: Uncontrolled resource consumption from missing rate limits, allowing denial of service. Processing extremely large inputs, recursive operations, or computationally expensive operations without limits. Accepting arbitrarily large strings for processing (e.g., recursive regex, string operations), no rate limiting on API endpoints. Prevention: Implement rate limiting on API endpoints. Validate and limit input sizes. Set timeouts for operations. Use iterative instead of recursive algorithms where possible. Detection: Look for recursive operations, large input processing, or expensive computations without size/complexity limits. Check for missing rate limiting.\\n\\nMissing Access Controls on API: Accessing API with missing access controls for POST, PUT and DELETE methods. API allows POST/PUT/DELETE requests without proper authorization checks, enabling unauthorized data modification. Prevention: Implement access control mechanisms for all API methods. Deny by default except for public resources. Use server-side enforcement. Detection: Review API endpoints to ensure all methods (especially POST, PUT, DELETE) have proper authorization checks, not just GET.\\n\\nSQL Injection: Improper neutralization of special elements used in an SQL command. User-supplied data is not validated, filtered, or sanitized and is directly used or concatenated into SQL queries. String query = \"SELECT * FROM accounts WHERE custID=\\'\" + request.getParameter(\"id\") + \"\\'\"; - Attacker sends: \\' UNION SELECT SLEEP(10);-- to modify query behavior. Prevention: Use parameterized queries or prepared statements. Never concatenate user input directly into SQL. Example: cursor.execute(\\'SELECT * FROM users WHERE id=?\\', (user_id,)) Detection: Look for string concatenation or string formatting (f-strings, %) used to build SQL queries with user input. Check for queries constructed without parameterization.', 'max_output_tokens': 4000, 'model': 'gpt-4.1-mini', 'temperature': 0.6, 'text': {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'$defs': {'VulnerabilityFinding': {'properties': {'title': {'description': 'Brief name for the vulnerability', 'title': 'Title', 'type': 'string'}, 'description': {'description': 'Detailed explanation', 'title': 'Description', 'type': 'string'}, 'severity': {'description': 'Severity 1-10', 'title': 'Severity', 'type': 'integer'}, 'file': {'description': 'File path', 'title': 'File', 'type': 'string'}, 'relevant_lines': {'description': 'Line numbers (max 20 lines per finding)', 'items': {'type': 'integer'}, 'maxItems': 20, 'title': 'Relevant Lines', 'type': 'array'}, 'suggested_fix': {'description': 'Recommended solution', 'title': 'Suggested Fix', 'type': 'string'}, 'cve_reference': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'CVE ID if applicable', 'title': 'Cve Reference'}}, 'required': ['title', 'description', 'severity', 'file', 'relevant_lines', 'suggested_fix', 'cve_reference'], 'title': 'VulnerabilityFinding', 'type': 'object', 'additionalProperties': False}}, 'properties': {'findings': {'description': 'Security vulnerabilities found', 'items': {'$ref': '#/$defs/VulnerabilityFinding'}, 'title': 'Findings', 'type': 'array'}}, 'required': ['findings'], 'title': 'SecurityOutput', 'type': 'object', 'additionalProperties': False}, 'strict': True}}, 'tools': []}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/responses\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x118935650> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x1188a2ed0> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/responses', 'headers': {'User-Agent': 'Agents/Python 0.5.1'}, 'files': None, 'idempotency_key': 'stainless-python-retry-38282b74-116c-400e-aed7-d200ef494c10', 'json_data': {'include': [], 'input': [{'content': '\\ndiff --git a/code_review/agents.py b/code_review/agents.py\\nindex abc123..def456 100644\\n--- a/code_review/agents.py\\n+++ b/code_review/agents.py\\n@@ -10,6 +10,10 @@ from code_review.rag import get_relevant_patterns\\n\\n+def new_helper_function(data):\\n+    \"\"\"Process data\"\"\"\\n+    return data.process()\\n+\\n code_analyzer = Agent(\\n     name=\"Code Analyzer\",\\n', 'role': 'user'}], 'instructions': \"You are a Best Practices agent reviewing a pull request diff.\\n\\nANALYSIS APPROACH:\\n1. Describe what changed in terms of code quality\\n2. Identify violations of best practices in the new/modified code\\n3. Consider: Does this change make the code harder to maintain?\\n\\nCRITICAL: Only create findings for actual violations of coding standards and best practices. If the code follows PEP 8, has proper docstrings, and is well-structured, return an empty findings list.\\n\\nCODE QUALITY ISSUES:\\n- Unclear variable names, functions exceeding 50 lines\\n- Nested complexity over 3 levels, missing docstrings\\n- Inconsistent formatting, magic numbers without explanation\\n- Violations of DRY principle\\n- Unclosed resources (files, database cursors, connections)\\n- Missing try-except blocks for error-prone operations\\n\\nDELETION AWARENESS:\\n- If helpful comments, docstrings, or error handling are removed, flag it\\n- If code is simplified but loses clarity, mention it\\n\\nIMPORTANT: For each issue, specify ONLY the specific lines with the violation (max 20 lines per finding).\\nDo NOT list entire files or large ranges. Be specific and targeted.\\n\\nRELEVANT BEST PRACTICES PATTERNS TO CHECK:\\nPoor Docstring Format: Docstrings that don't follow PEP 257 conventions: one-liner docstrings should be on one line, multi-line docstrings should have summary line + blank line + details def complex(): '''This function does something really complex and returns a value''' - One-liner too long, should be multi-line Prevention: One-liners: '''Do something and return result.''' Multi-line: '''Summary line.\\\\n\\\\nDetailed description.\\\\n''' Detection: Check docstring format: one-liners over 79 chars, multi-line without blank line after summary, closing quotes not on separate line\\n\\nMissing Docstrings: Public modules, functions, classes, and methods without docstrings. Docstrings are essential for code understanding and maintainability def calculate_tax(income, rate): return income * rate - No docstring explaining purpose, parameters, or return value Prevention: Add docstrings to all public modules, classes, and functions. Use triple quotes. For functions: describe purpose, parameters, return value, and exceptions raised Detection: Check for class definitions, function definitions (especially public ones) without docstring as first statement\\n\\nWildcard Import: Using from module import * makes it unclear which names are in namespace and can cause name conflicts from math import * - Imports everything, pollutes namespace, unclear which functions come from math Prevention: Import specific items: from math import sqrt, pi or import module: import math; math.sqrt() Detection: Look for from X import * statements\\n\\nInconsistent Return Types: Function returns different types in different code paths (e.g., sometimes list, sometimes None). Makes calling code fragile def get_users(count): if count > 0: return [users]; return None - Returns list OR None Prevention: Return consistent types: always return list (empty if no results), always return dict. Document return type. Raise exception for errors instead of returning None/False Detection: Check if function has multiple return statements with different types: return [], return None, return False\\n\\nExcessive Nesting Depth: Functions with deeply nested if/for/while statements (>3 levels) are hard to read and maintain if a: if b: if c: if d: if e: return x - 5 levels of nesting Prevention: Use guard clauses (early returns): if not a: return; if not b: return. Extract nested blocks into separate functions. Use all() for multiple conditions Detection: Count indentation levels in function body. Flag functions with >3 nested levels\", 'max_output_tokens': 4000, 'model': 'gpt-4.1-mini', 'temperature': 0.6, 'text': {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'$defs': {'BestPracticeFinding': {'properties': {'title': {'description': 'Brief name for the best practice violation', 'title': 'Title', 'type': 'string'}, 'description': {'description': 'Detailed explanation', 'title': 'Description', 'type': 'string'}, 'severity': {'description': 'Severity 1-10', 'title': 'Severity', 'type': 'integer'}, 'file': {'description': 'File path', 'title': 'File', 'type': 'string'}, 'relevant_lines': {'description': 'Line numbers (max 20 lines per finding)', 'items': {'type': 'integer'}, 'maxItems': 20, 'title': 'Relevant Lines', 'type': 'array'}, 'suggested_fix': {'description': 'Recommended solution', 'title': 'Suggested Fix', 'type': 'string'}}, 'required': ['title', 'description', 'severity', 'file', 'relevant_lines', 'suggested_fix'], 'title': 'BestPracticeFinding', 'type': 'object', 'additionalProperties': False}}, 'properties': {'findings': {'description': 'Style and best practice violations', 'items': {'$ref': '#/$defs/BestPracticeFinding'}, 'title': 'Findings', 'type': 'array'}}, 'required': ['findings'], 'title': 'BestPracticesOutput', 'type': 'object', 'additionalProperties': False}, 'strict': True}}, 'tools': []}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/responses\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/responses', 'headers': {'User-Agent': 'Agents/Python 0.5.1'}, 'files': None, 'idempotency_key': 'stainless-python-retry-ebb915a6-c743-4394-aa1c-07000f44e1b8', 'json_data': {'include': [], 'input': [{'content': '\\ndiff --git a/code_review/agents.py b/code_review/agents.py\\nindex abc123..def456 100644\\n--- a/code_review/agents.py\\n+++ b/code_review/agents.py\\n@@ -10,6 +10,10 @@ from code_review.rag import get_relevant_patterns\\n\\n+def new_helper_function(data):\\n+    \"\"\"Process data\"\"\"\\n+    return data.process()\\n+\\n code_analyzer = Agent(\\n     name=\"Code Analyzer\",\\n', 'role': 'user'}], 'instructions': \"You are a Code Analyzer agent reviewing a pull request diff.\\n\\nANALYSIS APPROACH:\\n1. First, describe what changed: What code was added? What was removed? What was modified?\\n2. Then, identify potential issues in the changes\\n3. Consider the inverse: What functionality might be LOST from deletions?\\n\\nCRITICAL: Only create findings for actual bugs, logic errors, or antipatterns. If the code is clean and correct, return an empty findings list.\\n\\nDELETION ANALYSIS (CRITICAL):\\n- When you see removed code (lines starting with -), pay special attention to:\\n  * Entire functions/classes being deleted - flag if they're called elsewhere\\n  * Helper functions removed - check if remaining code still works without them\\n  * Error handling removed - flag if this makes code less safe\\n  * Imports removed - verify they're truly unused\\n- If 10+ consecutive lines are deleted, describe what functionality is being removed\\n\\nBUG PATTERNS TO IDENTIFY:\\n- Logic errors, unhandled edge cases, null/undefined access, type mismatches\\n- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\\n- Infinite loops, missing error handling (no try-except blocks)\\n- Code duplication, overly complex functions\\n- Removed functionality that breaks remaining code\\n\\nIMPORTANT: For each issue, specify ONLY the specific lines where the issue occurs (max 20 lines per finding). \\nDo NOT list entire files or large ranges. Be precise and focused.\\n\\nRELEVANT PYTHON GOTCHAS TO CHECK:\\nModule Reimports Don't Reflect Changes: Python only reads the module file on the first time a module is imported. Subsequent import statements bind the module name to the already-loaded module object. Code changes aren't reflected without restart. import mymodule; # modify mymodule.py; import mymodule - Changes not loaded because module is cached Prevention: Use importlib.reload(module) to force re-reading: import importlib; importlib.reload(mymodule). Or restart Python interpreter. In production, this is desired behavior for performance. Detection: Look for development/debugging code that imports modules multiple times expecting changes to be reflected. Check for test code that modifies and re-imports modules.\\n\\nMutating Methods Return None: Methods that mutate an object in-place return None to make it clear the object was modified. This is consistent across list.sort(), list.reverse(), random.shuffle(), etc. sorted_list = my_list.sort() - sorted_list is None! The sort() method modified my_list in place and returned None Prevention: Don't assign return value of mutating methods. Use method for side effect only: my_list.sort(). Or use non-mutating versions: sorted_list = sorted(my_list) Detection: Look for assignments capturing return values of mutating methods: result = list.sort(), result = list.reverse(), result = dict.clear()\\n\\nCircular Import Failure: Using from module import name at module level fails with circular imports because names aren't available during initial module loading. Module is partially initialized when circular import occurs. Module A: from B import func_b; Module B: from A import func_a - Fails because when A tries to import from B, B tries to import from A which isn't finished loading Prevention: Refactor to remove circular dependencies. Or use import module then module.name. Or move imports inside functions. Or restructure code to extract shared functionality to third module. Detection: Look for from X import Y at top level in modules that might have circular dependencies. Check import order when ImportError or AttributeError occurs during import.\\n\\nRELEVANT CODE REVIEW PATTERNS TO CHECK:\\nAPI Signature Changes: Changing function signatures by adding required parameters, removing parameters, changing parameter order, or changing return types breaks existing callers. Changing def process(data) to def process(data, config) where config is required. Changing return type from list to dict. Reordering parameters: def func(a, b) to def func(b, a). Prevention: Add new parameters as optional with defaults: def process(data, config=None). Keep old signature and add new version: process_v2(). Use **kwargs for extensibility. Provide backward compatibility layer. Detection: Look for function signature modifications: added parameters without defaults, removed parameters, changed parameter order, changed return types. Check if function is used externally or in multiple places.\\n\\nDeleted Function Breaking Callers: Removing functions, methods, or classes without checking if they're called elsewhere in the codebase. This is a common breaking change that breaks existing functionality. Deleting helper function trim_filename() that's called from other modules. Removing validation functions that are used by multiple callers. Deleting utility methods without deprecation period. Prevention: Before deleting, search entire codebase for usage: git grep 'function_name'. Use deprecation warnings first: @deprecated decorator. Provide migration path. Update all callers in same PR or coordinate with dependent teams. Detection: When reviewing deletions (lines starting with -), check: 1) Is this a function/class/method definition? 2) Search codebase for calls to this name. 3) Check if removed from public API. 4) Look for imports of deleted names in other files.\\n\\nCross-File Dependency Break: Changes in one file breaking functionality in another file due to shared dependencies, interfaces, or contracts. Especially common with imports, function calls, and data structures. File A changes function signature. File B calls that function with old signature - breaks. File A removes helper class. File C imports that helper class - breaks. Prevention: Keep changes backward compatible. Update all callers in same PR. Use deprecation warnings for gradual migration. Run full test suite. Use static analysis tools to find all references. Detection: For multi-file PRs: 1) Check if modified functions are called from other files. 2) Search for imports of modified/deleted names. 3) Look for shared data structures being changed. 4) Run tests across all affected modules.\\n\\nRELEVANT REFACTORING PATTERNS TO CHECK (Multi-File Changes):\\nChange Function Declaration Breaking Callers: Renaming functions, adding parameters, removing parameters, or changing parameter order breaks all existing callers. This refactoring (aliases: Rename Function, Add Parameter, Remove Parameter, Change Signature) must update every call site to avoid runtime errors. def process(data, exclude_defaults=False, exclude_none=False) changed to def process(data, include_none=True). Old callers using exclude_none=True now fail. Parameter semantics inverted: exclude ‚Üí include requires logic changes at every call site. Prevention: Before changing signature: search entire codebase for all callers. Keep old signature with deprecation warning, add new signature as separate function initially. Update all callers in same PR or use adapter pattern. For semantic inversions, create clear migration guide showing before/after for each usage. Detection: For function signature changes: 1) List all call sites using grep/search. 2) Check if removed parameters had non-default values anywhere. 3) For parameter semantics inversion (exclude ‚Üí include), verify inverse logic applied everywhere. 4) Search for kwargs usage that might pass old parameter names.\\n\\nTest Coverage Regression During Refactoring: Removing test files during refactoring without replacing them causes test coverage regression. TDD principle: refactoring must maintain green tests - 'everything that used to work still works'. Deleting tests for old behavior without adding tests for new behavior leaves code unvalidated. tests/test_skip_defaults.py deleted entirely (testing exclude_defaults/exclude_none parameters). New include_none parameter added but no new tests written. Now serialization behavior untested: does include_none=False truly equal old exclude_none=True? Unknown - no tests verify equivalence. Prevention: Follow TDD red-green-refactor cycle: 1) Before removing old tests, write new tests for new behavior (red). 2) Implement new behavior until tests pass (green). 3) Remove old tests only after confirming new tests cover equivalent functionality (refactor). Maintain or improve coverage during refactoring. Run full test suite after each change to verify 'everything that used to work still works'. Detection: Look for: 1) Deleted test files without corresponding new test files. 2) Removed test functions without replacement tests for new behavior. 3) Refactored parameters/functions with no new test coverage. 4) Test file line count decreasing in PR. 5) Comments like 'removed obsolete tests' without adding new ones.\\n\\nIntroduce Parameter Object (Parameter Group Refactoring): When methods contain repeating groups of parameters (data clumps), consolidating them into a parameter object simplifies signatures. However, removing individual parameters to introduce the object breaks all existing callers unless done carefully. def configure(exclude_defaults=False, exclude_none=False, exclude_unset=False) refactored to def configure(exclusion_config: ExclusionConfig). All callers using individual kwargs now break: configure(exclude_none=True) ‚Üí TypeError. Prevention: Use gradual migration: 1) Create parameter object class. 2) Add new object parameter alongside old parameters initially. 3) Internally, populate object from old params if object not provided. 4) Mark old parameters deprecated. 5) After migration period, remove old parameters. Keep both forms working during transition. Detection: Look for: 1) New class/object introduced as parameter. 2) Multiple related boolean/scalar parameters removed simultaneously. 3) Same parameter group appearing in 3+ method signatures. 4) PR description mentioning 'simplification' or 'consolidation' of parameters.\\n\\nFlag Argument Semantic Inversion: Changing boolean flag parameters, especially inverting their semantics (exclude ‚Üí include, disable ‚Üí enable), breaks code silently. Unlike removing parameters (loud failure), semantic inversions cause wrong behavior with no errors: exclude_none=True ‚â† include_none=True. def serialize(obj, exclude_none=False): changed to def serialize(obj, include_none=True). Caller using old default exclude_none=False (excludes None) now gets include_none=True (includes None) - opposite behavior, no error raised. Silent data corruption. Prevention: Avoid inverting boolean semantics - breaking change requiring major version bump. If necessary: 1) Add new parameter alongside old. 2) Raise DeprecationWarning if old parameter used. 3) Document inverse relationship clearly. 4) Validate only one parameter provided. Better: use explicit methods instead of flags (Martin Fowler recommends regularBook()/premiumBook() over book(isPremium)). Detection: Look for boolean parameter renames with semantic opposites: exclude‚Üíinclude, disable‚Üíenable, skip‚Üíprocess, ignore‚Üíhandle. Check if default values inverted: exclude_none=False ‚Üí include_none=True. Search for all call sites to verify logic inversion applied correctly.\\n\\nShotgun Surgery (Multi-File Coordinated Changes): Making a single logical change requires modifying many small parts scattered across multiple classes or files simultaneously. This indicates a single responsibility has been distributed among many locations, making changes expensive and error-prone. Removing response_model_exclude_defaults parameter requires changes in applications.py (API layer), routing.py (request handling), encoders.py (serialization), and openapi/utils.py (schema generation). Each file needs 10+ line modifications to stay consistent. Prevention: Use Move Method to consolidate scattered behavior into single class. Create new class if none exists to hold consolidated logic. After consolidation, use Inline Class to remove nearly-empty leftover classes. Apply change in coordinated manner: update all call sites in same commit. Detection: Look for: 1) Same parameter/function name changed in 3+ files in single PR. 2) Related functionality changes across layers (API ‚Üí business logic ‚Üí serialization). 3) Comment in one file saying 'also changed in X, Y, Z files'. 4) Multiple files importing/calling same removed function.\\n\\nCODEBASE CONTEXT:\\nYou have access to FileSearchTool to search the entire codebase.\\nUse it to:\\n- Find how modified functions are used elsewhere\\n- Check for cross-file dependencies\\n- Understand the broader context of changes\", 'max_output_tokens': 4000, 'model': 'gpt-4.1-mini', 'temperature': 0.6, 'text': {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'$defs': {'BugFinding': {'properties': {'title': {'description': 'Brief name for the bug', 'title': 'Title', 'type': 'string'}, 'description': {'description': 'Detailed explanation', 'title': 'Description', 'type': 'string'}, 'severity': {'description': 'Severity 1-10', 'title': 'Severity', 'type': 'integer'}, 'file': {'description': 'File path', 'title': 'File', 'type': 'string'}, 'relevant_lines': {'description': 'Line numbers (max 20 lines per finding)', 'items': {'type': 'integer'}, 'maxItems': 20, 'title': 'Relevant Lines', 'type': 'array'}, 'suggested_fix': {'description': 'Recommended solution', 'title': 'Suggested Fix', 'type': 'string'}}, 'required': ['title', 'description', 'severity', 'file', 'relevant_lines', 'suggested_fix'], 'title': 'BugFinding', 'type': 'object', 'additionalProperties': False}}, 'properties': {'findings': {'description': 'Bugs and anti-patterns found', 'items': {'$ref': '#/$defs/BugFinding'}, 'title': 'Findings', 'type': 'array'}}, 'required': ['findings'], 'title': 'CodeAnalyzerOutput', 'type': 'object', 'additionalProperties': False}, 'strict': True}}, 'tools': [{'type': 'file_search', 'vector_store_ids': ['vs_6923134311808191a57eb46b25ede7c4'], 'max_num_results': 5}]}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/responses\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/responses', 'headers': {'User-Agent': 'Agents/Python 0.5.1'}, 'files': None, 'idempotency_key': 'stainless-python-retry-38e9bda6-10fc-4823-8131-1be23e46c75e', 'json_data': {'include': [], 'input': [{'content': '\\ndiff --git a/code_review/agents.py b/code_review/agents.py\\nindex abc123..def456 100644\\n--- a/code_review/agents.py\\n+++ b/code_review/agents.py\\n@@ -10,6 +10,10 @@ from code_review.rag import get_relevant_patterns\\n\\n+def new_helper_function(data):\\n+    \"\"\"Process data\"\"\"\\n+    return data.process()\\n+\\n code_analyzer = Agent(\\n     name=\"Code Analyzer\",\\n', 'role': 'user'}], 'instructions': 'You are a Test Coverage agent reviewing a pull request diff.\\n\\nANALYSIS APPROACH:\\n1. Identify what functions/methods are new or modified\\n2. For each, assess criticality and risk\\n3. Only flag missing tests for high-risk code\\n\\nCRITICAL: Only create test gap findings for functions that are genuinely risky if untested. Use priority 7-8 for critical code, priority 4-5 for nice-to-have tests.\\n\\nPRIORITY GUIDELINES:\\n- Priority 8-10: Functions handling user input, authentication, authorization, financial transactions, data persistence, security controls, or external API calls\\n- Priority 7: Functions with complex logic, multiple conditional branches, error-prone operations (file I/O, parsing, calculations)\\n- Priority 4-6: Simple utility functions, formatters, getters/setters, straightforward data transformations\\n- Priority 1-3: Trivial helpers (one-liners, simple wrappers, obvious logic)\\n\\nDO NOT FLAG: Trivial helper functions, simple string formatters, obvious getters/setters, or functions with self-evident correctness.\\n\\nFor each flagged function, suggest test cases covering:\\n- Normal input cases\\n- Edge cases (empty, null, boundary values)\\n- Error conditions (exceptions, failures, timeouts)\\n- Integration scenarios\\n\\nIMPORTANT: For each gap, specify ONLY the specific lines of the function needing tests (max 20 lines per gap).\\nDo NOT list entire files. Focus on the specific untested function location.\\n\\nCODEBASE CONTEXT:\\nYou have access to FileSearchTool to search the entire codebase.\\nUse it to:\\n- Check if tests already exist for modified functions\\n- Find existing test patterns to suggest\\n- Avoid flagging functions that are already tested', 'max_output_tokens': 4000, 'model': 'gpt-4.1-mini', 'temperature': 0.6, 'text': {'format': {'type': 'json_schema', 'name': 'final_output', 'schema': {'$defs': {'TestGap': {'properties': {'function_name': {'description': 'Name of the function/method lacking tests', 'title': 'Function Name', 'type': 'string'}, 'file': {'description': 'File containing the untested code', 'title': 'File', 'type': 'string'}, 'lines': {'description': 'Line numbers of the untested code (max 20 lines)', 'items': {'type': 'integer'}, 'maxItems': 20, 'title': 'Lines', 'type': 'array'}, 'missing_scenarios': {'description': \"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\", 'items': {'type': 'string'}, 'title': 'Missing Scenarios', 'type': 'array'}, 'priority': {'description': 'Priority 1-10, based on code criticality', 'title': 'Priority', 'type': 'integer'}, 'suggested_test_approach': {'description': 'How to test this (unit test, integration test, etc.)', 'title': 'Suggested Test Approach', 'type': 'string'}}, 'required': ['function_name', 'file', 'lines', 'missing_scenarios', 'priority', 'suggested_test_approach'], 'title': 'TestGap', 'type': 'object', 'additionalProperties': False}}, 'properties': {'findings': {'description': 'Testing gaps found', 'items': {'$ref': '#/$defs/TestGap'}, 'title': 'Findings', 'type': 'array'}}, 'required': ['findings'], 'title': 'TestCoverageOutput', 'type': 'object', 'additionalProperties': False}, 'strict': True}}, 'tools': [{'type': 'file_search', 'vector_store_ids': ['vs_6923134311808191a57eb46b25ede7c4'], 'max_num_results': 5}]}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/responses\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118a9c350>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1189331d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c78f50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1189331d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11871c560>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1189331d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c79cd0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1189331d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c6f440>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118a37f50>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c6d6a0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c796a0>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:00:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'197746'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'676ms'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_5e33e962ea9a4be597104ff831ce0082'), (b'openai-processing-ms', b'2169'), (b'x-envoy-upstream-service-time', b'2172'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a31313d2efaf832-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:00:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'196450'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'1.065s'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_e3a5398b586544b9b9c761285d04373b'), (b'openai-processing-ms', b'2288'), (b'x-envoy-upstream-service-time', b'2291'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a31313d2bfff406-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/responses \"200 OK\" Headers({'date': 'Sun, 23 Nov 2025 14:00:23 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '197746', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '676ms', 'openai-version': '2020-10-01', 'openai-organization': 'user-mtsmaztj6ekr3iwkmr0v9erf', 'openai-project': 'proj_c4meB9EhtGMyT5bmrzRVGYzB', 'x-request-id': 'req_5e33e962ea9a4be597104ff831ce0082', 'openai-processing-ms': '2169', 'x-envoy-upstream-service-time': '2172', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a31313d2efaf832-VIE', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_5e33e962ea9a4be597104ff831ce0082\n",
      "DEBUG:openai.agents:LLM responded\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/responses \"200 OK\" Headers({'date': 'Sun, 23 Nov 2025 14:00:23 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '196450', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '1.065s', 'openai-version': '2020-10-01', 'openai-organization': 'user-mtsmaztj6ekr3iwkmr0v9erf', 'openai-project': 'proj_c4meB9EhtGMyT5bmrzRVGYzB', 'x-request-id': 'req_e3a5398b586544b9b9c761285d04373b', 'openai-processing-ms': '2288', 'x-envoy-upstream-service-time': '2291', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a31313d2bfff406-VIE', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_e3a5398b586544b9b9c761285d04373b\n",
      "DEBUG:openai.agents:LLM responded\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c3bfb0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x111a9da50> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c78f20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://us.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Sun, 23 Nov 2025 14:00:25 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_5a6284c83f5634c79795546e7f8fa60c'), (b'openai-processing-ms', b'324'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-envoy-upstream-service-time', b'328'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a3131501b345ae9-VIE'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:00:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'198912'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'326ms'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_afcfb185e0b444a58b310d9cd136c17a'), (b'openai-processing-ms', b'3702'), (b'x-envoy-upstream-service-time', b'3704'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a31313d280034c2-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai.agents:Exported 5 items\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/responses \"200 OK\" Headers({'date': 'Sun, 23 Nov 2025 14:00:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '198912', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '326ms', 'openai-version': '2020-10-01', 'openai-organization': 'user-mtsmaztj6ekr3iwkmr0v9erf', 'openai-project': 'proj_c4meB9EhtGMyT5bmrzRVGYzB', 'x-request-id': 'req_afcfb185e0b444a58b310d9cd136c17a', 'openai-processing-ms': '3702', 'x-envoy-upstream-service-time': '3704', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a31313d280034c2-VIE', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_afcfb185e0b444a58b310d9cd136c17a\n",
      "DEBUG:openai.agents:LLM responded\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:00:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'193948'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'1.815s'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_eb18a2a8ad254cf79bf8007aa09b2645'), (b'openai-processing-ms', b'4762'), (b'x-envoy-upstream-service-time', b'4764'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a31313d3aa927b8-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/responses \"200 OK\" Headers({'date': 'Sun, 23 Nov 2025 14:00:26 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '193948', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '1.815s', 'openai-version': '2020-10-01', 'openai-organization': 'user-mtsmaztj6ekr3iwkmr0v9erf', 'openai-project': 'proj_c4meB9EhtGMyT5bmrzRVGYzB', 'x-request-id': 'req_eb18a2a8ad254cf79bf8007aa09b2645', 'openai-processing-ms': '4762', 'x-envoy-upstream-service-time': '4764', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a31313d3aa927b8-VIE', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_eb18a2a8ad254cf79bf8007aa09b2645\n",
      "DEBUG:openai.agents:LLM responded\n",
      "DEBUG:openai.agents:Processing output item type=file_search_call class=ResponseFileSearchToolCall\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x118c27750> with id None\n",
      "DEBUG:openai.agents:Running agent Aggregator (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.GenerationSpanData object at 0x1186a5010> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - \u001b[92mlitellm.acompletion(model='openrouter/x-ai/grok-4.1-fast', messages=[{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], tools=None, temperature=0.6, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.5.1'}, api_key='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', base_url=None, reasoning={'enabled': True})\u001b[0m\n",
      "DEBUG:LiteLLM:\u001b[92mlitellm.acompletion(model='openrouter/x-ai/grok-4.1-fast', messages=[{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], tools=None, temperature=0.6, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.5.1'}, api_key='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', base_url=None, reasoning={'enabled': True})\u001b[0m\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - \n",
      "\n",
      "DEBUG:LiteLLM:\n",
      "\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:489 - self.optional_params: {}\n",
      "DEBUG:LiteLLM:self.optional_params: {}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n",
      "DEBUG:LiteLLM:ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: main.py:505 - üîÑ NO SHARED SESSION: acompletion called without shared_session\n",
      "DEBUG:LiteLLM:üîÑ NO SHARED SESSION: acompletion called without shared_session\n",
      "\u001b[92m15:00:26 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= x-ai/grok-4.1-fast; provider = openrouter\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= x-ai/grok-4.1-fast; provider = openrouter\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3430 - \n",
      "LiteLLM: Params passed to completion() {'model': 'x-ai/grok-4.1-fast', 'functions': None, 'function_call': None, 'temperature': 0.6, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openrouter', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None, 'reasoning': {'enabled': True}}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Params passed to completion() {'model': 'x-ai/grok-4.1-fast', 'functions': None, 'function_call': None, 'temperature': 0.6, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openrouter', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None, 'reasoning': {'enabled': True}}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3433 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.6, 'stream': False}\n",
      "DEBUG:LiteLLM:\n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.6, 'stream': False}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - Final returned optional params: {'temperature': 0.6, 'stream': False, 'extra_body': {}, 'reasoning': {'enabled': True}}\n",
      "DEBUG:LiteLLM:Final returned optional params: {'temperature': 0.6, 'stream': False, 'extra_body': {}, 'reasoning': {'enabled': True}}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:489 - self.optional_params: {'temperature': 0.6, 'stream': False, 'reasoning': {'enabled': True}}\n",
      "DEBUG:LiteLLM:self.optional_params: {'temperature': 0.6, 'stream': False, 'reasoning': {'enabled': True}}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:4828 - checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: utils.py:5078 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "DEBUG:LiteLLM:Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: main.py:928 - Error getting model info: This model isn't mapped yet. model=x-ai/grok-4.1-fast, custom_llm_provider=openrouter. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
      "DEBUG:LiteLLM:Error getting model info: This model isn't mapped yet. model=x-ai/grok-4.1-fast, custom_llm_provider=openrouter. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:962 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://openrouter.ai/api/v1/chat/completions \\\n",
      "-H 'HTTP-Referer: https://litellm.ai' -H 'X-Title: liteLLM' -H 'User-Agent: Agents/Python 0.5.1' -H 'Authorization: Be****1b' -H 'Content-Type: application/json' \\\n",
      "-d '{'model': 'x-ai/grok-4.1-fast', 'messages': [{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], 'temperature': 0.6, 'stream': False, 'reasoning': {'enabled': True}, 'usage': {'include': True}}'\n",
      "\u001b[0m\n",
      "\n",
      "DEBUG:LiteLLM:\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://openrouter.ai/api/v1/chat/completions \\\n",
      "-H 'HTTP-Referer: https://litellm.ai' -H 'X-Title: liteLLM' -H 'User-Agent: Agents/Python 0.5.1' -H 'Authorization: Be****1b' -H 'Content-Type: application/json' \\\n",
      "-d '{'model': 'x-ai/grok-4.1-fast', 'messages': [{'content': 'You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\\n\\nCRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\\n\\nYou will be provided with findings from multiple agents:\\n<findings>\\n{organized}\\n</findings>\\n\\nAGGREGATION GUIDELINES:\\n\\n1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\\n   - Look for overlapping line numbers and similar descriptions\\n   - When multiple agents flag the same problem, merge into one issue\\n   - Use the HIGHEST severity when merging\\n\\n2. MULTI-FILE AWARENESS (CRITICAL):\\n   - If findings span multiple files, check for cross-file dependencies\\n   - Flag if changes in one file might break APIs/contracts in another file\\n   - Look for patterns like: \"File A removes function X, but does File B call it?\"\\n   - Consider the bigger picture: Do these changes work together?\\n\\n3. PRESERVE INFORMATION: \\n   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\\n   - Include file paths and line numbers\\n   - Maintain the most comprehensive description from merged findings\\n\\n4. CATEGORIZE each issue as:\\n   - Bug: Logic errors, crashes, incorrect behavior  \\n   - Security: Vulnerabilities, unsafe code\\n   - Performance: Inefficient algorithms, resource issues\\n   - Style: Naming, formatting, documentation\\n   - Test Gap: Missing test coverage\\n\\n5. CREATE SUMMARY TABLE with these columns:\\n   | Issue | File | Lines | Severity | Category | Fix | Found By |\\n\\n6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\\n\\nPresent your report in this format:\\n\\n# Code Review Report\\n\\n## Executive Summary\\n[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n[One row per unique issue]\\n\\n**Total Distinct Issues: [count]**\\n\\nCRITICAL REQUIREMENT: \\n- EVERY finding from EVERY agent must appear in the summary table\\n- This includes ALL test coverage gaps reported by the Test Coverage agent\\n- Test gaps should be listed as separate rows (one per function needing tests)\\n- Do NOT omit any findings, especially test coverage gaps\\n- The Total Distinct Issues count must match the number of rows in the table.', 'role': 'system'}, {'role': 'user', 'content': 'Aggregate these findings into a structured report:\\n\\n{\\'code_review/agents.py\\': [BestPracticeFinding(title=\\'Insufficient Docstring Detail\\', description=\"The new function \\'new_helper_function\\' has a docstring that is too brief and does not follow PEP 257 conventions. It should provide a summary line, followed by a blank line, and then a detailed explanation of the function\\'s purpose, parameters, and return value.\", severity=3, file=\\'code_review/agents.py\\', relevant_lines=[11, 12, 13], suggested_fix=\\'Expand the docstring to include a summary line, a blank line, and a detailed description. For example:\\\\n\\\\n\"\"\"Process the given data object.\\\\n\\\\nThis function calls the process method of the data object and returns the result.\\\\n\\\\nArgs:\\\\n    data: An object with a process() method.\\\\n\\\\nReturns:\\\\n    The result of data.process().\\\\n\"\"\"\\'), TestGap(function_name=\\'new_helper_function\\', file=\\'code_review/agents.py\\', lines=[11, 12, 13, 14], missing_scenarios=[\\'normal case: valid data object with a process method\\', \\'error handling: data object missing process method (AttributeError)\\', \\'error handling: data.process() raises an exception\\', \\'edge case: data is None or invalid type\\'], priority=7, suggested_test_approach=\\'unit test\\')]}'}], 'temperature': 0.6, 'stream': False, 'reasoning': {'enabled': True}, 'usage': {'include': True}}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1035 - RAW RESPONSE:\n",
      "<coroutine object BaseLLMHTTPHandler.async_completion at 0x11878e8c0>\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "<coroutine object BaseLLMHTTPHandler.async_completion at 0x11878e8c0>\n",
      "\n",
      "\n",
      "\u001b[92m15:00:26 - LiteLLM:DEBUG\u001b[0m: llm_http_handler.py:269 - Creating HTTP client with shared_session: None\n",
      "DEBUG:LiteLLM:Creating HTTP client with shared_session: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c7a4e0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x111a9da50> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c7adb0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Sun, 23 Nov 2025 14:00:30 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_559336381ebbc6967b7d402150c365da'), (b'openai-processing-ms', b'124'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-envoy-upstream-service-time', b'127'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a3131760ea8b825-VIE'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai.agents:Exported 4 items\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1035 - RAW RESPONSE:\n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "{\"id\":\"gen-1763906426-QHOk3ya1qG0wf0dDyIwQ\",\"provider\":\"xAI\",\"model\":\"x-ai/grok-4.1-fast\",\"object\":\"chat.completion\",\"created\":1763906426,\"choices\":[{\"logprobs\":null,\"finish_reason\":\"stop\",\"native_finish_reason\":\"completed\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"# Code Review Report\\n\\n## Executive Summary\\nThe proposed changes in `code_review/agents.py` introduce a new helper function with insufficient docstring detail that violates PEP 257 conventions. Additionally, there are critical test coverage gaps for this function, lacking scenarios for normal operation, error handling, and edge cases. No cross-file dependencies or multi-file issues detected.\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n| Insufficient Docstring Detail | code_review/agents.py | 11-13 | 3 | Style | Expand the docstring to include a summary line, a blank line, and a detailed description. For example:<br><br>\\\"\\\"\\\"<br>Process the given data object.<br><br>This function calls the process method of the data object and returns the result.<br><br>Args:<br>    data: An object with a process() method.<br><br>Returns:<br>    The result of data.process().<br>\\\"\\\"\\\" | Best Practices |\\n| Missing tests for new_helper_function | code_review/agents.py | 11-14 | 7 | Test Gap | Add unit tests covering: normal case: valid data object with a process method; error handling: data object missing process method (AttributeError); error handling: data.process() raises an exception; edge case: data is None or invalid type. Suggested test approach: unit test. | Test Coverage |\\n\\n**Total Distinct Issues: 2**\",\"refusal\":null,\"reasoning\":null,\"reasoning_details\":[{\"id\":\"rs_7f3eed31-82e3-374d-d358-19f63f685f35\",\"format\":\"xai-responses-v1\",\"index\":0,\"type\":\"reasoning.encrypted\",\"data\":\"rs0UbGGGl8vVWWyYanRRgJdJ2ZpG/Ns5lA8F3mCOlnbg1WWZtKweQZhe7HtkSSX4y+tIK+bovjYLQrYRLle4RsIetqI5aWlDgWqMyHUy2jOPyFeVYYrIxhLtvBqN2owYcJgKdeHFmYfyZMSTGh4hr4uFI0MX7KdJyd07DePimcEar6jjHXevQFzMjG67cEurQFBtLOSR4gICU5cMfVHRJbpnmxs7VdYzH/csl8Vx82gpnb4p39IPXTdb4Q/yFSyah1SEEE3B1e8UplAVNASt6z5gJ/AOelX3mNQLyz+gE5SQ3kHSoDyWvd0HxgFmz22GWHjQFn5HTj6jhwH8ZIIYfkkd+fXFpNzliTKllkZx6WJTf08oExawi5Rj1sFbAhr+1lh+uj1apSfuwhBNemNH4r10nrij2RuJycwWIZdFXtJLNw3XV02n2u0Q7b1Sm/0l5Lt8yXXBc00UjMPcJu2Wa8cf0tsfoUgg5D2aDKyPa1qN9xob6bAx9gFtxggcVR4HgqCobkvAVt/T34JSYkUBjElPILrxec9xg4pui8UpMoq+CCUkajpvGetr9ZQlUw7Mi4zQbiP9zqlMAl3xIUJeOjN0rOca8vd1arB90NFyUeDWJbN3a6JjgRAWsOAZR23+7cTKlzaAkIUfeMOiC8yayIvLiwBFrRk0EbquqaqnzMRLf5bGUS6wjy5MFTC9QKBQQRX6p8Zz2LtTJ22rwiWlTczRDDmZ+Lf0+Mnz8ScWW++9q9dXcPdOChYDwhvhpOKL19Nvwwmn3tKcmPo2pgg8i5qx+NzDOqbd5hVsQUOcBlBtjr5pVGj4hKmJPDb9+YaJ4s2QMzEKVs+XWm9nHT2i3qHNfVbCpRJ2VKAHjrUWn9MuNwtbLflNaFu+ecSPYn7Ckdlge6GF4AuW9RvEKBNdT99cookmk9iVIPOMOX8EhKkSwBy+8k5V9h9Z1ZDaayBMbIGtzxZ7TX+6haLRmsnS+uJPbwEomOpaArSxFl29V0neRnkaTPJom2ydhia+5eq7YU6tIBTeX7w0EW70Vx2MKrLJrnO4/6NFCMFGCn4AJhXpu3uAuGNRNXoKc4zzGjGA69RsV6K10312sj+aT8D3ObnKqaZCzZU7weSrtZ3gU30A/cOek4kiApMqUsAjnbzvtsFLXZlYKl1DtvRoDj/5jzeoC+x9jGKEmWyJULQK1wd0+AavpjcnWmM7nXfmMx2yENRR//xpCpwqDTgjHyymTdq8ZZvrxJf5ApJYOEE7zaIkmLaEKw8BkktlM/UkzeMKPC/hkYSJsw8qA4s5Btvo2RC/Tu/fy66m4snUJ8r3rA6o0fAXWjbj58AD8hTSEOZehJklAB6+eXn5T5IsJ1M/w0J6pRj/64DNhJhNXKyLBlMqYX93aQ29a4ieD/QXP30N8bjEpQB5DYlzVkRmqDdo6YTQ78CECcmFFpB1Z5IUdSYbVm6CmqT3xA4BJKUbKY0En54wS5wEDYP7Y5PFRJCs5Al2EbM0gIxcTWGiOinoeT7lHkhY4JnvRL3kgQHhj6HfdMR+MvSKOLSpwA5yFcvYhDdGm4n8LoJoOkMz0oj80YfW5zts7A5ng4MAR8ONfwfbP+15XrciM4sklIdhzohZxcapVso5fAFC7BC6kzARytGLcDvM7acey0JBQyi8oL4oTpYuSAKR8njwrMQDlk4i2TbZ2tNXrcMePRHEoZ1mVyPGGAMF5kIwLOGcpLMxlmodMFCOGIaiOKft6MNSqjKWKgRVqHt8LgUxakUG3HJE2n+sUO3TpqbvzNeFYISlvNNrd3lRSP0Pm+qhvRj94YNQsHLo7haP76W5mJupl3JnQSfE9NVyIYRJ63V9V3UW4dEBEAoLFvCs1+VIvNXBsv0vXgvREtRJ2b6TjePgKG3TkKiofYhRCIzwYgJZ3i0QSrKGeZa568XbEaSlBBBexk3QSXNYeRq7Kj2pUphC4b+ibb8Yu/OAHx1encwGqA2N9+awOKwog3V2bwkobxdqEtxUp4ZdRM7iVR0Ixe90oGx5IvahbywlX2UP4WSFNMsecaJtb+x3YQUStelHVLLsC/9FI2w94yVghj61mc9blvVx7BuysOgWhKz4YLG/xdGwoxqElEX9e2jelV2g5nHaRdO34hypVuxfTUSPMzexuUM2yPWsx8AZdoSd8704bZ2RZs8ht9sRk8kZATs5eF8ZkLGxeHFA5kHjeu704nwaAquAk5QElgPt6kMpgZG/UW+ks0f7LI9rbAxciBmkllwWinga44XE914qxkw7qRX3iaJjTAnm8KZGOA+tRijy85lBkXowxxDS1xmWnfu2A3+wKf3/WLisuDYE0D9lVetAWgq8HPiDYwLmLty57lt/NbVLeBbZufC5nWpNT7LN6d2kRby1u6i9Jk/QB0iDZS+U0OMDx1lf0shbEb1ZamQzdG7/bNSueTm2iER23xNYYFFr6XO1m2nZK9tgR2GZ/Oc8Cw2axsjfi/UZBg/PHwkPL6Gdyo0sX6hOlxqD8WRwLsounoIcAUkmbU41Lr1xXGdSviGdQlpUEEMK51xoSdUGPYi+WD4+S8jK+kpCOg5YWxX2jmf5vKvo+p+M5nukeDLRBkbebB4s2M/UrEEnx778z4oX1tHQNw1zb/azmQ/zk+E5704QOP/w/d6wqfnVj7q68gbkQ+AVw0kgu0uc94kGOojFM9TBt7yEr4MjEi946uoIO+J0hMybku3vCH9/yrCNojgFvczfFw0ddTN57moAxtK9jA8NJy+3M13yb7Ibll/tUxWnO0QWjLmG6dB0dtft/j1s6PCUz9EAzTiTebmzjV+gG/EL9L7AFCKt51dv3G8YfzE9wUCQQhx5cuCdz1CHX71PuybTrmqN49K/68FUnWmosw+4ncYghmI5LRQqIkQ1gBH4lcKluQ+dUy2dFdvuLEmVbjWd8RrzN6TTCR5r5qkfDHtSMHTMIE2JysNeHhhIYcBRJwg/1Uu7Y/69/vy4VpBjhSogrhyhl1rivu9DiS6aIr6SYfNHvrMcvHXqmmkR9L9HHuFiwi1DsKJbM+BwsDR2qCywh0dwE4XUWvSifCEuFkTBusnSXAl66IPannDDU6U9GXMqWZ5urnJUuQNJWEh713DW9F6SpOHZ/4Wh46e2dajfdxKijfIH6nj0nMjAiATN/7N+8lQj6eJB9A7lwgpu2nxQwravhebUqXh3D2udkABxg3NkmJKr/Yt9wNQ69NsjyTRZIInaRhdjeqZeq5CdBu6A8eJYkpmNn7X7Ld9dJzxIqyqY+Dfl4Peb5C3nh7UQNCFk8vTHZAQceWBn1AicurIZuoSluQ8czTy25ivbpMRaYPDUKbh1Z3Ioxh8QY6qv3XJd+VqogaCr22utmlvF5blzAONT+MqUM6R9kl9DUcaYFheUyAElCYe7voJYXIjeMYkBk3c0Fuvm/SnjyhBfl8sicpHJ4lHUR9aN/+89KKW5bY/OK95RWqDUx8bEcndlLqo4GOSViSt12AQB5q0+SsCjD10l9hXL8YB/IKRSjOigOT0YM1KcNftieiKuFBX2gaNyz6Tv/qvbEbcu+48DG6xgVwuclSO3/HBa3CRYUdwb7wOnSRnwbFuvz/Ru9Lctp4x6NFb+2tAwkEH55f6RSEdMkzb1WBTVRulo2X0JKcRyGrcH0yFX/yynaNRlr6+b7vuIHw+dRU5G+MNdWJVLwi3ufhBFdSn/q3mPeNdYpzzu3puObklOhMBPozQ8d29Fp2L+f4bnwJahj7M99RNsNe+iiI+OK7rngNFTOzsW/cIruphi7QMrJM8bOPSyWKe0+fjrRiqooTD/hFDcORGbWEM8mEXNEQ0AU6pz0mwRlAjxZIak/gKe0yWmqMfBkVcaXHW0B8paFhDy4Pz4ZZz3siNudI6K3rz4neEMRc55a7/S7gSckmkjq8kI+OKIMkxZS5vVfIUYclybId0qjU5pVJ4CgVSEE78N95Qpl8L6Opck1dPsSVJIMJg/Q7I9/x27salQnm+O9U4r/ifRkg7ZAOHs9/caJtbMh1JTpO8lYzWmMGPlc4SkvgnDtJ8R2kQ0o8z9OmXYkCpZja0XrWff2azp1sZidp36lIpx2TosviDPJxSrkbWGa2v5wgesKoCjKJ4pPLYBX8xQ0XQdknyXD7lsAwV5sRusqBpnnkTiOcT1d953gsz/1Ma2i2JT5+Pmst0g4v/TcpqB4lf2IZ+bAQsPg8kTgKy4nXM/VqIyjyBglCnZs3EidCEF/13zG4hiMVNQHm1f+s/1xmn7GgBjXi69JqP9L3n+Wv0UiXR0Q4c2XzlbJGYAcrtftoNfgQJHqFmyCFCAfKiQ+n+K/LJjg19u7SOIydDTmsoeRXc//rsLxXeNvd0RRjtjMow2408/Q79bE7Y3nRFxI6UAbzo6bZMsbqGVaPNmrhUzByP15GDWhCfJl8b2cM3bNPVh8nCnCjHodwFXuXomSvKSFk+qVgb5sAWfpkskoEXZMaBgM+Yd1lpCZvPkjBDnFcDw9mqK5DV4JMRl30Qq2VFIQlncaQk7gO9e+KuH8ZhuvcdF1fODytJpXyO5yo9+89455fRpgAKCjrYAcOsuwSBUhiZJvK4VonMP7j4lb8VSj/rRXsyDMbiqhII1iO5iKnGXEwZeYHlq1z0F3RNE8BerJKd03LC5aQHfQM9xrGTwLhFLr3y7woxjf3XSc5jC+MFbw08FrWFiHnjeVHAPMpQ9XUBnjeTm+LDh/gjnvFgCtenzLAFsULHTlBFA0AGwpaAx3GUjdhoFl6/+eMGD5b6SdG6kerN1a58Yg8+it/CQ3ytMttsBMHfMX1WuT91v2na2Pwce+cfIczoZuKUXzgho60cTxIUSxTwUqIAr6IoLLpV1urz+ukM0M4e5/Fjqzxfn085+Lp8Uh0+ia6IZVDWTUNAd8c3jw4Ld4zpPIYqh8t76e5xE+fUwyNe0wr5sNTA3mSDSyjCDcTL4Hm4xT1cM2acJsfDXZ4hByXsWvl/RBiO2x06eAZpQmtYgtrZocrqq4ivSFnLqeEEY7+Nh5qZRsVetKZkor+yn67zv5g+HYflj30Uo7GEY3U68H8Zau9k8pm/rpIjD3IbkJpQaW0GjFzbg4SYupDVgEgrYYFju8kZvCPJ3KO3pwPvLEZakwZAzKDxrXJKCPp4yl/s58axocHmDL1mZi1R/v4m65JNzEsnMPlg8AyE9CUhN+f3GFTLZiVvH5eiej7evdoeSxiNcGQvFq0cxxHd+DuR5ZRjoCyHy7X1qzpe3X6PkF1edpYb7o7Aq4rtvc5ykS8F12a1ytxzfT2ZVRXeyquZbjsAnBDrp5a50Xy7LEW8WdICopgniXSZKIPErRo49ev2rNWHztAkZc3yBoDsohjxXuw6f2lq3/Ev4fjj7GsMBfl95M0nVH/1GmU9pmahB9t212WgnJBAe6Neqz9loRqboFbOKm7noVD3W1cKurrFqfxuPduF67sjtEgWeSU+m5r7D8rnAYv6K3xyguWHwBFsgMew7VUUdCGzN1JZxY/Df1cWcNhpRYFRD/ZP9rHTBtg5I8IAoP/2enQVj7+4JXXIsSjJPTodjfzLLTeIWqLX1Lke5HTpnGbRn2xHKNfNfD0E/Q+NQv2jFDg7dgMWVshjFamLJnKnznxUIUJBnTYUHKU5z7O16yT6riVOXNpliCSvU8cU1aZBizA\"}]}}],\"usage\":{\"prompt_tokens\":968,\"completion_tokens\":1310,\"total_tokens\":2278,\"cost\":0,\"is_byok\":false,\"prompt_tokens_details\":{\"cached_tokens\":150,\"audio_tokens\":0,\"video_tokens\":0},\"cost_details\":{\"upstream_inference_cost\":null,\"upstream_inference_prompt_cost\":0,\"upstream_inference_completions_cost\":0},\"completion_tokens_details\":{\"reasoning_tokens\":993,\"image_tokens\":0}}}\n",
      "\n",
      "\n",
      "DEBUG:LiteLLM:RAW RESPONSE:\n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "\n",
      "         \n",
      "{\"id\":\"gen-1763906426-QHOk3ya1qG0wf0dDyIwQ\",\"provider\":\"xAI\",\"model\":\"x-ai/grok-4.1-fast\",\"object\":\"chat.completion\",\"created\":1763906426,\"choices\":[{\"logprobs\":null,\"finish_reason\":\"stop\",\"native_finish_reason\":\"completed\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"# Code Review Report\\n\\n## Executive Summary\\nThe proposed changes in `code_review/agents.py` introduce a new helper function with insufficient docstring detail that violates PEP 257 conventions. Additionally, there are critical test coverage gaps for this function, lacking scenarios for normal operation, error handling, and edge cases. No cross-file dependencies or multi-file issues detected.\\n\\n## Summary of Actions\\n| Issue | File | Lines | Severity | Category | Fix | Found By |\\n|-------|------|-------|----------|----------|-----|----------|\\n| Insufficient Docstring Detail | code_review/agents.py | 11-13 | 3 | Style | Expand the docstring to include a summary line, a blank line, and a detailed description. For example:<br><br>\\\"\\\"\\\"<br>Process the given data object.<br><br>This function calls the process method of the data object and returns the result.<br><br>Args:<br>    data: An object with a process() method.<br><br>Returns:<br>    The result of data.process().<br>\\\"\\\"\\\" | Best Practices |\\n| Missing tests for new_helper_function | code_review/agents.py | 11-14 | 7 | Test Gap | Add unit tests covering: normal case: valid data object with a process method; error handling: data object missing process method (AttributeError); error handling: data.process() raises an exception; edge case: data is None or invalid type. Suggested test approach: unit test. | Test Coverage |\\n\\n**Total Distinct Issues: 2**\",\"refusal\":null,\"reasoning\":null,\"reasoning_details\":[{\"id\":\"rs_7f3eed31-82e3-374d-d358-19f63f685f35\",\"format\":\"xai-responses-v1\",\"index\":0,\"type\":\"reasoning.encrypted\",\"data\":\"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\"}]}}],\"usage\":{\"prompt_tokens\":968,\"completion_tokens\":1310,\"total_tokens\":2278,\"cost\":0,\"is_byok\":false,\"prompt_tokens_details\":{\"cached_tokens\":150,\"audio_tokens\":0,\"video_tokens\":0},\"cost_details\":{\"upstream_inference_cost\":null,\"upstream_inference_prompt_cost\":0,\"upstream_inference_completions_cost\":0},\"completion_tokens_details\":{\"reasoning_tokens\":993,\"image_tokens\":0}}}\n",
      "\n",
      "\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2982 - Filtered callbacks: []\n",
      "DEBUG:LiteLLM:Filtered callbacks: []\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1315 - response_cost: 0.0\n",
      "DEBUG:LiteLLM:response_cost: 0.0\n",
      "DEBUG:openai.agents:Received model response\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x118676420>>\n",
      "DEBUG:LiteLLM:Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x118676420>>\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2982 - Filtered callbacks: []\n",
      "DEBUG:LiteLLM:Filtered callbacks: []\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n",
      "DEBUG:LiteLLM:Logging Details LiteLLM-Async Success Call, cache_hit=None\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4828 - checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:5078 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "DEBUG:LiteLLM:Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:4253 - Model=x-ai/grok-4.1-fast is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "DEBUG:LiteLLM:Model=x-ai/grok-4.1-fast is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:380 - Async success callbacks: Got a complete streaming response\n",
      "DEBUG:LiteLLM:Async success callbacks: Got a complete streaming response\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2294 - Model=x-ai/grok-4.1-fast; cost=0.0\n",
      "DEBUG:LiteLLM:Model=x-ai/grok-4.1-fast; cost=0.0\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4828 - checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "DEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'x-ai/grok-4.1-fast', 'combined_model_name': 'openrouter/x-ai/grok-4.1-fast', 'stripped_model_name': 'x-ai/grok-4.1-fast', 'combined_stripped_model_name': 'openrouter/x-ai/grok-4.1-fast', 'custom_llm_provider': 'openrouter'}\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: utils.py:5078 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "DEBUG:LiteLLM:Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m15:01:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:4253 - Model=x-ai/grok-4.1-fast is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "DEBUG:LiteLLM:Model=x-ai/grok-4.1-fast is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:openai.agents:Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The proposed changes in `code_review/agents.py` introduce a new helper function with insufficient docstring detail that violates PEP 257 conventions. Additionally, there are critical test coverage gaps for this function, lacking scenarios for normal operation, error handling, and edge cases. No cross-file dependencies or multi-file issues detected.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Insufficient Docstring Detail | code_review/agents.py | 11-13 | 3 | Style | Expand the docstring to include a summary line, a blank line, and a detailed description. For example:<br><br>\"\"\"<br>Process the given data object.<br><br>This function calls the process method of the data object and returns the result.<br><br>Args:<br>    data: An object with a process() method.<br><br>Returns:<br>    The result of data.process().<br>\"\"\" | Best Practices |\n",
      "| Missing tests for new_helper_function | code_review/agents.py | 11-14 | 7 | Test Gap | Add unit tests covering: normal case: valid data object with a process method; error handling: data object missing process method (AttributeError); error handling: data.process() raises an exception; edge case: data is None or invalid type. Suggested test approach: unit test. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Check the output above for file_search_call mentions\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11871e870>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x111a9da50> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11890b6b0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Sun, 23 Nov 2025 14:01:07 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_d96a2f1c82327e211c9cd24bbecf7a36'), (b'openai-processing-ms', b'432'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-envoy-upstream-service-time', b'436'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a3132591a5a5a80-VIE'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai.agents:Exported 2 items\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Check if FileSearchTool is being called\n",
    "# Enable verbose output to see tool calls\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "test_diff_explicit = '''\n",
    "diff --git a/code_review/agents.py b/code_review/agents.py\n",
    "index abc123..def456 100644\n",
    "--- a/code_review/agents.py\n",
    "+++ b/code_review/agents.py\n",
    "@@ -10,6 +10,10 @@ from code_review.rag import get_relevant_patterns\n",
    " \n",
    "+def new_helper_function(data):\n",
    "+    \"\"\"Process data\"\"\"\n",
    "+    return data.process()\n",
    "+\n",
    " code_analyzer = Agent(\n",
    "     name=\"Code Analyzer\",\n",
    "'''\n",
    "\n",
    "print(\"üîç Test 3: Checking if FileSearchTool is actually called...\")\n",
    "print(\"=\"*60)\n",
    "print(\"This should trigger codebase search for 'process()' method\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = await review_code(\n",
    "    test_diff_explicit,\n",
    "    save_output=False,\n",
    "    min_severity=1,\n",
    "    vector_store_id=vector_store_id\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Check the output above for file_search_call mentions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "r95bmslxcsp",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai.agents:Creating trace Agent workflow with id trace_0c29b502a7b745c6bde1f478a857e60e\n",
      "DEBUG:openai.agents:Setting current trace: trace_0c29b502a7b745c6bde1f478a857e60e\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.AgentSpanData object at 0x1186bc5f0> with id None\n",
      "DEBUG:openai.agents:Running agent Test Agent (turn 1)\n",
      "DEBUG:openai.agents:Creating span <agents.tracing.span_data.ResponseSpanData object at 0x1186d5b10> with id None\n",
      "DEBUG:openai.agents:Calling LLM\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/responses', 'headers': {'User-Agent': 'Agents/Python 0.5.1'}, 'files': None, 'idempotency_key': 'stainless-python-retry-ebe6b07d-5034-4732-8d23-488574918b7f', 'json_data': {'include': [], 'input': [{'content': 'Tell me about the review_code function in the codebase.', 'role': 'user'}], 'instructions': \"You MUST use FileSearchTool to search the codebase for information about 'review_code' function.\\n\\n    Report back:\\n    1. Where is review_code defined?\\n    2. What files import review_code?\\n    3. What are its parameters?\\n\\n    DO NOT respond without searching. Use the tool first.\", 'model': 'gpt-4.1-mini', 'tools': [{'type': 'file_search', 'vector_store_ids': ['vs_6923134311808191a57eb46b25ede7c4'], 'max_num_results': 5}]}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/responses\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118c78650>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1189331d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x118a6ff50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Test 4: Force FileSearchTool usage...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118a9d280>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x111a9da50> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c80560>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Sun, 23 Nov 2025 14:08:36 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_b234346ba662340e8d3c624f28ec6373'), (b'openai-processing-ms', b'164'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-envoy-upstream-service-time', b'172'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=1zer1J8i8NbvbUhDdlx3feho_MoijsJ3OmyOeXSzGnE-1763906916-1.0.1.1-TrDPr6niYadgXsORHh0mplLlJuykHvmcax1dGh1Z539GzcxJwKoSDEnh_ypl7nqbo4Og3YiU8vnz8fvPPaZ66Ni4nOIcn1nDk6a8UL48j8o; path=/; expires=Sun, 23-Nov-25 14:38:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a313d4f9dc95b0f-VIE'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai.agents:Exported 1 items\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:08:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'195242'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'1.427s'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_10a05aafd2dc48feb1784232d1a01b6e'), (b'openai-processing-ms', b'18780'), (b'x-envoy-upstream-service-time', b'18784'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=vW4Do8gj6OogbgPr1KPwuTewYnxv4Q1XqNhjoTR1Wv0-1763906933-1.0.1.1-lTUMiKF.F1_xsBrfuZNB9nsaMrqKDgw3axxMOIc.MkDwtGBp2efWWZdPmcx1v2AUQUFEavzw2pYLDCM4ecMM4mJ9tkyWs.g1ga8eqctr80U; path=/; expires=Sun, 23-Nov-25 14:38:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a313d49482651d5-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/responses \"200 OK\" Headers({'date': 'Sun, 23 Nov 2025 14:08:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '195242', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '1.427s', 'openai-version': '2020-10-01', 'openai-organization': 'user-mtsmaztj6ekr3iwkmr0v9erf', 'openai-project': 'proj_c4meB9EhtGMyT5bmrzRVGYzB', 'x-request-id': 'req_10a05aafd2dc48feb1784232d1a01b6e', 'openai-processing-ms': '18780', 'x-envoy-upstream-service-time': '18784', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=vW4Do8gj6OogbgPr1KPwuTewYnxv4Q1XqNhjoTR1Wv0-1763906933-1.0.1.1-lTUMiKF.F1_xsBrfuZNB9nsaMrqKDgw3axxMOIc.MkDwtGBp2efWWZdPmcx1v2AUQUFEavzw2pYLDCM4ecMM4mJ9tkyWs.g1ga8eqctr80U; path=/; expires=Sun, 23-Nov-25 14:38:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9a313d49482651d5-VIE', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_10a05aafd2dc48feb1784232d1a01b6e\n",
      "DEBUG:openai.agents:LLM responded\n",
      "DEBUG:openai.agents:Processing output item type=file_search_call class=ResponseFileSearchToolCall\n",
      "DEBUG:openai.agents:Processing output item type=message class=ResponseOutputMessage\n",
      "DEBUG:openai.agents:Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Here is what I found about the `review_code` function in the codebase:\n",
      "\n",
      "1. **Where is `review_code` defined?**  \n",
      "   It is defined in the file **pipeline.py**.\n",
      "\n",
      "2. **What files import `review_code`?**  \n",
      "   It is imported and used in **review_pr.py**.\n",
      "\n",
      "3. **What are its parameters?**  \n",
      "   The function definition is:\n",
      "   ```python\n",
      "   async def review_code(diff: str, save_output: bool = True, min_severity: int = 5) -> str:\n",
      "   ```\n",
      "   - `diff` (str): The code diff to review.  \n",
      "   - `save_output` (bool, default True): Whether to save the review report to file.  \n",
      "   - `min_severity` (int, default 5): Minimum severity threshold (1-10). Findings below this threshold are filtered out.\n",
      "\n",
      "4. **What does it do?**  \n",
      "   - It runs a multi-agent code review pipeline on the given code diff asynchronously.  \n",
      "   - First, it runs multiple agents analyzing the code (code analysis, security, best practices, test coverage) simultaneously.  \n",
      "   - It filters out findings below the given severity threshold.  \n",
      "   - It organizes the findings by file.  \n",
      "   - If there are no findings meeting the threshold, it returns a clean report message.  \n",
      "   - Otherwise, it aggregates the findings into a structured markdown report.  \n",
      "   - Optionally, it saves the report to a timestamped file in `user-data/`.  \n",
      "   - Finally, it returns the markdown-formatted code review report as a string.\n",
      "\n",
      "5. **Additional details:**  \n",
      "   The function uses `trace` to mark the operation scope and calls internal functions such as `run_all_agents(diff)` and `organize_findings(...)`. The aggregated report is printed and conditionally saved.\n",
      "\n",
      "This is a relevant excerpt showing the full definition and docstring:\n",
      "```python\n",
      "async def review_code(diff: str, save_output: bool = True, min_severity: int = 5) -> str:\n",
      "    \"\"\"Complete code review pipeline\n",
      "\n",
      "    Args:\n",
      "        diff: The code diff to review\n",
      "        min_severity: Minimum severity threshold (1-10). Findings below this are filtered out. (default: 5)\n",
      "\n",
      "    Returns:\n",
      "        Markdown-formatted code review report\n",
      "    \"\"\"\n",
      "    with trace(\"Multi-Agent Code Review\"):\n",
      "        results = await run_all_agents(diff)\n",
      "        code_result, security_result, best_practices_result, test_coverage_result = results\n",
      "\n",
      "        # Filter findings by severity threshold\n",
      "        def filter_by_severity(result):\n",
      "            filtered_findings = [\n",
      "                finding for finding in result.final_output.findings\n",
      "                if getattr(finding, 'severity', getattr(finding, 'priority', 0)) >= min_severity\n",
      "            ]\n",
      "            result.final_output.findings = filtered_findings\n",
      "            return result\n",
      "\n",
      "        code_result = filter_by_severity(code_result)\n",
      "        security_result = filter_by_severity(security_result)\n",
      "        best_practices_result = filter_by_severity(best_practices_result)\n",
      "        test_coverage_result = filter_by_severity(test_coverage_result)\n",
      "\n",
      "        organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
      "\n",
      "        # If all findings were filtered out, return early with a clean report\n",
      "        if not any(organized.values()):\n",
      "            clean_report = \"No issues found meeting severity threshold.\\n\"\n",
      "            print(clean_report)\n",
      "            return clean_report\n",
      "\n",
      "        result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
      "        report = result.final_output\n",
      "\n",
      "        print(report)\n",
      "\n",
      "        if save_output:\n",
      "            os.makedirs(\"user-data\", exist_ok=True)\n",
      "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
      "            filepath = f\"user-data/code_review_{timestamp}.md\"\n",
      "            with open(filepath, \"w\") as f:\n",
      "                f.write(report)\n",
      "            print(f\"Report saved to {filepath}\")\n",
      "\n",
      "        return report\n",
      "```\n",
      "\n",
      "And an example import usage in `review_pr.py`:\n",
      "```python\n",
      "from code_review import review_code\n",
      "...\n",
      "report = await review_code(diff, save_output=False, min_severity=min_severity)\n",
      "```\n",
      "\n",
      "This covers its definition, usage, and parameters comprehensively.\n",
      "\n",
      "============================================================\n",
      "‚úÖ If you see file paths and details, the tool was called!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c6e180>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x111a9da50> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118a9d010>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 204, b'No Content', [(b'Date', b'Sun, 23 Nov 2025 14:08:57 GMT'), (b'Connection', b'keep-alive'), (b'x-request-id', b'req_0269f028311ac614cfbfb3a2337fd09c'), (b'openai-processing-ms', b'107'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-envoy-upstream-service-time', b'110'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a313dd39e9928d0-VIE'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai.agents:Exported 2 items\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Force tool usage by making it necessary\n",
    "# Modify agent instructions to explicitly require codebase search\n",
    "\n",
    "from agents import Agent, FileSearchTool, Runner\n",
    "\n",
    "# Create a test agent that MUST use FileSearchTool\n",
    "test_agent = Agent(\n",
    "    name=\"Test Agent\",\n",
    "    instructions=\"\"\"You MUST use FileSearchTool to search the codebase for information about 'review_code' function.\n",
    "    \n",
    "    Report back:\n",
    "    1. Where is review_code defined?\n",
    "    2. What files import review_code?\n",
    "    3. What are its parameters?\n",
    "    \n",
    "    DO NOT respond without searching. Use the tool first.\"\"\",\n",
    "    tools=[\n",
    "        FileSearchTool(\n",
    "            max_num_results=5,\n",
    "            vector_store_ids=[vector_store_id]\n",
    "        )\n",
    "    ],\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")\n",
    "\n",
    "print(\"üß™ Test 4: Force FileSearchTool usage...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = await Runner.run(test_agent, \"Tell me about the review_code function in the codebase.\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(result.final_output)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ If you see file paths and details, the tool was called!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbmxplorwf6",
   "metadata": {},
   "source": [
    "## Cleanup Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dflcvivr2xw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'delete', 'url': '/vector_stores/vs_6923134311808191a57eb46b25ede7c4', 'headers': {'OpenAI-Beta': 'assistants=v2'}, 'idempotency_key': 'stainless-python-retry-2e82b188-ba0f-488c-8d0b-a62ff4e85450', 'json_data': None}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: DELETE https://api.openai.com/v1/vector_stores/vs_6923134311808191a57eb46b25ede7c4\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c782c0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x118c96550> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118c3bc50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'DELETE']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'DELETE']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'DELETE']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 23 Nov 2025 14:10:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-version', b'2020-10-01'), (b'openai-organization', b'user-mtsmaztj6ekr3iwkmr0v9erf'), (b'openai-project', b'proj_c4meB9EhtGMyT5bmrzRVGYzB'), (b'x-request-id', b'req_4864091e936c9844f5435e60d852342a'), (b'openai-processing-ms', b'1115'), (b'x-envoy-upstream-service-time', b'1117'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=G7T1EzgB2e6XY7.vyPJK5dFafArap6UslP3xZfRIZV0-1763907012-1.0.1.1-AEihuCh49_p8Q6Q2WAZY_EbFZx8jVrOcOwx1BWX9v54DMlMUr0eDMhWhUunOyJNKJQcLBBi6VyArwFYipAshZ5cPfhj6gJshPvG.H5_pP6Y; path=/; expires=Sun, 23-Nov-25 14:40:12 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=fSkFsqEtBohGjvxoxlYf_cE30AgsrXb2dBDyOhJQYzc-1763907012771-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9a313fa5dac6c1f9-VIE'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'DELETE']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: DELETE https://api.openai.com/v1/vector_stores/vs_6923134311808191a57eb46b25ede7c4 \"200 OK\" Headers([('date', 'Sun, 23 Nov 2025 14:10:12 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-version', '2020-10-01'), ('openai-organization', 'user-mtsmaztj6ekr3iwkmr0v9erf'), ('openai-project', 'proj_c4meB9EhtGMyT5bmrzRVGYzB'), ('x-request-id', 'req_4864091e936c9844f5435e60d852342a'), ('openai-processing-ms', '1115'), ('x-envoy-upstream-service-time', '1117'), ('x-openai-proxy-wasm', 'v0.1'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=G7T1EzgB2e6XY7.vyPJK5dFafArap6UslP3xZfRIZV0-1763907012-1.0.1.1-AEihuCh49_p8Q6Q2WAZY_EbFZx8jVrOcOwx1BWX9v54DMlMUr0eDMhWhUunOyJNKJQcLBBi6VyArwFYipAshZ5cPfhj6gJshPvG.H5_pP6Y; path=/; expires=Sun, 23-Nov-25 14:40:12 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=fSkFsqEtBohGjvxoxlYf_cE30AgsrXb2dBDyOhJQYzc-1763907012771-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9a313fa5dac6c1f9-VIE'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_4864091e936c9844f5435e60d852342a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deleted vector store: vs_6923134311808191a57eb46b25ede7c4\n"
     ]
    }
   ],
   "source": [
    "# Optional: Delete vector store manually (otherwise auto-expires in 1 day)\n",
    "\n",
    "def delete_vector_store(vector_store_id: str):\n",
    "    \"\"\"Delete a vector store\"\"\"\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    try:\n",
    "        client.vector_stores.delete(vector_store_id)\n",
    "        print(f\"‚úÖ Deleted vector store: {vector_store_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to delete: {e}\")\n",
    "\n",
    "# Uncomment to delete:\n",
    "delete_vector_store(vector_store_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc06abb",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c495b5",
   "metadata": {},
   "source": [
    "## Synthetic Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c351b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"You are an evaluation judge for code review systems comparing expected findings (ground truth) against actual findings.\n",
    "\n",
    "CRITICAL MATCHING RULES:\n",
    "1. Each actual finding can match AT MOST ONE expected finding\n",
    "2. Each expected finding can match AT MOST ONE actual finding\n",
    "3. Once an actual finding is matched, it CANNOT be used again\n",
    "4. Only match within same category (bugs ‚â† test gaps)\n",
    "\n",
    "PROCESS:\n",
    "1. Count total_actual from \"Total Distinct Issues: X\" in report\n",
    "2. For EACH expected finding:\n",
    "   - Find the BEST matching actual finding that hasn't been used yet\n",
    "   - If good match exists: mark as matched=True, record which actual finding\n",
    "   - If no match: mark as matched=False\n",
    "   - NEVER reuse an actual finding for multiple expected findings\n",
    "\n",
    "A match means the same type of issue was identified, even if worded differently.\n",
    "\"\"\"\n",
    "\n",
    "class MatchedFinding(BaseModel):\n",
    "    expected: str = Field(description=\"the expected finding text\")\n",
    "    matched: bool = Field(description=\"true if the expected finding is present, else false\")\n",
    "    actual_finding: Optional[str] = Field(default=None, description=\"the matching text from report (if matched)\")\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    matched_findings: list[MatchedFinding]\n",
    "    total_expected: int = Field(description=\"Total number of expected findings from ground truth\")\n",
    "    total_actual: int = Field(description=\"Count of distinct issues in the report's summary section\")\n",
    "    # matches: int = Field(description=\"Number of expected findings successfully matched\")\n",
    "    \n",
    "    def model_post_init(self, __context):\n",
    "        # Calculate matches from the list\n",
    "        matches = sum(1 for mf in self.matched_findings if mf.matched)\n",
    "        \n",
    "        # Check for duplicate actual findings\n",
    "        actual_findings_used = [\n",
    "            mf.actual_finding for mf in self.matched_findings \n",
    "            if mf.matched and mf.actual_finding\n",
    "        ]\n",
    "        unique_actuals = len(set(actual_findings_used))\n",
    "        \n",
    "        if matches > unique_actuals:\n",
    "            print(f\"ERROR: {matches} matches but only {unique_actuals} unique actual findings used!\")\n",
    "            print(\"The judge matched the same actual finding multiple times.\")\n",
    "        \n",
    "        if matches > self.total_actual:\n",
    "            print(f\"WARNING: Matches ({matches}) > Total Actual ({self.total_actual})\")\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_report(report: str, ground_truth_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixed evaluation function with proper counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        name=\"Evaluation Judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"gpt-5.1\",\n",
    "        output_type=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "GROUND TRUTH (expected findings):\n",
    "{ground_truth_content}\n",
    "\n",
    "ACTUAL REPORT (what the system found):\n",
    "{report}\n",
    "\n",
    "For each expected finding, determine if it matches any actual finding.\n",
    "Output matched_findings list, total_expected, and total_actual.\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(judge_agent, prompt)\n",
    "    eval_result = result.final_output\n",
    "    \n",
    "    # Calculate matches from the actual data - don't trust LLM counting\n",
    "    matches = sum(1 for mf in eval_result.matched_findings if mf.matched)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = matches / eval_result.total_expected if eval_result.total_expected > 0 else 0\n",
    "    precision = matches / eval_result.total_actual if eval_result.total_actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"matches\": matches,\n",
    "        \"total_expected\": eval_result.total_expected,\n",
    "        \"total_actual\": eval_result.total_actual,\n",
    "        \"details\": eval_result.matched_findings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test 2\n",
    "test_dir = Path(\"test-cases\")\n",
    "diff_file = test_dir / \"01_sql_injection.diff\"\n",
    "\n",
    "# Load files\n",
    "diff_content = diff_file.read_text()\n",
    "expected_file = diff_file.with_name(\"01_sql_injection_expected.json\")\n",
    "ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# Run review WITH saving\n",
    "report = await review_code(diff_content, save_output=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_result = await evaluate_report(report, ground_truth_content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JUDGE OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "print(f\"matches: {eval_result['matches']}\")\n",
    "print(f\"\\nmatched_findings:\")\n",
    "for mf in eval_result['details']:\n",
    "    print(f\"\\n  Expected: {mf.expected}\")\n",
    "    print(f\"  Matched: {mf.matched}\")\n",
    "    if mf.actual_finding:\n",
    "        print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATED METRICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all test cases\n",
    "\n",
    "test_cases = [\n",
    "    \"01_sql_injection\",\n",
    "    \"02_logic_bug\",\n",
    "    \"03_code_quality\",\n",
    "    \"04_multi_file_security\",\n",
    "    \"05_multi_file_mixed\"\n",
    "]\n",
    "\n",
    "async def run_all_tests():\n",
    "    test_dir = Path(\"test-cases\")\n",
    "    results = []\n",
    "    \n",
    "    for test_name in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {test_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load files\n",
    "        diff_file = test_dir / f\"{test_name}.diff\"\n",
    "        diff_content = diff_file.read_text()\n",
    "        expected_file = test_dir / f\"{test_name}_expected.json\"\n",
    "        ground_truth_content = expected_file.read_text()\n",
    "        \n",
    "        # Run review\n",
    "        report = await review_code(diff_content, save_output=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = await evaluate_report(report, ground_truth_content)\n",
    "        \n",
    "        # Print detailed judge output\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"JUDGE OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "        print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "        print(f\"matches: {eval_result['matches']}\")\n",
    "        print(f\"\\nmatched_findings:\")\n",
    "        for mf in eval_result['details']:\n",
    "            print(f\"\\n  Expected: {mf.expected}\")\n",
    "            print(f\"  Matched: {mf.matched}\")\n",
    "            if mf.actual_finding:\n",
    "                print(f\"  Actual: {mf.actual_finding[:100]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'recall': eval_result['recall'],\n",
    "            'precision': eval_result['precision'],\n",
    "            'f1': eval_result['f1'],\n",
    "            'passed': eval_result['recall'] >= 0.80 and \n",
    "                     eval_result['precision'] >= 0.85 and \n",
    "                     eval_result['f1'] >= 0.82\n",
    "        })\n",
    "        \n",
    "        # Print calculated metrics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATED METRICS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "        print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "        print(f\"F1 Score: {eval_result['f1']:.2f}\")\n",
    "        print(f\"Status: {'‚úì PASSED' if results[-1]['passed'] else '‚úó FAILED'}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        status = '‚úì' if result['passed'] else '‚úó'\n",
    "        print(f\"{status} {result['test_name']}: R={result['recall']:.2f} P={result['precision']:.2f} F1={result['f1']:.2f}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r['passed'])\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run all tests\n",
    "results = await run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580248",
   "metadata": {},
   "source": [
    "## BugsInPy Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation (Option 4): Models and Utilities\n",
    "import re\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa90b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation Function\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid(report: str, bug_patch: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation: Location metrics (automated) + LLM relevance (semantic).\n",
    "    \n",
    "    Stage 1: Calculate automated location overlap\n",
    "    Stage 2: If file_recall > 0, use LLM to judge semantic relevance\n",
    "    \n",
    "    Returns:\n",
    "        dict with file_recall, line_precision, line_recall, llm_relevance, composite_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(bug_patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance (only if there's file overlap)\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_instructions = \"\"\"You are evaluating the semantic relevance of code review findings to an actual bug fix.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. ACTUAL FIX PATCH: The changes that were made to fix bugs\n",
    "2. CODE REVIEW REPORT: What the review system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the bugs that were fixed\n",
    "- 0.7-0.9: Findings flag related issues that would lead to discovering the bugs\n",
    "- 0.4-0.6: Findings flag the general area but miss specific bugs\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Be objective and strict in your assessment.\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"Relevance Judge\",\n",
    "            instructions=llm_judge_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{bug_patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to the actual fix.\n",
    "\"\"\"\n",
    "        with trace(\"LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test with 10 bugs - showing what agents missed\n",
    "\n",
    "async def test_bugsinpy_with_miss_analysis(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with detailed miss analysis.\n",
    "    Shows what the agents caught vs. what they missed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            print(\"\\nACTUAL FIX (first 500 chars):\")\n",
    "            print(bug_patch[:500])\n",
    "            print(\"...\" if len(bug_patch) > 500 else \"\")\n",
    "            \n",
    "            # Reverse diff\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Parse locations to show what was missed\n",
    "            actual_locations = parse_changed_locations(bug_patch)\n",
    "            flagged_locations = parse_flagged_locations(report)\n",
    "            \n",
    "            # Find missed files\n",
    "            missed_files = actual_locations['files'] - flagged_locations['files']\n",
    "            \n",
    "            # Find missed line ranges\n",
    "            missed_lines = {}\n",
    "            for file in actual_locations['files']:\n",
    "                actual_lines = actual_locations['lines'].get(file, set())\n",
    "                flagged_lines_in_file = flagged_locations['lines'].get(file, set())\n",
    "                \n",
    "                # Lines that weren't caught (no flagged line within 5 lines)\n",
    "                uncaught = []\n",
    "                for actual_line in actual_lines:\n",
    "                    if not any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                        uncaught.append(actual_line)\n",
    "                \n",
    "                if uncaught:\n",
    "                    missed_lines[file] = sorted(uncaught)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60,\n",
    "                'missed_files': list(missed_files),\n",
    "                'missed_lines': missed_lines\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìç LOCATION METRICS:\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.0%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.0%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.0%}\")\n",
    "            print(f\"\\nü§ñ LLM RELEVANCE: {eval_result['llm_relevance']:.0%}\")\n",
    "            print(f\"üéØ COMPOSITE: {eval_result['composite_score']:.0%}\")\n",
    "            \n",
    "            # Show what was missed\n",
    "            if missed_files:\n",
    "                print(f\"\\n‚ùå MISSED FILES: {', '.join(missed_files)}\")\n",
    "            \n",
    "            if missed_lines:\n",
    "                print(f\"\\n‚ùå MISSED LINES:\")\n",
    "                for file, lines in missed_lines.items():\n",
    "                    line_ranges = []\n",
    "                    start = lines[0]\n",
    "                    end = start\n",
    "                    for i in range(1, len(lines)):\n",
    "                        if lines[i] == end + 1:\n",
    "                            end = lines[i]\n",
    "                        else:\n",
    "                            line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                            start = lines[i]\n",
    "                            end = start\n",
    "                    line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                    print(f\"  {file}: lines {', '.join(line_ranges)}\")\n",
    "            \n",
    "            print(f\"\\n{'‚úì PASSED' if result['passed'] else '‚úó FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"‚úó {result['project']}/{result['bug_id']}: ERROR\")\n",
    "        else:\n",
    "            status = '‚úì' if result['passed'] else '‚úó'\n",
    "            missed_info = \"\"\n",
    "            if result['missed_files']:\n",
    "                missed_info += f\" | Missed files: {len(result['missed_files'])}\"\n",
    "            if result['missed_lines']:\n",
    "                total_missed = sum(len(lines) for lines in result['missed_lines'].values())\n",
    "                missed_info += f\" | Missed lines: {total_missed}\"\n",
    "            \n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"Composite={result['composite_score']:.0%} \"\n",
    "                  f\"(LineRec={result['line_recall']:.0%}, LLM={result['llm_relevance']:.0%})\"\n",
    "                  f\"{missed_info}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)} ({passed/len(results):.0%})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 20 diverse bugs - different projects not yet tested\n",
    "bugs_to_test_20_diverse = [\n",
    "    (\"scrapy\", 2),        # Web scraping framework\n",
    "    (\"ansible\", 2),       # Automation tool\n",
    "    (\"pytest\", 2),        # Testing framework\n",
    "    (\"sanic\", 2),         # Async web framework\n",
    "    (\"spacy\", 2),         # NLP library\n",
    "    (\"youtube-dl\", 2),    # Video downloader\n",
    "    (\"thefuck\", 2),       # Command corrector\n",
    "    (\"luigi\", 4),         # Pipeline framework\n",
    "    (\"black\", 2),         # Code formatter\n",
    "    (\"pandas\", 3),        # Data analysis\n",
    "    (\"keras\", 3),         # ML framework\n",
    "    (\"matplotlib\", 2),    # Plotting library\n",
    "    (\"tornado\", 2),       # Async networking\n",
    "    (\"tqdm\", 2),          # Progress bar\n",
    "    (\"httpie\", 2),        # HTTP client\n",
    "    (\"cookiecutter\", 2),  # Project templating\n",
    "    (\"fastapi\", 2),       # API framework\n",
    "    (\"scrapy\", 3),        # More scrapy\n",
    "    (\"ansible\", 3),       # More ansible\n",
    "    (\"pytest\", 3),        # More pytest\n",
    "]\n",
    "\n",
    "results = await test_bugsinpy_with_miss_analysis(bugs_to_test_20_diverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641b91b",
   "metadata": {},
   "source": [
    "## CVE Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Utilities\n",
    "import re\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13030d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE Dataset Loading Functions\n",
    "\n",
    "def load_cve_dataset(json_path: str = \"cve_dataset.json\") -> list[dict]:\n",
    "    \"\"\"Load curated CVE dataset\"\"\"\n",
    "    with open(json_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_cve_patch(cve_id: str, patches_dir: str = \"cve_patches\") -> str:\n",
    "    \"\"\"Load patch file for specific CVE\"\"\"\n",
    "    patch_path = Path(patches_dir) / f\"{cve_id}.patch\"\n",
    "    return patch_path.read_text()\n",
    "\n",
    "# Test loading\n",
    "cve_dataset = load_cve_dataset()\n",
    "print(f\"‚úì Loaded {len(cve_dataset)} CVEs\")\n",
    "print(f\"\\nCWE Coverage:\")\n",
    "cwe_counts = {}\n",
    "for cve in cve_dataset:\n",
    "    cwe = cve['cwe_name']\n",
    "    cwe_counts[cwe] = cwe_counts.get(cwe, 0) + 1\n",
    "\n",
    "for cwe, count in sorted(cwe_counts.items()):\n",
    "    print(f\"  {cwe}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h14kceo638o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Helper Functions\n",
    "\n",
    "def check_security_agent_flagged(report: str) -> bool:\n",
    "    \"\"\"Check if Security Agent found anything\"\"\"\n",
    "    return \"Security\" in report and \"Found By\" in report\n",
    "\n",
    "def extract_max_severity(report: str) -> int:\n",
    "    \"\"\"Extract highest severity from report (1-10 scale)\"\"\"\n",
    "    import re\n",
    "    # Parse markdown table for severity column (finds Security findings)\n",
    "    severities = re.findall(r'\\|\\s*(\\d+)\\s*\\|.*\\|\\s*Security\\s*\\|', report, re.IGNORECASE)\n",
    "    return max(map(int, severities)) if severities else 0\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nxhejx209k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE-Specific Evaluation\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid_cve(report: str, patch: str, \n",
    "                               cve_id: str, cwe_id: str, cwe_name: str,\n",
    "                               cvss_score: float, severity: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation for CVEs: Location metrics + LLM relevance + Security detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance with CVE context\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_cve_instructions = f\"\"\"You are evaluating code review findings against a real CVE.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. CVE ID: {cve_id}\n",
    "2. CWE Type: {cwe_name} ({cwe_id})\n",
    "3. CVSS Score: {cvss_score} ({severity})\n",
    "4. ACTUAL FIX PATCH: The changes that fixed the vulnerability\n",
    "5. CODE REVIEW REPORT: What our system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the CVE vulnerability type\n",
    "- 0.7-0.9: Findings flag related security issues that would lead to discovery\n",
    "- 0.4-0.6: Findings flag the general area but miss specific vulnerability\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Special attention:\n",
    "- Did the Security Agent flag this as a security issue?\n",
    "- Is the severity appropriate for the CVE?\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"CVE Relevance Judge\",\n",
    "            instructions=llm_judge_cve_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to this CVE.\n",
    "\"\"\"\n",
    "        with trace(\"CVE LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Stage 3: Security detection check\n",
    "    security_flagged = check_security_agent_flagged(report)\n",
    "    severity_from_report = extract_max_severity(report)\n",
    "    severity_appropriate = abs(severity_from_report - cvss_score) <= 3 if severity_from_report > 0 else False\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score,\n",
    "        'severity_appropriate': severity_appropriate,\n",
    "        'security_finding_present': security_flagged\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "npvow3atsm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: CVE Testing Framework\n",
    "\n",
    "async def test_cve_benchmark(cve_dataset: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test code review system on CVE dataset.\n",
    "    Reuses existing hybrid evaluation with CVE enhancements.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cve in cve_dataset:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {cve['cve_id']} - {cve['cwe_name']}\")\n",
    "        print(f\"CVSS: {cve['cvss_score']} | Project: {cve['project']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load patch\n",
    "            patch = load_cve_patch(cve['cve_id'])\n",
    "            \n",
    "            # Reverse diff (show vulnerability introduction)\n",
    "            reversed_diff = reverse_diff(patch)\n",
    "            \n",
    "            # Run code review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation with CVE context\n",
    "            eval_result = await evaluate_hybrid_cve(\n",
    "                report, patch,\n",
    "                cve['cve_id'], cve['cwe_id'], cve['cwe_name'],\n",
    "                cve['cvss_score'], cve['severity']\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'cwe_id': cve['cwe_id'],\n",
    "                'cwe_name': cve['cwe_name'],\n",
    "                'cvss_score': cve['cvss_score'],\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'security_flagged': eval_result['security_finding_present'],\n",
    "                'severity_appropriate': eval_result['severity_appropriate'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìç Location: FileRec={result['file_recall']:.0%}, LineRec={result['line_recall']:.0%}\")\n",
    "            print(f\"ü§ñ LLM Relevance: {result['llm_relevance']:.0%}\")\n",
    "            print(f\"üõ°Ô∏è  Security Agent: {'‚úì FLAGGED' if result['security_flagged'] else '‚úó MISSED'}\")\n",
    "            print(f\"üìä Composite: {result['composite_score']:.0%} - {'‚úì PASSED' if result['passed'] else '‚úó FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"CVE BENCHMARK SUMMARY\")\n",
    "    print('='*60)\n",
    "    \n",
    "    valid_results = [r for r in results if 'error' not in r]\n",
    "    passed = sum(r['passed'] for r in valid_results)\n",
    "    security_detected = sum(r['security_flagged'] for r in valid_results)\n",
    "    \n",
    "    print(f\"Overall Pass Rate: {passed}/{len(valid_results)} ({passed/len(valid_results):.0%})\")\n",
    "    print(f\"Security Agent Detection: {security_detected}/{len(valid_results)} ({security_detected/len(valid_results):.0%})\")\n",
    "    \n",
    "    # By CWE type\n",
    "    print(f\"\\nüìã Results by CWE Type:\")\n",
    "    cwe_results = {}\n",
    "    for r in valid_results:\n",
    "        cwe = r['cwe_name']\n",
    "        if cwe not in cwe_results:\n",
    "            cwe_results[cwe] = {'total': 0, 'passed': 0}\n",
    "        cwe_results[cwe]['total'] += 1\n",
    "        cwe_results[cwe]['passed'] += r['passed']\n",
    "    \n",
    "    for cwe, stats in sorted(cwe_results.items()):\n",
    "        print(f\"  {cwe}: {stats['passed']}/{stats['total']} passed\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"user-data\", exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"user-data/cve_benchmark_{timestamp}.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nüíæ Results saved to {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì CVE testing framework loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3ikvcg60kl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 6: Run CVE Benchmark\n",
    "\n",
    "# # Load CVE dataset\n",
    "# cve_dataset = load_cve_dataset()\n",
    "# print(f\"Loaded {len(cve_dataset)} CVEs\\n\")\n",
    "\n",
    "# # Run benchmark on all CVEs\n",
    "# results = await test_cve_benchmark(cve_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4sphvor2cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the two problematic CVEs that got stuck\n",
    "\n",
    "problematic_cves = [\"CVE-2024-53908\", \"CVE-2024-23346\"]  # SQL Injection and Code Injection\n",
    "problematic_dataset = [cve for cve in cve_dataset if cve['cve_id'] in problematic_cves]\n",
    "\n",
    "print(f\"Testing {len(problematic_dataset)} problematic CVEs with fixed schema (max 20 lines per finding)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run quick test on just these two\n",
    "results_test = await test_cve_benchmark(problematic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmuyx9qdx57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE Benchmark Comparison Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CVE BENCHMARK COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfiguration changes:\")\n",
    "print(\"  - Security KB: 13 patterns ‚Üí 43 patterns (OWASP Top 10 2021)\")\n",
    "print(\"  - RAG retrieval: n_results=5 ‚Üí n_results=15\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RESULTS COMPARISON:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\\nüìä No RAG (Baseline):\")\n",
    "print(\"  Pass Rate: 16/17 (94%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\nüîç RAG with 13 patterns (n_results=5):\")\n",
    "print(\"  Pass Rate: 15/17 (88%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\nüéØ RAG with 43 patterns (n_results=15):\")\n",
    "print(\"  Run cell above to see results...\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6552020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
