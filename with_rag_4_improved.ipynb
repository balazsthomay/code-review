{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_fast=LitellmModel(model=\"openrouter/x-ai/grok-4-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added deletion analysis, chain-of-thought, and better semantic understanding\n",
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed: What code was added? What was removed? What was modified?\n",
    "2. Then, identify potential issues in the changes\n",
    "3. Consider the inverse: What functionality might be LOST from deletions?\n",
    "\n",
    "DELETION ANALYSIS (CRITICAL):\n",
    "- When you see removed code (lines starting with -), pay special attention to:\n",
    "  * Entire functions/classes being deleted - flag if they're called elsewhere\n",
    "  * Helper functions removed - check if remaining code still works without them\n",
    "  * Error handling removed - flag if this makes code less safe\n",
    "  * Imports removed - verify they're truly unused\n",
    "- If 10+ consecutive lines are deleted, describe what functionality is being removed\n",
    "\n",
    "BUG PATTERNS TO IDENTIFY:\n",
    "- Logic errors, unhandled edge cases, null/undefined access, type mismatches\n",
    "- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\n",
    "- Infinite loops, missing error handling (no try-except blocks)\n",
    "- Code duplication, overly complex functions\n",
    "- Removed functionality that breaks remaining code\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed from a security perspective\n",
    "2. Identify what security controls or validations were added or removed\n",
    "3. Consider: Does this change introduce new attack surface?\n",
    "\n",
    "SECURITY PATTERNS:\n",
    "- SQL injection, command injection, XSS vulnerabilities\n",
    "- Hardcoded secrets/credentials, insecure authentication\n",
    "- Path traversal, insecure deserialization\n",
    "- Improper input validation\n",
    "- Missing error handling that could expose sensitive information\n",
    "- Removed security checks or validation code\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\n",
    "- Consider what protections are LOST, not just what bugs are added\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Describe what changed in terms of code quality\n",
    "2. Identify violations of best practices in the new/modified code\n",
    "3. Consider: Does this change make the code harder to maintain?\n",
    "\n",
    "CODE QUALITY ISSUES:\n",
    "- Unclear variable names, functions exceeding 50 lines\n",
    "- Nested complexity over 3 levels, missing docstrings\n",
    "- Inconsistent formatting, magic numbers without explanation\n",
    "- Violations of DRY principle\n",
    "- Unclosed resources (files, database cursors, connections)\n",
    "- Missing try-except blocks for error-prone operations\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If helpful comments, docstrings, or error handling are removed, flag it\n",
    "- If code is simplified but loses clarity, mention it\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Identify what functions/methods are new or modified\n",
    "2. For each, list what test scenarios are needed\n",
    "3. Consider edge cases and error conditions\n",
    "\n",
    "For each new or modified function, suggest test cases covering:\n",
    "- Normal input cases\n",
    "- Edge cases (empty, null, boundary values)\n",
    "- Error conditions (exceptions, failures, timeouts)\n",
    "- Integration scenarios\n",
    "\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(temperature=0.5),\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(temperature=0.5),\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(temperature=0.5),\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(temperature=0.5),\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc44cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_all_agents(diff):\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer, diff),\n",
    "#         Runner.run(security_agent, diff),\n",
    "#         Runner.run(best_practices_agent, diff),\n",
    "#         Runner.run(test_coverage_agent, diff)\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "async def run_all_agents(diff):\n",
    "    # Get RAG context for both security and best practices agents\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=5)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(temperature=0.5),\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(temperature=0.5),\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(best_practices_agent_rag, diff),  # Now uses RAG too!\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added multi-file awareness and cross-file dependency detection\n",
    "\n",
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "AGGREGATION GUIDELINES:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. MULTI-FILE AWARENESS (CRITICAL):\n",
    "   - If findings span multiple files, check for cross-file dependencies\n",
    "   - Flag if changes in one file might break APIs/contracts in another file\n",
    "   - Look for patterns like: \"File A removes function X, but does File B call it?\"\n",
    "   - Consider the bigger picture: Do these changes work together?\n",
    "\n",
    "3. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "4. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "5. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_fast,\n",
    "    model_settings=ModelSettings(temperature=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    results = await run_all_agents(diff)\n",
    "    code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "    \n",
    "    organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALLING AGGREGATOR...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = await aggregator_agent(organized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATOR OUTPUT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if save_output:\n",
    "        os.makedirs(\"user-data\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation (Option 4): Models and Utilities\n",
    "import re\n",
    "\n",
    "class HybridEvaluation(BaseModel):\n",
    "    \"\"\"Hybrid evaluation combining location metrics + LLM relevance.\"\"\"\n",
    "    file_recall: float = Field(description=\"Recall at file level (0.0-1.0)\")\n",
    "    line_precision: float = Field(description=\"Precision at line level (0.0-1.0)\")\n",
    "    line_recall: float = Field(description=\"Recall at line level (0.0-1.0)\")\n",
    "    llm_relevance: float = Field(description=\"LLM-judged relevance score (0.0-1.0)\")\n",
    "    composite_score: float = Field(description=\"Combined score: (line_recall + llm_relevance) / 2\")\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ri2gmqdlr5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation Function\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid(report: str, bug_patch: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation: Location metrics (automated) + LLM relevance (semantic).\n",
    "    \n",
    "    Stage 1: Calculate automated location overlap\n",
    "    Stage 2: If file_recall > 0, use LLM to judge semantic relevance\n",
    "    \n",
    "    Returns:\n",
    "        dict with file_recall, line_precision, line_recall, llm_relevance, composite_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(bug_patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance (only if there's file overlap)\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_instructions = \"\"\"You are evaluating the semantic relevance of code review findings to an actual bug fix.\n",
    "\n",
    "Given:\n",
    "1. ACTUAL FIX PATCH: The changes that were made to fix bugs\n",
    "2. CODE REVIEW REPORT: What the review system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the bugs that were fixed\n",
    "- 0.7-0.9: Findings flag related issues that would lead to discovering the bugs\n",
    "- 0.4-0.6: Findings flag the general area but miss specific bugs\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Be objective and strict in your assessment.\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"Relevance Judge\",\n",
    "            instructions=llm_judge_instructions,\n",
    "            model=\"gpt-5.1\",\n",
    "            model_settings=ModelSettings(temperature=0.5),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{bug_patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to the actual fix.\n",
    "\"\"\"\n",
    "        \n",
    "        result = await Runner.run(llm_judge, prompt)\n",
    "        llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e63ligp8p4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Batch Testing\n",
    "\n",
    "async def test_bugsinpy_hybrid(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with hybrid evaluation.\n",
    "    \n",
    "    Args:\n",
    "        bugs_to_test: List of (project, bug_id) tuples\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results with hybrid metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            # Reverse diff to show bug introduction\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run code review on buggy code\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60  # 60% threshold for hybrid\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìç LOCATION METRICS (Automated):\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.2%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.2%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.2%}\")\n",
    "            print(f\"\\nü§ñ LLM RELEVANCE (Semantic):\")\n",
    "            print(f\"  Relevance Score: {eval_result['llm_relevance']:.2%}\")\n",
    "            print(f\"\\nüéØ COMPOSITE SCORE:\")\n",
    "            print(f\"  Score: {eval_result['composite_score']:.2%}\")\n",
    "            print(f\"  Status: {'‚úì PASSED' if result['passed'] else '‚úó FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"‚úó {result['project']}/{result['bug_id']}: ERROR - {result['error']}\")\n",
    "        else:\n",
    "            status = '‚úì' if result['passed'] else '‚úó'\n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"FileRec={result['file_recall']:.0%}, \"\n",
    "                  f\"LineRec={result['line_recall']:.0%}, \"\n",
    "                  f\"LLM={result['llm_relevance']:.0%}, \"\n",
    "                  f\"Composite={result['composite_score']:.0%}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    print(f\"Success Rate: {passed/len(results):.0%}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wu6y1lr2hy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 4 bugs (commented out - see 10-bug test below)\n",
    "# bugs_to_test = [\n",
    "#     (\"luigi\", 2),\n",
    "#     (\"black\", 4),\n",
    "#     (\"keras\", 1),\n",
    "#     (\"pandas\", 1),\n",
    "# ]\n",
    "# \n",
    "# results = await test_bugsinpy_hybrid(bugs_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bgahnfexbho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test with 10 bugs - showing what agents missed\n",
    "\n",
    "async def test_bugsinpy_with_miss_analysis(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with detailed miss analysis.\n",
    "    Shows what the agents caught vs. what they missed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            print(\"\\nACTUAL FIX (first 500 chars):\")\n",
    "            print(bug_patch[:500])\n",
    "            print(\"...\" if len(bug_patch) > 500 else \"\")\n",
    "            \n",
    "            # Reverse diff\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Parse locations to show what was missed\n",
    "            actual_locations = parse_changed_locations(bug_patch)\n",
    "            flagged_locations = parse_flagged_locations(report)\n",
    "            \n",
    "            # Find missed files\n",
    "            missed_files = actual_locations['files'] - flagged_locations['files']\n",
    "            \n",
    "            # Find missed line ranges\n",
    "            missed_lines = {}\n",
    "            for file in actual_locations['files']:\n",
    "                actual_lines = actual_locations['lines'].get(file, set())\n",
    "                flagged_lines_in_file = flagged_locations['lines'].get(file, set())\n",
    "                \n",
    "                # Lines that weren't caught (no flagged line within 5 lines)\n",
    "                uncaught = []\n",
    "                for actual_line in actual_lines:\n",
    "                    if not any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                        uncaught.append(actual_line)\n",
    "                \n",
    "                if uncaught:\n",
    "                    missed_lines[file] = sorted(uncaught)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60,\n",
    "                'missed_files': list(missed_files),\n",
    "                'missed_lines': missed_lines\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìç LOCATION METRICS:\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.0%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.0%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.0%}\")\n",
    "            print(f\"\\nü§ñ LLM RELEVANCE: {eval_result['llm_relevance']:.0%}\")\n",
    "            print(f\"üéØ COMPOSITE: {eval_result['composite_score']:.0%}\")\n",
    "            \n",
    "            # Show what was missed\n",
    "            if missed_files:\n",
    "                print(f\"\\n‚ùå MISSED FILES: {', '.join(missed_files)}\")\n",
    "            \n",
    "            if missed_lines:\n",
    "                print(f\"\\n‚ùå MISSED LINES:\")\n",
    "                for file, lines in missed_lines.items():\n",
    "                    line_ranges = []\n",
    "                    start = lines[0]\n",
    "                    end = start\n",
    "                    for i in range(1, len(lines)):\n",
    "                        if lines[i] == end + 1:\n",
    "                            end = lines[i]\n",
    "                        else:\n",
    "                            line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                            start = lines[i]\n",
    "                            end = start\n",
    "                    line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                    print(f\"  {file}: lines {', '.join(line_ranges)}\")\n",
    "            \n",
    "            print(f\"\\n{'‚úì PASSED' if result['passed'] else '‚úó FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"‚úó {result['project']}/{result['bug_id']}: ERROR\")\n",
    "        else:\n",
    "            status = '‚úì' if result['passed'] else '‚úó'\n",
    "            missed_info = \"\"\n",
    "            if result['missed_files']:\n",
    "                missed_info += f\" | Missed files: {len(result['missed_files'])}\"\n",
    "            if result['missed_lines']:\n",
    "                total_missed = sum(len(lines) for lines in result['missed_lines'].values())\n",
    "                missed_info += f\" | Missed lines: {total_missed}\"\n",
    "            \n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"Composite={result['composite_score']:.0%} \"\n",
    "                  f\"(LineRec={result['line_recall']:.0%}, LLM={result['llm_relevance']:.0%})\"\n",
    "                  f\"{missed_info}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)} ({passed/len(results):.0%})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "il199vb2y1h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING IMPROVED SYSTEM (with deletion analysis & chain-of-thought)\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "TESTING: luigi bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/luigi/contrib/beam_dataflow.py b/luigi/contrib/beam_dataflow.py\n",
      "index dd510786..42cdc742 100644\n",
      "--- a/luigi/contrib/beam_dataflow.py\n",
      "+++ b/luigi/contrib/beam_dataflow.py\n",
      "@@ -219,6 +219,7 @@ class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n",
      "     def __init__(self):\n",
      "         if not isinstance(self.dataflow_params, DataflowParamKeys):\n",
      "             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n",
      "+        super(BeamDataflowJobTask, self).__init__()\n",
      " \n",
      "  \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issues involve a high-severity logic error in the `get_target_path` method for BigQueryTarget, which could cause None returns and break downstream path usage (severity 9), and a missing superclass `__init__` call in BeamDataflowJobTask that risks improper task initialization (severity 8). Multiple agents flagged overlapping concerns in these areas, but all findings are contained within a single file with no apparent cross-file dependencies or API breaks. Additionally, documentation and minor formatting issues reduce code maintainability, while test gaps highlight missing coverage for key scenarios in `__init__` and `get_target_path`.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Missing call to superclass __init__ in BeamDataflowJobTask | luigi/contrib/beam_dataflow.py | [220-222] | 8 | Bug | Restore the call to super(BeamDataflowJobTask, self).__init__() in the __init__ method to ensure proper initialization of the base luigi.Task class, preventing potential breaks in task setup or lifecycle management. | Code Analyzer, Security, Best Practices |\n",
      "| Logic error in get_target_path for BigQueryTarget (missing return and inconsistent attributes) | luigi/contrib/beam_dataflow.py | [474-480] | 9 | Bug | Add a return statement with the correctly formatted string using consistent attribute access (e.g., return \"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id) or target.table.* as per object definition) to ensure a valid path string is returned for BigQueryTarget instances, avoiding None returns and attribute errors. | Code Analyzer, Security, Best Practices |\n",
      "| Removed docstring for get_target_path | luigi/contrib/beam_dataflow.py | [472-476] | 3 | Style | Restore or rewrite the docstring to describe the method's purpose, parameters, and usage, improving code clarity and documentation quality. | Code Analyzer, Best Practices |\n",
      "| Inconsistent error message formatting in get_target_path | luigi/contrib/beam_dataflow.py | [482-483] | 4 | Style | Restore the original formatted error message including the target instance (e.g., raise ValueError(f'Target {target} not supported')) to provide better debugging information. | Best Practices |\n",
      "| Test coverage gap for __init__ method | luigi/contrib/beam_dataflow.py | [220-224] | 7 | Test Gap | Add unit tests covering normal initialization with valid DataflowParamKeys, invalid dataflow_params type (expect ValueError), and missing/None dataflow_params behavior. | Test Coverage |\n",
      "| Test coverage gap for get_target_path method | luigi/contrib/beam_dataflow.py | [472-485] | 8 | Test Gap | Add unit tests for luigi.LocalTarget and gcs.GCSTarget (return path strings), bigquery.BigQueryTarget (formatted project:dataset.table string), unsupported target types (raise ValueError), and None/invalid inputs (raise ValueError). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 100%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: black bug 4\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/black.py b/black.py\n",
      "index f7022d8..05edf1a 100644\n",
      "--- a/black.py\n",
      "+++ b/black.py\n",
      "@@ -1480,7 +1480,13 @@ class EmptyLineTracker:\n",
      "         lines (two on module-level).\n",
      "         \"\"\"\n",
      "         before, after = self._maybe_empty_lines(current_line)\n",
      "-        before -= self.previous_after\n",
      "+        before = (\n",
      "+            # Black should not insert empty lines at the beginning\n",
      "+            # of the file\n",
      "+            0\n",
      "+            if self.previous_line is None\n",
      "+            else before - self.pr\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue identified across multiple agents is a logic change in `black.py` that removes the explicit check for the start of the file in empty line calculations, potentially leading to unwanted empty lines being inserted at the beginning of files and reducing code clarity. This bug has a high severity (6) and affects the `EmptyLineTracker._maybe_empty_lines` method. Additionally, there are significant test coverage gaps for this function, requiring unit tests for various edge cases to ensure robustness.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect handling of empty line calculation at file start | black.py | [1480, 1481, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492] | 6 | Bug | Restore the conditional check for 'self.previous_line is None' to set 'before' to 0 at the start of the file, ensuring empty lines are not inserted there. For example: if self.previous_line is None: before = 0 else: before = before - self.previous_after. This preserves original behavior, prevents negative or incorrect counts, and maintains clarity. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for EmptyLineTracker._maybe_empty_lines | black.py | [1480, 1487] | 7 | Test Gap | Add unit tests covering: Normal case with typical lines having expected empty lines before and after; Edge case where previous_line is None (start of file); Edge case where previous_after is zero; Edge case where previous_after is greater than before; Error condition if current_line is None or invalid type (if applicable); Integration scenario verifying interaction with other formatting components that rely on EmptyLineTracker. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 100%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: pandas bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/pandas/core/dtypes/common.py b/pandas/core/dtypes/common.py\n",
      "index 5b20b8e1b..a4a5ae1bf 100644\n",
      "--- a/pandas/core/dtypes/common.py\n",
      "+++ b/pandas/core/dtypes/common.py\n",
      "@@ -599,7 +599,7 @@ def is_string_dtype(arr_or_dtype) -> bool:\n",
      "         \"\"\"\n",
      "         These have kind = \"O\" but aren't string dtypes so need to be explicitly excluded\n",
      "         \"\"\"\n",
      "-        is_excluded_checks = (is_period_dtype, is_interval_dtype)\n",
      "+        is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categor\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical finding is the removal of categorical dtype exclusion from the `is_string_dtype` function, which could lead to misclassification of categorical dtypes as string dtypes, potentially causing logic errors in data processing; this is flagged as a high-severity bug and best practice violation. A related lower-severity security concern highlights potential indirect impacts on data validation. Comprehensive test coverage is lacking for various input scenarios in this function, requiring new unit tests to verify behavior.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed categorical dtype exclusion from is_string_dtype | pandas/core/dtypes/common.py | [599, 600, 601, 602] | 6 | Bug | Re-add is_categorical_dtype to the is_excluded_checks tuple if categorical dtypes should not be considered string dtypes, to maintain previous behavior and avoid misclassification. Review the rationale for the removal to ensure it does not lead to incorrect behavior in downstream code. | Code Analyzer, Best Practices |\n",
      "| Potential change in dtype exclusion logic affecting string dtype detection | pandas/core/dtypes/common.py | [600, 601] | 3 | Security | Review the impact of removing is_categorical_dtype from the exclusion list to ensure it does not cause incorrect data handling or downstream security issues. If necessary, add explicit validation or type checks where dtype classification is critical. | Security |\n",
      "| Missing tests for is_string_dtype | pandas/core/dtypes/common.py | [599, 606] | 7 | Test Gap | Add unit tests covering: Verify behavior when input is a Period dtype (should return False); Verify behavior when input is an Interval dtype (should return False); Verify behavior when input is a Categorical dtype (should return True or False as per new logic); Test with typical string dtype inputs (should return True); Test with non-string object dtype inputs (should return False); Test with numpy array inputs of various dtypes (string, categorical, period, interval); Test with pandas Series inputs of various dtypes; Test error handling when input is None or invalid type. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 100%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: scrapy bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py\n",
      "index 232e96cb..36f80969 100644\n",
      "--- a/scrapy/spidermiddlewares/offsite.py\n",
      "+++ b/scrapy/spidermiddlewares/offsite.py\n",
      "@@ -54,12 +54,16 @@ class OffsiteMiddleware(object):\n",
      "         if not allowed_domains:\n",
      "             return re.compile('')  # allow all by default\n",
      "         url_pattern = re.compile(\"^https?://.*$\")\n",
      "+        domains = []\n",
      "         for domain in allowed_domains:\n",
      "-            if url_pattern.match(domai\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is a logic error in the `_compile_allowed_domains` function of `offsite.py`, where URL entries are not properly filtered from `allowed_domains`, potentially allowing invalid domains to bypass offsite restrictions and introducing a security risk for unintended requests. This bug, flagged by multiple agents, reduces code correctness and clarity with unreachable code paths. Additionally, there are significant test coverage gaps for this function across various scenarios, including edge cases like empty lists, None values, and large inputs.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect domain filtering in _compile_allowed_domains allowing URLs to be treated as domains | scrapy/spidermiddlewares/offsite.py | [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] | 7 | Bug | Adjust the filtering logic to exclude entries matching the URL pattern and warn about them, then build domains list only from entries that are not URLs and not None (e.g., first warn about URL entries, then create domains list as [re.escape(d) for d in allowed_domains if d is not None and not url_pattern.match(d)]). Restore filtering to skip None and URL entries inside the loop, append only valid escaped domain strings to the domains list, and avoid empty if-blocks or redundant list comprehensions. Implement strict validation to reject or remove URLs/malformed domains instead of only warning, with normalization and whitelist patterns to prevent offsite bypass. | Code Analyzer, Security, Best Practices |\n",
      "| Test coverage gap for _compile_allowed_domains function | scrapy/spidermiddlewares/offsite.py | [54, 69] | 7 | Test Gap | Add unit tests covering: allowed_domains is empty list; allowed_domains contains None values; allowed_domains contains valid domain names only; allowed_domains contains full URLs triggering warnings; allowed_domains contains mix of domains and URLs; allowed_domains contains domains with special regex characters; allowed_domains contains internationalized domain names (IDN); performance with very large allowed_domains list. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 95%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 90%\n",
      "üéØ COMPOSITE: 95%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: thefuck bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/thefuck/rules/pip_unknown_command.py b/thefuck/rules/pip_unknown_command.py\n",
      "index 75fcc7c..2720cda 100644\n",
      "--- a/thefuck/rules/pip_unknown_command.py\n",
      "+++ b/thefuck/rules/pip_unknown_command.py\n",
      "@@ -12,8 +12,8 @@ def match(command):\n",
      " \n",
      " \n",
      " def get_new_command(command):\n",
      "-    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n",
      "+    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n",
      "                             command.output)[0]\n",
      "-    new_cmd = re.findall(r'maybe you m\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical finding is an inconsistency in regex patterns within the `get_new_command` function of `thefuck/rules/pip_unknown_command.py`, which could lead to IndexErrors due to unchecked `findall` results and unexpected regex behavior from mixed quoting styles. A related minor security concern exists around unescaped user-controlled input in regex extraction, though it does not introduce new vulnerabilities. Test coverage is insufficient, missing scenarios for normal, edge, and error cases in the same function, potentially allowing undetected bugs to persist.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Inconsistent and potentially unsafe regex delimiters in get_new_command | thefuck/rules/pip_unknown_command.py | 13,14,15,16,17,18 | 7 | Bug | Ensure consistent use of raw strings and quotes in regex patterns, for example use r'ERROR: unknown command \"([a-z]+)\"' uniformly. Add checks to verify that re.findall returns a non-empty list before accessing the first element to avoid IndexError, or use re.search and check for None before accessing groups. Use consistent regex patterns with clear and consistent quoting style. Prefer raw strings with unescaped quotes and a character class that matches expected command characters. Also, keep the assignment statements on a single line or use proper line continuation for readability. | Code Analyzer, Best Practices |\n",
      "| Use of Regular Expressions with Unescaped User-Controlled Input | thefuck/rules/pip_unknown_command.py | 13,14,15,16,17 | 2 | Security | Validate or sanitize the extracted command names before using them in replacements to prevent unexpected behavior. Ensure that the replacement function handles edge cases safely. | Security |\n",
      "| Test coverage gap for get_new_command | thefuck/rules/pip_unknown_command.py | 13,14,15,16,17,18 | 7 | Test Gap | Add unit tests covering: normal case: output contains 'ERROR: unknown command \"<command>\"' and 'maybe you meant \"<suggestion>\"' with lowercase alphabetic commands; edge case: output missing 'ERROR: unknown command' pattern leading to IndexError on accessing [0]; edge case: output missing 'maybe you meant' pattern leading to IndexError on accessing [0]; edge case: output contains uppercase or non-alphabetic characters in commands (should regex handle or fail?); error case: command.output is None or empty string; error case: command.output contains multiple matches for patterns (ensure first is used). Suggested test approach: unit test. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 50%\n",
      "üéØ COMPOSITE: 75%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: matplotlib bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py\n",
      "index 71eb153f2..8009207dd 100644\n",
      "--- a/lib/matplotlib/backend_bases.py\n",
      "+++ b/lib/matplotlib/backend_bases.py\n",
      "@@ -46,6 +46,7 @@ from matplotlib._pylab_helpers import Gcf\n",
      " from matplotlib.backend_managers import ToolManager\n",
      " from matplotlib.transforms import Affine2D\n",
      " from matplotlib.path import Path\n",
      "+from matplotlib.cbook import _setattr_cm\n",
      " \n",
      " \n",
      " _log = logging.getLogger(__name__)\n",
      "@@ -1502,15 +1503,14 @@ class KeyEvent\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary concern across multiple files is the replacement of a scoped context manager (_setattr_cm) with permanent monkey patching of renderer drawing methods using no-op lambdas when draw_disabled=True, which risks side effects like unintended rendering failures if renderers are reused. This change spans backend_bases.py (core implementation), figure.py (removal of related imports and patching), and tight_layout.py (updated renderer acquisition), potentially breaking cross-file dependencies in drawing and layout operations. Additionally, several test coverage gaps exist for verifying the new behavior and error handling.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Permanent monkey patching of renderer methods without restoration, replacing context manager | lib/matplotlib/backend_bases.py | [1502, 1503, 1507, 1529, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107] | 7 | Security | Implement a context manager or try-finally block to temporarily replace methods on the renderer and ensure original methods are restored after use; verify no side effects from permanent overrides and consider cross-file reuse of renderers in drawing and layout operations. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for _get_renderer function | lib/matplotlib/backend_bases.py | [1503, 1536] | 8 | Test Gap | Add unit tests covering normal case with default parameters, print_method provision, draw_disabled=True disabling draw/group methods, verification of no-op replacements, error handling for Figure.draw() exceptions, and integration with Figure drawing/caching. | Test Coverage |\n",
      "| Missing tests for FigureCanvasBase.draw method usage of _get_renderer | lib/matplotlib/backend_bases.py | [2087, 2107] | 7 | Test Gap | Add integration tests for normal drawing with print_method/orientation, draw_disabled=True during tight bbox calculation, confirmation of no side effects in tight bbox, and error handling for drawing exceptions. | Test Coverage |\n",
      "| Removal of _setattr_cm and RendererBase imports with no replacement patching mechanism | lib/matplotlib/figure.py | [2392, 2393, 2400, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424] | 6 | Bug | Reintroduce a context manager or equivalent safe patching mechanism for temporary renderer method replacement during tight layout; ensure removal does not break downstream dependencies and verify consistency with backend_bases.py changes. | Code Analyzer, Best Practices |\n",
      "| Missing tests for Figure.tight_layout usage of get_tight_layout_figure and renderer with draw_disabled | lib/matplotlib/figure.py | [2392, 2417] | 6 | Test Gap | Add integration tests for normal layout adjustment with disabled draw methods, verification of no side effects/errors in layout calculation, and coverage of various subplot configurations. | Test Coverage |\n",
      "| Changed behavior of get_renderer to always disable draw methods in tight_layout | lib/matplotlib/tight_layout.py | [173, 176] | 3 | Bug | Confirm disabling draw methods during tight layout does not impact calculations or cause cross-file side effects (e.g., with backend_bases.py renderer patching); if drawing is needed, add selective enabling option. | Code Analyzer |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 86%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 0%\n",
      "üéØ COMPOSITE: 43%\n",
      "\n",
      "‚ùå MISSED LINES:\n",
      "  lib/matplotlib/backend_bases.py: lines 46-52, 1513-1516\n",
      "\n",
      "‚úó FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: tqdm bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tqdm/contrib/__init__.py b/tqdm/contrib/__init__.py\n",
      "index 1dddacf..935ab63 100644\n",
      "--- a/tqdm/contrib/__init__.py\n",
      "+++ b/tqdm/contrib/__init__.py\n",
      "@@ -38,7 +38,7 @@ def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n",
      "         if isinstance(iterable, np.ndarray):\n",
      "             return tqdm_class(np.ndenumerate(iterable),\n",
      "                               total=total or len(iterable), **tqdm_kwargs)\n",
      "-    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n",
      "+    return \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue is a critical bug in the `tenumerate` function of `tqdm/contrib/__init__.py`, where the 'start' parameter is incorrectly passed to `tqdm_class` instead of `enumerate`, potentially breaking enumeration indexing and iteration behavior; this is flagged by multiple agents with the highest severity of 7. Additionally, there are comprehensive test coverage gaps for this function, highlighting missing scenarios for normal, edge, error, and integration cases. No cross-file dependencies are evident, as all findings are isolated to this single file.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect handling of 'start' parameter in tenumerate function leading to broken enumeration indexing and potential iteration behavior changes | tqdm/contrib/__init__.py | [38, 39, 42, 43] | 7 | Bug | Use enumerate(tqdm_class(iterable, **tqdm_kwargs), start=start) to correctly pass the start index to enumerate; verify tqdm_class does not expect or mishandle 'start' as a positional argument, and validate inputs if user-controlled to avoid logic errors or unexpected side effects | Code Analyzer, Security, Best Practices |\n",
      "| Missing comprehensive unit tests for tenumerate function, including normal iteration with progress bar, edge cases (empty iterable, numpy ndarray using np.ndenumerate), error handling (invalid iterable types), parameter variations (start=0/>0/negative, total provided/omitted), and integration with custom/mock tqdm_class | tqdm/contrib/__init__.py | [38, 44] | 7 | Test Gap | Implement unit tests covering: Normal case (list/iterable with progress bar), Edge cases (empty iterable, numpy ndarray with np.ndenumerate and total set), Error cases (None/invalid iterable with exceptions), Parameter variations (start=0/>0/negative if allowed, total provided/omitted/less than length), Integration (mock/custom tqdm_class to verify parameters and interactions) | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 100%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: tornado bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tornado/websocket.py b/tornado/websocket.py\n",
      "index 00d08bab..d991fee5 100644\n",
      "--- a/tornado/websocket.py\n",
      "+++ b/tornado/websocket.py\n",
      "@@ -558,8 +558,8 @@ class WebSocketHandler(tornado.web.RequestHandler):\n",
      " \n",
      "         .. versionadded:: 3.1\n",
      "         \"\"\"\n",
      "-        assert self.stream is not None\n",
      "-        self.stream.set_nodelay(value)\n",
      "+        assert self.ws_connection is not None\n",
      "+        self.ws_connection.set_nodelay(value)\n",
      " \n",
      "     def on_connection_close(self) -> None:\n",
      "         if self.ws\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The proposed changes in tornado/websocket.py primarily involve modifications and removals related to the set_nodelay method across WebSocketHandler, WebSocketProtocol, and WebSocketProtocol13, which could lead to inconsistent TCP_NODELAY handling, potential performance degradation, and interface inconsistencies without breaking cross-file dependencies since all changes are isolated to this file. Critical issues include the incomplete implementation in WebSocketHandler (severity 7) and the removal of the method in WebSocketProtocol13 (severity 7), both risking unexpected buffering or missing functionality. Additionally, three test coverage gaps highlight untested scenarios for these methods, emphasizing the need for comprehensive validation.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect set_nodelay implementation in WebSocketHandler | tornado/websocket.py | [558,559,560,561,562] | 7 | Bug | Restore the call to self.ws_connection.set_nodelay(value) after asserting self.ws_connection is not None, use consistent None checks for both stream and ws_connection with explicit error handling, and verify that setting TCP_NODELAY on stream instead of ws_connection is correct and does not degrade performance or security. | Code Analyzer, Security, Best Practices |\n",
      "| Removal of abstract set_nodelay method from WebSocketProtocol | tornado/websocket.py | [714,715,716,717,718] | 6 | Bug | Reinstate the abstract set_nodelay method in WebSocketProtocol to enforce implementation in all subclasses, ensure all subclasses properly implement this method, and restore to maintain control over socket TCP_NODELAY settings for performance and timing. | Code Analyzer, Security, Best Practices |\n",
      "| Removal of set_nodelay implementation in WebSocketProtocol13 | tornado/websocket.py | [1345,1346,1347,1348,1349,1350,1351] | 7 | Bug | Uncomment and restore the set_nodelay method in WebSocketProtocol13 to ensure TCP_NODELAY is correctly applied, maintain interface consistency, and prevent runtime errors or missing functionality impacting performance. | Code Analyzer, Security, Best Practices |\n",
      "| Missing tests for WebSocketHandler.set_nodelay | tornado/websocket.py | [559,560,561,562] | 7 | Test Gap | Add unit tests for normal cases (set_nodelay True/False on valid stream), edge case (call when self.stream is None, expecting assert), and integration tests to verify underlying stream behavior. | Test Coverage |\n",
      "| Missing tests for WebSocketProtocol.set_nodelay | tornado/websocket.py | [716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746] | 5 | Test Gap | Add unit and integration tests to verify set_nodelay removal/replacement in subclasses, test WebSocketProtocol13 behavior post-removal, and edge cases for calls on base class or subclasses without implementation. | Test Coverage |\n",
      "| Missing tests for WebSocketProtocol13.set_nodelay | tornado/websocket.py | [1349,1350,1351] | 7 | Test Gap | Add unit and integration tests for normal cases (set_nodelay True/False on stream), edge case (call when stream is None, expecting handle/raise), and verify impact on actual socket behavior. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 60%\n",
      "  Line Recall: 96%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 98%\n",
      "\n",
      "‚ùå MISSED LINES:\n",
      "  tornado/websocket.py: lines 1357\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: httpie bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/httpie/downloads.py b/httpie/downloads.py\n",
      "index b49e335..972151e 100644\n",
      "--- a/httpie/downloads.py\n",
      "+++ b/httpie/downloads.py\n",
      "@@ -7,6 +7,7 @@ from __future__ import division\n",
      " import os\n",
      " import re\n",
      " import sys\n",
      "+import errno\n",
      " import mimetypes\n",
      " import threading\n",
      " from time import sleep, time\n",
      "@@ -135,12 +136,43 @@ def filename_from_url(url, content_type):\n",
      "     return fn\n",
      " \n",
      " \n",
      "+def trim_filename(filename, max_len):\n",
      "+    if len(filename) > max_len:\n",
      "+        trim_by = len(filename) - max_len\n",
      "+  \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue in the provided findings is the removal of filename trimming and length-handling functions in `httpie/downloads.py`, which could lead to filesystem errors, potential denial of service, or unexpected behavior when saving files with long names. This affects the `get_unique_filename` function specifically, where suffix appending no longer accounts for length limits, increasing the risk of failures on filesystems with strict constraints. Additionally, there are significant test coverage gaps for `get_unique_filename`, leaving edge cases like special characters and exceptions untested; no cross-file dependencies are evident as all findings are isolated to this single file.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removal of Filename Trimming Logic Leading to Potential Filesystem Errors and Collisions | httpie/downloads.py | [135-192] | 7 | Bug | Restore filename trimming logic or implement an alternative method to ensure filenames do not exceed filesystem limits before attempting to create unique filenames. Reintroduce checks and trimming when appending suffixes in get_unique_filename to prevent exceeding limits or unexpected collisions. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Test Coverage for get_unique_filename | httpie/downloads.py | [142, 157] | 8 | Test Gap | Implement unit tests with mocking of the exists function covering normal cases (non-existent filename), conflict resolution (appending suffixes, multiple increments), edge cases (empty input, special characters), error handling (exists raises exception), and an integration test verifying filesystem interactions. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 84%\n",
      "  Line Recall: 86%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 93%\n",
      "\n",
      "‚ùå MISSED LINES:\n",
      "  httpie/downloads.py: lines 7-13\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: cookiecutter bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py\n",
      "index 37365a4..c526b97 100644\n",
      "--- a/cookiecutter/generate.py\n",
      "+++ b/cookiecutter/generate.py\n",
      "@@ -82,7 +82,7 @@ def generate_context(\n",
      "     context = OrderedDict([])\n",
      " \n",
      "     try:\n",
      "-        with open(context_file) as file_handle:\n",
      "+        with open(context_file, encoding='utf-8') as file_handle:\n",
      "             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n",
      "     except ValueError as e:\n",
      "         # JSON decoding error.  Let's thr\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary critical finding is the removal of explicit UTF-8 encoding when opening the context_file in the `generate_context` function, which can lead to decoding errors, JSON parsing failures, or incorrect behavior across different system locales‚Äîthis issue spans bug, security, and best practices concerns with a highest severity of 7. Additionally, there are significant test coverage gaps for the `generate_context` function, including untested error cases like invalid JSON or missing files, which could exacerbate undetected issues from the encoding change. No cross-file dependencies are present, as all findings are isolated to a single file.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Missing encoding in file open leading to potential decoding/JSON parsing errors | cookiecutter/generate.py | 84 | 7 | Bug | Restore the encoding parameter when opening the file: `with open(context_file, encoding='utf-8') as file_handle:` to ensure consistent UTF-8 decoding regardless of system defaults, preventing errors with non-ASCII characters or varying locales. | Code Analyzer, Security, Best Practices |\n",
      "| Test coverage gap for generate_context function | cookiecutter/generate.py | 82, 89 | 7 | Test Gap | Add unit tests covering: normal case (valid JSON), error cases (FileNotFoundError, empty file, invalid JSON ValueError), edge cases (very large file, encoding issues), and integration with context dict expectations. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ü§ñ LLM RELEVANCE: 100%\n",
      "üéØ COMPOSITE: 100%\n",
      "\n",
      "‚úì PASSED\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "‚úì luigi/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "‚úì black/4: Composite=100% (LineRec=100%, LLM=100%)\n",
      "‚úì pandas/1: Composite=100% (LineRec=100%, LLM=100%)\n",
      "‚úì scrapy/1: Composite=95% (LineRec=100%, LLM=90%)\n",
      "‚úì thefuck/1: Composite=75% (LineRec=100%, LLM=50%)\n",
      "‚úó matplotlib/1: Composite=43% (LineRec=86%, LLM=0%) | Missed lines: 11\n",
      "‚úì tqdm/1: Composite=100% (LineRec=100%, LLM=100%)\n",
      "‚úì tornado/1: Composite=98% (LineRec=96%, LLM=100%) | Missed lines: 1\n",
      "‚úì httpie/1: Composite=93% (LineRec=86%, LLM=100%) | Missed lines: 7\n",
      "‚úì cookiecutter/1: Composite=100% (LineRec=100%, LLM=100%)\n",
      "\n",
      "Passed: 9/10 (90%)\n"
     ]
    }
   ],
   "source": [
    "# Test 10 bugs with IMPROVED system\n",
    "bugs_to_test_10 = [\n",
    "    (\"luigi\", 2),\n",
    "    (\"black\", 4),\n",
    "    (\"pandas\", 1),\n",
    "    (\"scrapy\", 1),\n",
    "    (\"thefuck\", 1),\n",
    "    (\"matplotlib\", 1),\n",
    "    (\"tqdm\", 1),\n",
    "    (\"tornado\", 1),\n",
    "    (\"httpie\", 1),\n",
    "    (\"cookiecutter\", 1),\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING IMPROVED SYSTEM (with deletion analysis & chain-of-thought)\")\n",
    "print(\"=\"*70)\n",
    "results_improved = await test_bugsinpy_with_miss_analysis(bugs_to_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3puek6co2le",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE vs IMPROVED COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Bug             Baseline                  Improved                  Change         \n",
      "----------------------------------------------------------------------\n",
      "luigi/2         100% (LR:100%, LLM:100%)  ‚Üí  100% (LR:100%, LLM:100%)  ‚úì +0%\n",
      "black/4         75% (LR:100%, LLM:50%)  ‚Üí  100% (LR:100%, LLM:100%)  ‚úì +25%\n",
      "pandas/1        100% (LR:100%, LLM:100%)  ‚Üí  100% (LR:100%, LLM:100%)  ‚úì +0%\n",
      "scrapy/1        90% (LR:100%, LLM:80%)  ‚Üí  95% (LR:100%, LLM:90%)  ‚úì +5%\n",
      "thefuck/1       65% (LR:100%, LLM:30%)  ‚Üí  75% (LR:100%, LLM:50%)  ‚úì +10%\n",
      "matplotlib/1    53% (LR:86%, LLM:20%)  ‚Üí  43% (LR:86%, LLM:0%)  ‚úó -10%\n",
      "tqdm/1          50% (LR:0%, LLM:100%)  ‚Üí  100% (LR:100%, LLM:100%)  ‚úì +50%\n",
      "tornado/1       87% (LR:74%, LLM:100%)  ‚Üí  98% (LR:96%, LLM:100%)  ‚úì +11%\n",
      "httpie/1        37% (LR:54%, LLM:20%)  ‚Üí  93% (LR:86%, LLM:100%)  ‚úì +56%\n",
      "cookiecutter/1  100% (LR:100%, LLM:100%)  ‚Üí  100% (LR:100%, LLM:100%)  ‚úì +0%\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Baseline Pass Rate: 7/10 (70%)\n",
      "Improved Pass Rate: 9/10 (90%)\n",
      "Average Composite: 76% ‚Üí 90%\n",
      "\n",
      "‚úì IMPROVEMENTS (5):\n",
      "  httpie/1: +56%\n",
      "  tqdm/1: +50%\n",
      "  black/4: +25%\n",
      "  tornado/1: +11%\n",
      "  thefuck/1: +10%\n",
      "\n",
      "‚úó REGRESSIONS (1):\n",
      "  matplotlib/1: -10%\n",
      "\n",
      "======================================================================\n",
      "Net Change: +2 bugs passing\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare: Baseline vs Improved Results\n",
    "\n",
    "baseline_results = {\n",
    "    \"luigi/2\": {\"composite\": 1.00, \"line_recall\": 1.00, \"llm_relevance\": 1.00},\n",
    "    \"black/4\": {\"composite\": 0.75, \"line_recall\": 1.00, \"llm_relevance\": 0.50},\n",
    "    \"pandas/1\": {\"composite\": 1.00, \"line_recall\": 1.00, \"llm_relevance\": 1.00},\n",
    "    \"scrapy/1\": {\"composite\": 0.90, \"line_recall\": 1.00, \"llm_relevance\": 0.80},\n",
    "    \"thefuck/1\": {\"composite\": 0.65, \"line_recall\": 1.00, \"llm_relevance\": 0.30},\n",
    "    \"matplotlib/1\": {\"composite\": 0.53, \"line_recall\": 0.86, \"llm_relevance\": 0.20},\n",
    "    \"tqdm/1\": {\"composite\": 0.50, \"line_recall\": 0.00, \"llm_relevance\": 1.00},\n",
    "    \"tornado/1\": {\"composite\": 0.87, \"line_recall\": 0.74, \"llm_relevance\": 1.00},\n",
    "    \"httpie/1\": {\"composite\": 0.37, \"line_recall\": 0.54, \"llm_relevance\": 0.20},\n",
    "    \"cookiecutter/1\": {\"composite\": 1.00, \"line_recall\": 1.00, \"llm_relevance\": 1.00},\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE vs IMPROVED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Bug':<15} {'Baseline':<25} {'Improved':<25} {'Change':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "total_baseline_composite = 0\n",
    "total_improved_composite = 0\n",
    "improvements = []\n",
    "regressions = []\n",
    "\n",
    "for result in results_improved:\n",
    "    if 'error' in result:\n",
    "        continue\n",
    "    \n",
    "    bug_key = f\"{result['project']}/{result['bug_id']}\"\n",
    "    baseline = baseline_results.get(bug_key, {})\n",
    "    \n",
    "    baseline_comp = baseline.get('composite', 0)\n",
    "    improved_comp = result['composite_score']\n",
    "    \n",
    "    total_baseline_composite += baseline_comp\n",
    "    total_improved_composite += improved_comp\n",
    "    \n",
    "    change = improved_comp - baseline_comp\n",
    "    if change > 0.05:\n",
    "        improvements.append((bug_key, change))\n",
    "    elif change < -0.05:\n",
    "        regressions.append((bug_key, change))\n",
    "    \n",
    "    change_str = f\"{change:+.0%}\"\n",
    "    status = \"‚úì\" if change >= 0 else \"‚úó\"\n",
    "    \n",
    "    print(f\"{bug_key:<15} {baseline_comp:.0%} (LR:{baseline.get('line_recall', 0):.0%}, LLM:{baseline.get('llm_relevance', 0):.0%})\"\n",
    "          f\"  ‚Üí  {improved_comp:.0%} (LR:{result['line_recall']:.0%}, LLM:{result['llm_relevance']:.0%})\"\n",
    "          f\"  {status} {change_str}\")\n",
    "\n",
    "baseline_pass = sum(1 for b in baseline_results.values() if b['composite'] >= 0.60)\n",
    "improved_pass = sum(1 for r in results_improved if r.get('composite_score', 0) >= 0.60 and 'error' not in r)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline Pass Rate: {baseline_pass}/10 ({baseline_pass*10}%)\")\n",
    "print(f\"Improved Pass Rate: {improved_pass}/10 ({improved_pass*10}%)\")\n",
    "print(f\"Average Composite: {total_baseline_composite/10:.0%} ‚Üí {total_improved_composite/len([r for r in results_improved if 'error' not in r]):.0%}\")\n",
    "\n",
    "if improvements:\n",
    "    print(f\"\\n‚úì IMPROVEMENTS ({len(improvements)}):\")\n",
    "    for bug, change in sorted(improvements, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {bug}: {change:+.0%}\")\n",
    "\n",
    "if regressions:\n",
    "    print(f\"\\n‚úó REGRESSIONS ({len(regressions)}):\")\n",
    "    for bug, change in sorted(regressions, key=lambda x: x[1]):\n",
    "        print(f\"  {bug}: {change:+.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Net Change: {improved_pass - baseline_pass:+d} bugs passing\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
