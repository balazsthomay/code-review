{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_fast=LitellmModel(model=\"openrouter/x-ai/grok-4-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff. \n",
    "Identify bugs and anti-patterns including: logic errors, unhandled edge cases, null/undefined access, type mismatches, off-by-one errors, resource leaks (unclosed files/cursors/connections), infinite loops, missing error handling (no try-except blocks), code duplication, and overly complex functions. \n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff. \n",
    "Identify security vulnerabilities including: SQL injection, command injection, XSS vulnerabilities, hardcoded secrets/credentials, insecure authentication, path traversal, insecure deserialization, improper input validation, and missing error handling that could expose sensitive information.\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff. \n",
    "Identify code quality issues including: unclear variable names, functions exceeding 50 lines, nested complexity over 3 levels, missing docstrings, inconsistent formatting, magic numbers without explanation, violations of DRY principle, unclosed resources (files, database cursors, connections), and missing try-except blocks for error-prone operations.\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff. \n",
    "For each new or modified function, suggest test cases covering: normal input cases, edge cases (empty, null, boundary values), error conditions (exceptions, failures, timeouts), and integration scenarios.\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc44cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3566dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING SUBTLE BEST PRACTICES VIOLATIONS\n",
      "============================================================\n",
      "\n",
      "Code contains:\n",
      "1. Missing edge case handling (n=0, n>len)\n",
      "2. Mutable default argument\n",
      "3. Unclosed file resource\n",
      "4. Using range(len()) instead of enumerate\n",
      "5. Inconsistent return types\n",
      "6. Excessive nesting (4 levels)\n",
      "7. Magic numbers (1.5, 2.0, 0.75)\n",
      "8. Missing docstrings\n",
      "9. Missing error handling\n",
      "\n",
      "============================================================\n",
      "WITH RAG:\n",
      "============================================================\n",
      "Found 8 issues\n",
      "- Mutable Default Argument Usage (severity 8)\n",
      "- Missing Error Handling for File Operations (severity 9)\n",
      "- Inconsistent Return Types in get_users (severity 7)\n",
      "- Incorrect Loop on Empty List in get_users (severity 6)\n",
      "- Unclear Variable Names in calculate (severity 5)\n",
      "- Magic Numbers Without Explanation in calculate (severity 5)\n",
      "- Excessive Nested Conditionals in calculate (severity 6)\n",
      "- Missing Edge Case Handling in process_items (severity 7)\n",
      "\n",
      "============================================================\n",
      "WITHOUT RAG:\n",
      "============================================================\n",
      "Found 4 issues\n",
      "- Mutable Default Argument (severity 7)\n",
      "- Unclosed File Resource (severity 7)\n",
      "- Incorrect Loop Condition Using Empty List (severity 6)\n",
      "- Nested Complexity Over 3 Levels (severity 5)\n",
      "\n",
      "============================================================\n",
      "COMPARISON:\n",
      "============================================================\n",
      "RAG found 4 additional issues\n"
     ]
    }
   ],
   "source": [
    "# Test Best Practices RAG\n",
    "\n",
    "async def run_best_practices_agent_with_rag(code_diff: str):\n",
    "    \"\"\"\n",
    "    Runs best practices agent with RAG-enhanced context.\n",
    "    \"\"\"\n",
    "    # Get relevant best practices patterns\n",
    "    patterns = get_relevant_best_practices_patterns(code_diff, n_results=5)\n",
    "    \n",
    "    enhanced_instructions = f\"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "{patterns}\n",
    "\n",
    "Based on these patterns and your expertise, identify code quality issues including: unclear variable names, functions exceeding 50 lines, nested complexity over 3 levels, missing docstrings, inconsistent formatting, magic numbers without explanation, violations of DRY principle, unclosed resources (files, database cursors, connections), and missing try-except blocks for error-prone operations.\n",
    "\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent (RAG)\",\n",
    "        instructions=enhanced_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run the agent\n",
    "    result = await Runner.run(best_practices_agent_rag, code_diff)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test with subtle best practices violations\n",
    "subtle_bp_test = \"\"\"\n",
    "diff --git a/data_processor.py b/data_processor.py\n",
    "+def process_items(items, n):\n",
    "+    return items[-n:]\n",
    "+\n",
    "+def add_to_list(item, my_list=[]):\n",
    "+    my_list.append(item)\n",
    "+    return my_list\n",
    "+\n",
    "+def read_data(path):\n",
    "+    f = open(path)\n",
    "+    data = f.read()\n",
    "+    return data\n",
    "+\n",
    "+def get_users(count):\n",
    "+    users = []\n",
    "+    for i in range(len(users)):\n",
    "+        users.append(fetch_user(i))\n",
    "+    if count > 0:\n",
    "+        return users\n",
    "+    return None\n",
    "+\n",
    "+def calculate(a, b, c):\n",
    "+    if a > 0:\n",
    "+        if b > 0:\n",
    "+            if c > 0:\n",
    "+                x = a * 1.5\n",
    "+                y = b * 2.0\n",
    "+                return x + y + c * 0.75\n",
    "+            else:\n",
    "+                return 0\n",
    "+        else:\n",
    "+            return 0\n",
    "+    else:\n",
    "+        return 0\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING SUBTLE BEST PRACTICES VIOLATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCode contains:\")\n",
    "print(\"1. Missing edge case handling (n=0, n>len)\")\n",
    "print(\"2. Mutable default argument\")\n",
    "print(\"3. Unclosed file resource\")\n",
    "print(\"4. Using range(len()) instead of enumerate\")\n",
    "print(\"5. Inconsistent return types\")\n",
    "print(\"6. Excessive nesting (4 levels)\")\n",
    "print(\"7. Magic numbers (1.5, 2.0, 0.75)\")\n",
    "print(\"8. Missing docstrings\")\n",
    "print(\"9. Missing error handling\")\n",
    "\n",
    "# With RAG\n",
    "result_rag = await run_best_practices_agent_with_rag(subtle_bp_test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WITH RAG:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Found {len(result_rag.final_output.findings)} issues\")\n",
    "for f in result_rag.final_output.findings:\n",
    "    print(f\"- {f.title} (severity {f.severity})\")\n",
    "\n",
    "# Without RAG  \n",
    "result_no_rag = await Runner.run(best_practices_agent, subtle_bp_test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WITHOUT RAG:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Found {len(result_no_rag.final_output.findings)} issues\")\n",
    "for f in result_no_rag.final_output.findings:\n",
    "    print(f\"- {f.title} (severity {f.severity})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RAG found {len(result_rag.final_output.findings) - len(result_no_rag.final_output.findings)} additional issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_all_agents(diff):\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer, diff),\n",
    "#         Runner.run(security_agent, diff),\n",
    "#         Runner.run(best_practices_agent, diff),\n",
    "#         Runner.run(test_coverage_agent, diff)\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "async def run_all_agents(diff):\n",
    "    # Get RAG context for both security and best practices agents\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=5)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(best_practices_agent_rag, diff),  # Now uses RAG too!\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "When creating the report, follow these guidelines:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "3. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "4. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "5. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_fast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "#     \"\"\"\n",
    "#     Complete code review pipeline.\n",
    "    \n",
    "#     Args:\n",
    "#         diff: The code diff to review\n",
    "        \n",
    "#     Returns:\n",
    "#         Markdown-formatted code review report\n",
    "#     \"\"\"\n",
    "#     results = await run_all_agents(diff)\n",
    "#     code_result, security_result, best_practices_result, test_coverage_result = results    \n",
    "#     organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "#     report = await aggregator_agent(organized)\n",
    "    \n",
    "#     if save_output:\n",
    "#         os.makedirs(\"user-data\", exist_ok=True)\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "#         with open(filepath, \"w\") as f:\n",
    "#             f.write(report)\n",
    "#         print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "#     return report\n",
    "\n",
    "\n",
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    results = await run_all_agents(diff)\n",
    "    code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "    \n",
    "    # # DEBUG: Print all agent outputs\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"CODE ANALYZER RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in code_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"SECURITY AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in security_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"BEST PRACTICES AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in best_practices_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"TEST COVERAGE AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for gap in test_coverage_result.final_output.findings:\n",
    "    #     print(f\"\\nFunction: {gap.function_name}\")\n",
    "    #     print(f\"Priority: {gap.priority}\")\n",
    "    #     print(f\"Lines: {gap.lines}\")\n",
    "    #     print(f\"Missing scenarios: {gap.missing_scenarios}\")\n",
    "    # print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALLING AGGREGATOR...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = await aggregator_agent(organized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATOR OUTPUT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if save_output:\n",
    "        os.makedirs(\"user-data\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "211922ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CODE ANALYZER RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [8, 9, 10]\n",
      "Description: The authenticate method constructs SQL queries by directly concatenating user inputs (username and password) into the query string. This allows an att...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SECURITY AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection in authenticate method\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method constructs an SQL query by directly concatenating user inputs (username and password) into the query string. This allows an at...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BEST PRACTICES AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9]\n",
      "Description: The authenticate method builds the SQL query by concatenating user inputs directly into the query string. This approach is vulnerable to SQL injection...\n",
      "\n",
      "Title: Missing Cursor Closure\n",
      "Severity: 5\n",
      "Lines: [8, 9]\n",
      "Description: The database cursor created in the authenticate method is not explicitly closed, which can lead to resource leaks....\n",
      "\n",
      "Title: No Exception Handling for Database Operations\n",
      "Severity: 6\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method does not include try-except blocks around database operations, making it prone to unhandled exceptions that could crash the ap...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST COVERAGE AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Function: authenticate\n",
      "Priority: 9\n",
      "Lines: [7, 8, 9, 10, 11, 12]\n",
      "Missing scenarios: ['normal case: valid username and password', 'edge case: empty username and/or password', 'edge case: SQL injection attempt in username or password', 'error handling: database connection failure or query execution error', 'integration: interaction with actual database and user records']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue identified is a severe SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to compromise the database or bypass authentication. Additional concerns include missing cursor closure leading to potential resource leaks and lack of exception handling around database operations, both increasing the risk of application instability. Test coverage is notably deficient for the `authenticate` function, lacking scenarios for normal use, edges, security attempts, errors, and integration.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate method | user_auth.py | 7-10 | 9 | Security | Use parameterized queries with placeholders to safely insert user input values into the SQL query, preventing injection attacks. For example, use `query = \"SELECT * FROM users WHERE username=? AND password=?\"` and pass `(username, password)` as parameters to `cursor.execute()`. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Cursor Closure | user_auth.py | 8-9 | 5 | Performance | Use a context manager (with statement) when creating the cursor to ensure it is closed automatically, e.g., `with self.db.cursor() as cursor: cursor.execute(query, (username, password)) ...`. | Best Practices |\n",
      "| No Exception Handling for Database Operations | user_auth.py | 7-10 | 6 | Bug | Add try-except blocks to catch database exceptions and handle them gracefully, for example: `try: ... except sqlite3.DatabaseError as e: # Handle or log the error return False`. | Best Practices |\n",
      "| Missing tests for authenticate function | user_auth.py | 7-12 | 9 | Test Gap | Unit tests with mocking for query execution and integration tests with a test database, covering: normal case (valid username and password), edge cases (empty username/password, SQL injection attempts), error handling (database connection failure or query execution error), and integration with actual database and user records. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_file = Path(\"test-cases/01_sql_injection.diff\")\n",
    "diff_content = diff_file.read_text()\n",
    "\n",
    "report = await review_code(diff_content, save_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a73ded2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"You are an evaluation judge for code review systems comparing expected findings (ground truth) against actual findings.\n",
    "\n",
    "CRITICAL MATCHING RULES:\n",
    "1. Each actual finding can match AT MOST ONE expected finding\n",
    "2. Each expected finding can match AT MOST ONE actual finding\n",
    "3. Once an actual finding is matched, it CANNOT be used again\n",
    "4. Only match within same category (bugs ≠ test gaps)\n",
    "\n",
    "PROCESS:\n",
    "1. Count total_actual from \"Total Distinct Issues: X\" in report\n",
    "2. For EACH expected finding:\n",
    "   - Find the BEST matching actual finding that hasn't been used yet\n",
    "   - If good match exists: mark as matched=True, record which actual finding\n",
    "   - If no match: mark as matched=False\n",
    "   - NEVER reuse an actual finding for multiple expected findings\n",
    "\n",
    "A match means the same type of issue was identified, even if worded differently.\n",
    "\"\"\"\n",
    "\n",
    "class MatchedFinding(BaseModel):\n",
    "    expected: str = Field(description=\"the expected finding text\")\n",
    "    matched: bool = Field(description=\"true if the expected finding is present, else false\")\n",
    "    actual_finding: Optional[str] = Field(default=None, description=\"the matching text from report (if matched)\")\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    matched_findings: list[MatchedFinding]\n",
    "    total_expected: int = Field(description=\"Total number of expected findings from ground truth\")\n",
    "    total_actual: int = Field(description=\"Count of distinct issues in the report's summary section\")\n",
    "    # matches: int = Field(description=\"Number of expected findings successfully matched\")\n",
    "    \n",
    "    def model_post_init(self, __context):\n",
    "        # Calculate matches from the list\n",
    "        matches = sum(1 for mf in self.matched_findings if mf.matched)\n",
    "        \n",
    "        # Check for duplicate actual findings\n",
    "        actual_findings_used = [\n",
    "            mf.actual_finding for mf in self.matched_findings \n",
    "            if mf.matched and mf.actual_finding\n",
    "        ]\n",
    "        unique_actuals = len(set(actual_findings_used))\n",
    "        \n",
    "        if matches > unique_actuals:\n",
    "            print(f\"ERROR: {matches} matches but only {unique_actuals} unique actual findings used!\")\n",
    "            print(\"The judge matched the same actual finding multiple times.\")\n",
    "        \n",
    "        if matches > self.total_actual:\n",
    "            print(f\"WARNING: Matches ({matches}) > Total Actual ({self.total_actual})\")\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_report(report: str, ground_truth_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixed evaluation function with proper counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        name=\"Evaluation Judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"gpt-5.1\",\n",
    "        output_type=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "GROUND TRUTH (expected findings):\n",
    "{ground_truth_content}\n",
    "\n",
    "ACTUAL REPORT (what the system found):\n",
    "{report}\n",
    "\n",
    "For each expected finding, determine if it matches any actual finding.\n",
    "Output matched_findings list, total_expected, and total_actual.\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(judge_agent, prompt)\n",
    "    eval_result = result.final_output\n",
    "    \n",
    "    # Calculate matches from the actual data - don't trust LLM counting\n",
    "    matches = sum(1 for mf in eval_result.matched_findings if mf.matched)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = matches / eval_result.total_expected if eval_result.total_expected > 0 else 0\n",
    "    precision = matches / eval_result.total_actual if eval_result.total_actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"matches\": matches,\n",
    "        \"total_expected\": eval_result.total_expected,\n",
    "        \"total_actual\": eval_result.total_actual,\n",
    "        \"details\": eval_result.matched_findings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c9aa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run test 2\n",
    "# test_dir = Path(\"test-cases\")\n",
    "# diff_file = test_dir / \"01_sql_injection.diff\"\n",
    "\n",
    "# # Load files\n",
    "# diff_content = diff_file.read_text()\n",
    "# expected_file = diff_file.with_name(\"01_sql_injection_expected.json\")\n",
    "# ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# # Run review WITH saving\n",
    "# report = await review_code(diff_content, save_output=False)\n",
    "\n",
    "# # Evaluate\n",
    "# eval_result = await evaluate_report(report, ground_truth_content)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"JUDGE OUTPUT:\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "# print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "# print(f\"matches: {eval_result['matches']}\")\n",
    "# print(f\"\\nmatched_findings:\")\n",
    "# for mf in eval_result['details']:\n",
    "#     print(f\"\\n  Expected: {mf.expected}\")\n",
    "#     print(f\"  Matched: {mf.matched}\")\n",
    "#     if mf.actual_finding:\n",
    "#         print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"CALCULATED METRICS:\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "# print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "# print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a2694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the same test twice to check consistency\n",
    "# test_dir = Path(\"test-cases\")\n",
    "# diff_file = test_dir / \"04_multi_file_security.diff\"\n",
    "\n",
    "# # Load files\n",
    "# diff_content = diff_file.read_text()\n",
    "# expected_file = test_dir / \"04_multi_file_security_expected.json\"\n",
    "# ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# # Run twice\n",
    "# for i in range(2):\n",
    "#     print(f\"\\n=== RUN {i+1} ===\")\n",
    "    \n",
    "#     # Run review\n",
    "#     report = await review_code(diff_content, save_output=False)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     eval_result = await evaluate_report(report, ground_truth_content)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"JUDGE OUTPUT:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "#     print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "#     print(f\"matches: {eval_result['matches']}\")\n",
    "#     print(f\"\\nmatched_findings:\")\n",
    "#     for mf in eval_result['details']:\n",
    "#         print(f\"\\n  Expected: {mf.expected}\")\n",
    "#         print(f\"  Matched: {mf.matched}\")\n",
    "#         if mf.actual_finding:\n",
    "#             print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"CALCULATED METRICS:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "#     print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "#     print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b57fa37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING: 01_sql_injection\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical finding is a high-severity SQL injection vulnerability in the `authenticate` method of `user_auth.py`, which allows attackers to inject arbitrary SQL code via untrusted user inputs, potentially leading to data breaches or authentication bypass. Additional issues include missing error handling that could cause crashes during database operations and improper resource management without context managers, risking leaks. Finally, there is a significant test coverage gap for the `authenticate` function, lacking tests for normal, edge, error, and security scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate Method | user_auth.py | 7,8,9,10,11,12 | 9 | Security | Use parameterized queries with placeholders and pass user inputs as parameters to avoid SQL injection. For example, use: query = \"SELECT * FROM users WHERE username=? AND password=?\" and execute with cursor.execute(query, (username, password)) instead of string concatenation. | Code Analyzer, Security, Best Practices |\n",
      "| No Context Manager Used for Database Cursor | user_auth.py | 10,11 | 6 | Performance | Use the cursor as a context manager to ensure it closes properly: with self.db.cursor() as cursor: result = cursor.execute(query) ... | Best Practices |\n",
      "| Missing Error Handling in Database Operations | user_auth.py | 8,9,10,11,12 | 7 | Bug | Wrap database operations in try-except blocks catching specific exceptions (e.g., sqlite3.DatabaseError) and handle or propagate errors appropriately. | Best Practices |\n",
      "| Missing Test Coverage for authenticate function | user_auth.py | 7,8,9,10,11 | 9 | Test Gap | unit tests with mocks for database, integration tests with actual test database. Missing scenarios: normal case: valid username and password; edge case: empty username and/or password; edge case: SQL injection attempt in username and password; error condition: database connection failure; error condition: SQL execution error; integration: interaction with actual database with various user states | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 5\n",
      "total_actual: 4\n",
      "matches: 4\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: SQL injection vulnerability in authenticate method due to string concatenation of user inputs\n",
      "  Matched: True\n",
      "  Actual: SQL Injection Vulnerability in authenticate Method...\n",
      "\n",
      "  Expected: Missing docstring for authenticate method\n",
      "  Matched: False\n",
      "\n",
      "  Expected: No error handling for database query failures\n",
      "  Matched: True\n",
      "  Actual: Missing Error Handling in Database Operations...\n",
      "\n",
      "  Expected: Database cursor not explicitly closed (best practice)\n",
      "  Matched: True\n",
      "  Actual: No Context Manager Used for Database Cursor...\n",
      "\n",
      "  Expected: Missing test coverage for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing Test Coverage for authenticate function...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.80\n",
      "Precision: 1.00\n",
      "F1 Score: 0.89\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 02_logic_bug\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is an off-by-one error in the `find_item` method of `inventory.py`, which causes an IndexError and potential denial-of-service, flagged by multiple agents with high severity. Additional concerns include missing edge case handling in `get_last_n_items` leading to incorrect behavior, and significant test coverage gaps for both methods, particularly around error-prone scenarios like empty lists and invalid inputs. Style improvements for Pythonic iteration and documentation are recommended to enhance maintainability.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Off-by-one error in find_item method | inventory.py | [13,14,15,16,17] | 8 | Bug | Change the for loop range to range(len(self.items)) to avoid index out of range. For example: for i in range(len(self.items)): Alternatively, iterate directly over self.items to avoid index errors: for item in self.items: if item.id == item_id: return item. Add exception handling if needed. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Edge Case Handling in get_last_n_items | inventory.py | [3,9,10,11,12,13] | 6 | Bug | Add explicit checks for n to ensure it is positive and does not exceed the length of items. Document the expected input range. For example, if n <= 0 return an empty list; if n > len(self.items), return all items. | Best Practices |\n",
      "| Use of range(len()) Instead of enumerate in get_last_n_items | inventory.py | [9,10,11] | 3 | Style | Refactor the loop by either using enumerate or better yet, use list slicing: return self.items[-n:] with proper edge case handling. | Best Practices |\n",
      "| Use of range(len()) Instead of direct iteration in find_item | inventory.py | [15,16,17] | 3 | Style | Refactor to iterate directly over items: for item in self.items: if item.id == item_id: return item | Best Practices |\n",
      "| Missing Docstrings for methods | inventory.py | [8,14] | 4 | Style | Add proper docstrings to both methods detailing inputs, outputs, behaviors, and edge cases. | Best Practices |\n",
      "| Test coverage gap for get_last_n_items | inventory.py | [9,16] | 6 | Test Gap | unit test | Test Coverage |\n",
      "| Test coverage gap for find_item | inventory.py | [17,23] | 8 | Test Gap | unit test | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 7**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 6\n",
      "total_actual: 7\n",
      "matches: 6\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Off-by-one error in find_item causing IndexError - loop iterates beyond list bounds\n",
      "  Matched: True\n",
      "  Actual: Off-by-one error in find_item method...\n",
      "\n",
      "  Expected: Inefficient implementation of get_last_n_items - should use list slicing\n",
      "  Matched: True\n",
      "  Actual: Use of range(len()) Instead of enumerate in get_last_n_items...\n",
      "\n",
      "  Expected: No error handling for edge cases in get_last_n_items (n=0, n>length, negative n, empty list)\n",
      "  Matched: True\n",
      "  Actual: Missing Edge Case Handling in get_last_n_items...\n",
      "\n",
      "  Expected: Missing docstrings for both methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings for methods...\n",
      "\n",
      "  Expected: Missing test coverage for get_last_n_items\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for get_last_n_items...\n",
      "\n",
      "  Expected: Missing test coverage for find_item\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for find_item...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 1.00\n",
      "Precision: 0.86\n",
      "F1 Score: 0.92\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 03_code_quality\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code in calculator.py exhibits significant readability and maintainability issues, particularly with deeply nested conditional statements in the calculate method, which multiple agents flagged as an anti-pattern leading to complexity (highest severity 7). Critical bugs include missing input validation that could cause runtime exceptions (severity 6), alongside style issues like absent docstrings, unclear variable names, and unexplained magic numbers (severities 5-6). Test coverage is inadequate, with high-priority gaps in the calculate function (priority 9) covering normal, edge, and error scenarios, and lower-priority gaps in the process function (priority 5); all findings must be addressed to improve robustness and documentation.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Excessive Nested Conditionals and Repeated Early Returns | calculator.py | [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] | 7 | Style | Replace nested if statements with a single condition checking all parameters together (e.g., if all(param > 0 for param in [a, b, c, d, e])), handle validation once at the start, and return 0 only once if the condition fails to simplify control flow and eliminate repetition. | Code Analyzer, Best Practices |\n",
      "| Missing Input Validation | calculator.py | [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] | 6 | Bug | Add input validation to ensure all parameters are numbers before performing comparisons and calculations; use try-except blocks or explicit type checks to handle invalid inputs (e.g., None or incompatible types) gracefully. | Code Analyzer |\n",
      "| Missing Docstring in Class and Methods | calculator.py | [1, 3, 22] | 6 | Style | Add descriptive docstrings to the Calculator class and its methods following PEP 257 conventions, including purpose, parameters, return values, and exceptions (e.g., for calculate: document weighted sum if all inputs positive, else 0; for process: unchanged input return). | Best Practices |\n",
      "| Unclear Variable Names | calculator.py | [4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] | 5 | Style | Use descriptive variable names for inputs and intermediate values that indicate their purpose or meaning, following lowercase_with_underscores convention (e.g., replace 'a' with 'input_a' or a meaningful name). | Best Practices |\n",
      "| Magic Numbers without Explanation | calculator.py | [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] | 6 | Style | Define named constants at the top of the module with descriptive names for these multipliers (e.g., MULTIPLIER_A = 1.15), and use these constants in calculations with comments explaining their significance. | Best Practices |\n",
      "| Missing Tests for calculate function | calculator.py | [2, 24] | 9 | Test Gap | Add unit tests covering: normal case (all inputs positive), edge cases (one or more inputs zero/negative, boundary values around zero like 0.0001/-0.0001), error conditions (non-numeric inputs like strings/None, extremely large values), and integration scenarios with other methods or typical data. | Test Coverage |\n",
      "| Missing Tests for process function | calculator.py | [25, 26] | 5 | Test Gap | Add unit tests covering: normal case (typical data input), edge cases (empty input, None input, unexpected data types like integer/list), and error conditions (malformed data causing exceptions). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 7**\n",
      "============================================================\n",
      "\n",
      "ERROR: 7 matches but only 6 unique actual findings used!\n",
      "The judge matched the same actual finding multiple times.\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 8\n",
      "total_actual: 7\n",
      "matches: 7\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Excessive nesting depth in calculate method (5 levels deep)\n",
      "  Matched: True\n",
      "  Actual: Excessive Nested Conditionals and Repeated Early Returns...\n",
      "\n",
      "  Expected: Magic numbers without explanation (1.15, 0.95, 1.08, 1.12, 0.88)\n",
      "  Matched: True\n",
      "  Actual: Magic Numbers without Explanation...\n",
      "\n",
      "  Expected: Unclear variable names (a, b, c, d, e, x, y, z)\n",
      "  Matched: True\n",
      "  Actual: Unclear Variable Names...\n",
      "\n",
      "  Expected: Missing docstrings for class and both methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstring in Class and Methods...\n",
      "\n",
      "  Expected: calculate method returns 0 for negative inputs without clear justification and has repeated return statements\n",
      "  Matched: True\n",
      "  Actual: Excessive Nested Conditionals and Repeated Early Returns...\n",
      "\n",
      "  Expected: process method does nothing useful and should be removed or implemented\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for calculate method\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for calculate function...\n",
      "\n",
      "  Expected: Missing test coverage for process method\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for process function...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.88\n",
      "Precision: 1.00\n",
      "F1 Score: 0.93\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 04_multi_file_security\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code exhibits critical security vulnerabilities, including command injection in api_handler.py's execute_command method (severity 10) and path traversal issues in file_handler.py's read_user_file and delete_file methods (severity 9), which could lead to unauthorized access or execution of arbitrary commands. Additional high-severity concerns involve hardcoded credentials and insecure deserialization in api_handler.py, posing risks of credential leakage and remote code execution. Bugs like missing exception handling and style issues require fixes for robustness, while significant test gaps exist for functions handling user input and file operations to ensure comprehensive coverage.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Hardcoded Credentials in APIHandler | api_handler.py | [3,4,6,7] | 9 | Security | Remove hardcoded credentials from source code. Use environment variables, secure vaults, or configuration files to manage secrets (e.g., os.environ.get('API_KEY')) and validate their presence to prevent leakage. | Code Analyzer, Security, Best Practices |\n",
      "| Command Injection in execute_command | api_handler.py | [9,10] | 10 | Security | Avoid using os.system with unsanitized user input. Use subprocess.run with a list of arguments (e.g., subprocess.run(['echo', user_cmd])) and no shell=True, or strictly sanitize/validate user_cmd to prevent shell injection. | Code Analyzer, Security, Best Practices |\n",
      "| Insecure Deserialization in load_data | api_handler.py | [12,13] | 9 | Security | Avoid using pickle.load with untrusted data sources due to risks of arbitrary code execution. Use safer serialization like JSON, add try-except for exceptions (e.g., FileNotFoundError, UnpicklingError), validate data before unpickling, and confirm file existence. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Tests for execute_command | api_handler.py | [7,8,9] | 9 | Test Gap | unit test with mocking of os.system | Test Coverage |\n",
      "| Missing Tests for load_data | api_handler.py | [10,11,12,13] | 8 | Test Gap | unit test with controlled test files and exception handling checks | Test Coverage |\n",
      "| Path Traversal Vulnerability in read_user_file | file_handler.py | [2,3,4,5,6] | 9 | Security | Sanitize or validate filename input to prevent path traversal (e.g., disallow '../' sequences). Use os.path.join securely and ensure the resulting path is within the intended directory, with an allowlist of permitted filenames or access controls. | Code Analyzer, Security |\n",
      "| Missing Exception Handling for File Operations | file_handler.py | [3,4,5,9,10,11] | 7 | Bug | Wrap file open, read, and os.remove calls in try-except blocks to catch specific exceptions (e.g., FileNotFoundError, PermissionError, IOError) and handle them appropriately (e.g., propagate with informative messages) to prevent crashes. | Code Analyzer, Best Practices |\n",
      "| Path Traversal Vulnerability in delete_file | file_handler.py | [8,10,11] | 9 | Security | Validate and sanitize filename inputs to prevent path traversal (e.g., block '../' sequences). Use secure path joining methods and restrict deletions to authorized files only within the /tmp directory. | Security |\n",
      "| Improper Import Order in delete_file | file_handler.py | [9] | 3 | Style | Move all import statements (e.g., import os) to the top of the file, grouped by standard library, third-party, and local imports, with blank lines separating groups. | Best Practices |\n",
      "| Missing Tests for read_user_file | file_handler.py | [2,3,4,5,6] | 7 | Test Gap | unit test with file system mocks or temp files | Test Coverage |\n",
      "| Missing Tests for delete_file | file_handler.py | [7,8,9,10] | 7 | Test Gap | unit tests with mocks or temp files | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 11**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 11\n",
      "total_actual: 11\n",
      "matches: 10\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Hardcoded secrets (API key and password) in api_handler.py\n",
      "  Matched: True\n",
      "  Actual: Hardcoded Credentials in APIHandler...\n",
      "\n",
      "  Expected: Command injection vulnerability in execute_command via os.system with unsanitized user input\n",
      "  Matched: True\n",
      "  Actual: Command Injection in execute_command...\n",
      "\n",
      "  Expected: Insecure deserialization using pickle.load in load_data which can execute arbitrary code\n",
      "  Matched: True\n",
      "  Actual: Insecure Deserialization in load_data...\n",
      "\n",
      "  Expected: Path traversal vulnerability in read_user_file allowing access to arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in read_user_file...\n",
      "\n",
      "  Expected: Path traversal vulnerability in delete_file allowing deletion of arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in delete_file...\n",
      "\n",
      "  Expected: Missing error handling for file operations and database operations\n",
      "  Matched: True\n",
      "  Actual: Missing Exception Handling for File Operations...\n",
      "\n",
      "  Expected: Missing docstrings for classes and methods\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for execute_command including command injection attempts\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for execute_command...\n",
      "\n",
      "  Expected: Missing test coverage for load_data including malicious pickle payloads\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for load_data...\n",
      "\n",
      "  Expected: Missing test coverage for read_user_file including path traversal attempts and error conditions\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for read_user_file...\n",
      "\n",
      "  Expected: Missing test coverage for delete_file including path traversal attempts and error conditions\n",
      "  Matched: True\n",
      "  Actual: Missing Tests for delete_file...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.91\n",
      "Precision: 0.91\n",
      "F1 Score: 0.91\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 05_multi_file_mixed\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code exhibits critical security vulnerabilities, particularly in password hashing using the insecure MD5 algorithm in user_manager.py and weak session token generation in session_manager.py, which could lead to data breaches or session hijacking. Additional bugs include undefined methods and inadequate input validation across both files, while best practices issues like missing docstrings and error handling further degrade maintainability. Test coverage is insufficient for key functions, with multiple scenarios untested in create_user, get_users, validate_email, generate_token, and validate_session, requiring immediate unit tests to ensure reliability.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Weak Password Hashing Algorithm (MD5) | user_manager.py | [4,5,6,7,8,9,10] | 9 | Security | Replace MD5 hashing with a stronger algorithm like bcrypt, scrypt, or Argon2 that is designed for password hashing with salting and multiple iterations. Use a cryptographically secure library such as bcrypt or Argon2 to securely store passwords. | Code Analyzer, Security |\n",
      "| Undefined Method fetch_user Used | user_manager.py | [12,13,14] | 8 | Bug | Implement the fetch_user method or remove calls to it. Ensure it handles index properly and returns user objects or handle the user retrieval logic differently. | Code Analyzer |\n",
      "| validate_email Method Always Returns True | user_manager.py | [17,18,19,20] | 7 | Bug | Implement proper email validation using regex or a validation library to ensure email format correctness. Leverage existing, well-tested libraries to verify email format and validity. Implement actual email validation using regex or library functions or explicitly raise NotImplementedError if not implemented yet. | Code Analyzer, Security, Best Practices |\n",
      "| Inconsistent Return Types in get_users | user_manager.py | [12,16] | 5 | Bug | Ensure that fetch_user always returns a user object or raise exception. Also document get_users to specify return type is always a list of user dicts. Optionally, filter out None if needed. | Best Practices |\n",
      "| Missing Input Validation in create_user | user_manager.py | [6,12] | 6 | Style | Add input validation for username (non-empty string), password (non-empty string), and email (valid email format). Raise ValueError for invalid inputs. | Best Practices |\n",
      "| Missing Error Handling for Password Hashing | user_manager.py | [7,8] | 5 | Style | Add try-except block around password encoding and hashing to catch TypeError or AttributeError, and raise meaningful exceptions or handle errors gracefully. | Best Practices |\n",
      "| Missing Docstrings | user_manager.py | [2,6,10,14] | 4 | Style | Add PEP 257 compliant docstrings to each class and method, e.g., \"\"\"Create user with username, password, and email.\\n\\nParameters:\\n username (str): User's username.\\n password (str): Plain text password.\\n email (str): User email address.\\n\\nReturns:\\n dict: User info with hashed password.\\n\"\"\" | Best Practices |\n",
      "| Test Gap for create_user | user_manager.py | [3,12] | 8 | Test Gap | Unit test with scenarios: normal input, password with special characters, empty username, empty password, empty email, non-string inputs, verify password is hashed correctly, check hash length and format. | Test Coverage |\n",
      "| Test Gap for get_users | user_manager.py | [13,19] | 7 | Test Gap | Unit test with mocking for scenarios: count is zero, count is negative, count is very large, non-integer count, verify fetch_user is called correct number of times, handle fetch_user exceptions/failures. | Test Coverage |\n",
      "| Test Gap for validate_email | user_manager.py | [20,22] | 6 | Test Gap | Unit test for scenarios: valid email formats, invalid email formats, empty string email, null or None email, email with unicode characters, error handling if input is not string. | Test Coverage |\n",
      "| Session Token Generation Not Cryptographically Secure | session_manager.py | [4,5,6,7,8,9,10] | 8 | Security | Use secrets module (e.g., secrets.token_hex) or os.urandom to generate secure random tokens for sessions. Use a cryptographically secure random token generator such as os.urandom or secrets.token_hex to generate sufficiently random and unpredictable session tokens. | Code Analyzer, Security |\n",
      "| validate_session Method Simplistic Token Check | session_manager.py | [10,11,12,13,14,15] | 8 | Security | Implement proper session validation logic, verifying token validity against stored session data including expiry and match. Implement proper session management by checking the token against a secure session store or database to confirm session validity and user authentication. | Code Analyzer, Security |\n",
      "| Magic Number Without Explanation | session_manager.py | [6,9] | 3 | Style | Define a constant TOKEN_LENGTH = 16 at class or module level, use TOKEN_LENGTH in the loop for clarity and easy modification. | Best Practices |\n",
      "| Compare Boolean to True Explicitly | session_manager.py | [11,13] | 2 | Style | Return bool(token) directly to simplify the method, i.e. return bool(token) instead of if-else. | Best Practices |\n",
      "| Test Gap for generate_token | session_manager.py | [4,10] | 7 | Test Gap | Unit test for scenarios: token length is exactly 16, token contains only digits, consecutive calls generate unique tokens, check randomness distribution, handle randomness failures if any. | Test Coverage |\n",
      "| Test Gap for validate_session | session_manager.py | [11,14] | 7 | Test Gap | Unit test for scenarios: valid non-empty token returns True, empty string token returns False, None token returns False, token with whitespace, token with invalid characters. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 16**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 12\n",
      "total_actual: 16\n",
      "matches: 11\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Insecure password hashing using MD5 without salt in create_user\n",
      "  Matched: True\n",
      "  Actual: Weak Password Hashing Algorithm (MD5)...\n",
      "\n",
      "  Expected: Insecure token generation using random.randint with weak digit-only token space\n",
      "  Matched: True\n",
      "  Actual: Session Token Generation Not Cryptographically Secure...\n",
      "\n",
      "  Expected: Weak session validation in validate_session - only checks if token exists without verifying authenticity or expiry\n",
      "  Matched: True\n",
      "  Actual: validate_session Method Simplistic Token Check...\n",
      "\n",
      "  Expected: validate_email always returns True without any actual email validation logic\n",
      "  Matched: True\n",
      "  Actual: validate_email Method Always Returns True...\n",
      "\n",
      "  Expected: get_users calls undefined method fetch_user which will cause AttributeError at runtime\n",
      "  Matched: True\n",
      "  Actual: Undefined Method fetch_user Used...\n",
      "\n",
      "  Expected: Missing docstrings for classes and methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings...\n",
      "\n",
      "  Expected: Missing error handling in get_users and other methods\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for create_user\n",
      "  Matched: True\n",
      "  Actual: Test Gap for create_user...\n",
      "\n",
      "  Expected: Missing test coverage for generate_token\n",
      "  Matched: True\n",
      "  Actual: Test Gap for generate_token...\n",
      "\n",
      "  Expected: Missing test coverage for validate_email\n",
      "  Matched: True\n",
      "  Actual: Test Gap for validate_email...\n",
      "\n",
      "  Expected: Missing test coverage for get_users\n",
      "  Matched: True\n",
      "  Actual: Test Gap for get_users...\n",
      "\n",
      "  Expected: Missing test coverage for validate_session\n",
      "  Matched: True\n",
      "  Actual: Test Gap for validate_session...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.92\n",
      "Precision: 0.69\n",
      "F1 Score: 0.79\n",
      "Status: ✗ FAILED\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "✓ 01_sql_injection: R=0.80 P=1.00 F1=0.89\n",
      "✓ 02_logic_bug: R=1.00 P=0.86 F1=0.92\n",
      "✓ 03_code_quality: R=0.88 P=1.00 F1=0.93\n",
      "✓ 04_multi_file_security: R=0.91 P=0.91 F1=0.91\n",
      "✗ 05_multi_file_mixed: R=0.92 P=0.69 F1=0.79\n",
      "\n",
      "Passed: 4/5\n"
     ]
    }
   ],
   "source": [
    "# test all test cases\n",
    "\n",
    "test_cases = [\n",
    "    \"01_sql_injection\",\n",
    "    \"02_logic_bug\",\n",
    "    \"03_code_quality\",\n",
    "    \"04_multi_file_security\",\n",
    "    \"05_multi_file_mixed\"\n",
    "]\n",
    "\n",
    "async def run_all_tests():\n",
    "    test_dir = Path(\"test-cases\")\n",
    "    results = []\n",
    "    \n",
    "    for test_name in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {test_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load files\n",
    "        diff_file = test_dir / f\"{test_name}.diff\"\n",
    "        diff_content = diff_file.read_text()\n",
    "        expected_file = test_dir / f\"{test_name}_expected.json\"\n",
    "        ground_truth_content = expected_file.read_text()\n",
    "        \n",
    "        # Run review\n",
    "        report = await review_code(diff_content, save_output=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = await evaluate_report(report, ground_truth_content)\n",
    "        \n",
    "        # Print detailed judge output\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"JUDGE OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "        print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "        print(f\"matches: {eval_result['matches']}\")\n",
    "        print(f\"\\nmatched_findings:\")\n",
    "        for mf in eval_result['details']:\n",
    "            print(f\"\\n  Expected: {mf.expected}\")\n",
    "            print(f\"  Matched: {mf.matched}\")\n",
    "            if mf.actual_finding:\n",
    "                print(f\"  Actual: {mf.actual_finding[:100]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'recall': eval_result['recall'],\n",
    "            'precision': eval_result['precision'],\n",
    "            'f1': eval_result['f1'],\n",
    "            'passed': eval_result['recall'] >= 0.80 and \n",
    "                     eval_result['precision'] >= 0.85 and \n",
    "                     eval_result['f1'] >= 0.82\n",
    "        })\n",
    "        \n",
    "        # Print calculated metrics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATED METRICS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "        print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "        print(f\"F1 Score: {eval_result['f1']:.2f}\")\n",
    "        print(f\"Status: {'✓ PASSED' if results[-1]['passed'] else '✗ FAILED'}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        status = '✓' if result['passed'] else '✗'\n",
    "        print(f\"{status} {result['test_name']}: R={result['recall']:.2f} P={result['precision']:.2f} F1={result['f1']:.2f}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r['passed'])\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run all tests\n",
    "results = await run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
