{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_fast=LitellmModel(model=\"openrouter/x-ai/grok-4-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff. \n",
    "Identify bugs and anti-patterns including: logic errors, unhandled edge cases, null/undefined access, type mismatches, off-by-one errors, resource leaks (unclosed files/cursors/connections), infinite loops, missing error handling (no try-except blocks), code duplication, and overly complex functions. \n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff. \n",
    "Identify security vulnerabilities including: SQL injection, command injection, XSS vulnerabilities, hardcoded secrets/credentials, insecure authentication, path traversal, insecure deserialization, improper input validation, and missing error handling that could expose sensitive information.\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff. \n",
    "Identify code quality issues including: unclear variable names, functions exceeding 50 lines, nested complexity over 3 levels, missing docstrings, inconsistent formatting, magic numbers without explanation, violations of DRY principle, unclosed resources (files, database cursors, connections), and missing try-except blocks for error-prone operations.\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff. \n",
    "For each new or modified function, suggest test cases covering: normal input cases, edge cases (empty, null, boundary values), error conditions (exceptions, failures, timeouts), and integration scenarios.\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all_agents(diff):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent, diff),\n",
    "        Runner.run(best_practices_agent, diff),\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "When creating the report, follow these guidelines:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "3. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "4. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "5. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_fast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "#     \"\"\"\n",
    "#     Complete code review pipeline.\n",
    "    \n",
    "#     Args:\n",
    "#         diff: The code diff to review\n",
    "        \n",
    "#     Returns:\n",
    "#         Markdown-formatted code review report\n",
    "#     \"\"\"\n",
    "#     results = await run_all_agents(diff)\n",
    "#     code_result, security_result, best_practices_result, test_coverage_result = results    \n",
    "#     organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "#     report = await aggregator_agent(organized)\n",
    "    \n",
    "#     if save_output:\n",
    "#         os.makedirs(\"user-data\", exist_ok=True)\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "#         with open(filepath, \"w\") as f:\n",
    "#             f.write(report)\n",
    "#         print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "#     return report\n",
    "\n",
    "\n",
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    results = await run_all_agents(diff)\n",
    "    code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "    \n",
    "    # # DEBUG: Print all agent outputs\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"CODE ANALYZER RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in code_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"SECURITY AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in security_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"BEST PRACTICES AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in best_practices_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"TEST COVERAGE AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for gap in test_coverage_result.final_output.findings:\n",
    "    #     print(f\"\\nFunction: {gap.function_name}\")\n",
    "    #     print(f\"Priority: {gap.priority}\")\n",
    "    #     print(f\"Lines: {gap.lines}\")\n",
    "    #     print(f\"Missing scenarios: {gap.missing_scenarios}\")\n",
    "    # print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALLING AGGREGATOR...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = await aggregator_agent(organized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATOR OUTPUT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if save_output:\n",
    "        os.makedirs(\"user-data\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "211922ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CODE ANALYZER RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [8, 9, 10]\n",
      "Description: The authenticate method constructs SQL queries by directly concatenating user inputs (username and password) into the query string. This allows an att...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SECURITY AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection in authenticate method\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method constructs an SQL query by directly concatenating user inputs (username and password) into the query string. This allows an at...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BEST PRACTICES AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9]\n",
      "Description: The authenticate method builds the SQL query by concatenating user inputs directly into the query string. This approach is vulnerable to SQL injection...\n",
      "\n",
      "Title: Missing Cursor Closure\n",
      "Severity: 5\n",
      "Lines: [8, 9]\n",
      "Description: The database cursor created in the authenticate method is not explicitly closed, which can lead to resource leaks....\n",
      "\n",
      "Title: No Exception Handling for Database Operations\n",
      "Severity: 6\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method does not include try-except blocks around database operations, making it prone to unhandled exceptions that could crash the ap...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST COVERAGE AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Function: authenticate\n",
      "Priority: 9\n",
      "Lines: [7, 8, 9, 10, 11, 12]\n",
      "Missing scenarios: ['normal case: valid username and password', 'edge case: empty username and/or password', 'edge case: SQL injection attempt in username or password', 'error handling: database connection failure or query execution error', 'integration: interaction with actual database and user records']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue identified is a severe SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to compromise the database or bypass authentication. Additional concerns include missing cursor closure leading to potential resource leaks and lack of exception handling around database operations, both increasing the risk of application instability. Test coverage is notably deficient for the `authenticate` function, lacking scenarios for normal use, edges, security attempts, errors, and integration.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate method | user_auth.py | 7-10 | 9 | Security | Use parameterized queries with placeholders to safely insert user input values into the SQL query, preventing injection attacks. For example, use `query = \"SELECT * FROM users WHERE username=? AND password=?\"` and pass `(username, password)` as parameters to `cursor.execute()`. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Cursor Closure | user_auth.py | 8-9 | 5 | Performance | Use a context manager (with statement) when creating the cursor to ensure it is closed automatically, e.g., `with self.db.cursor() as cursor: cursor.execute(query, (username, password)) ...`. | Best Practices |\n",
      "| No Exception Handling for Database Operations | user_auth.py | 7-10 | 6 | Bug | Add try-except blocks to catch database exceptions and handle them gracefully, for example: `try: ... except sqlite3.DatabaseError as e: # Handle or log the error return False`. | Best Practices |\n",
      "| Missing tests for authenticate function | user_auth.py | 7-12 | 9 | Test Gap | Unit tests with mocking for query execution and integration tests with a test database, covering: normal case (valid username and password), edge cases (empty username/password, SQL injection attempts), error handling (database connection failure or query execution error), and integration with actual database and user records. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_file = Path(\"test-cases/01_sql_injection.diff\")\n",
    "diff_content = diff_file.read_text()\n",
    "\n",
    "report = await review_code(diff_content, save_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a73ded2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"You are an evaluation judge for code review systems comparing expected findings (ground truth) against actual findings.\n",
    "\n",
    "CRITICAL MATCHING RULES:\n",
    "1. Each actual finding can match AT MOST ONE expected finding\n",
    "2. Each expected finding can match AT MOST ONE actual finding\n",
    "3. Once an actual finding is matched, it CANNOT be used again\n",
    "4. Only match within same category (bugs ≠ test gaps)\n",
    "\n",
    "PROCESS:\n",
    "1. Count total_actual from \"Total Distinct Issues: X\" in report\n",
    "2. For EACH expected finding:\n",
    "   - Find the BEST matching actual finding that hasn't been used yet\n",
    "   - If good match exists: mark as matched=True, record which actual finding\n",
    "   - If no match: mark as matched=False\n",
    "   - NEVER reuse an actual finding for multiple expected findings\n",
    "\n",
    "A match means the same type of issue was identified, even if worded differently.\n",
    "\"\"\"\n",
    "\n",
    "class MatchedFinding(BaseModel):\n",
    "    expected: str = Field(description=\"the expected finding text\")\n",
    "    matched: bool = Field(description=\"true if the expected finding is present, else false\")\n",
    "    actual_finding: Optional[str] = Field(default=None, description=\"the matching text from report (if matched)\")\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    matched_findings: list[MatchedFinding]\n",
    "    total_expected: int = Field(description=\"Total number of expected findings from ground truth\")\n",
    "    total_actual: int = Field(description=\"Count of distinct issues in the report's summary section\")\n",
    "    # matches: int = Field(description=\"Number of expected findings successfully matched\")\n",
    "    \n",
    "    def model_post_init(self, __context):\n",
    "        # Calculate matches from the list\n",
    "        matches = sum(1 for mf in self.matched_findings if mf.matched)\n",
    "        \n",
    "        # Check for duplicate actual findings\n",
    "        actual_findings_used = [\n",
    "            mf.actual_finding for mf in self.matched_findings \n",
    "            if mf.matched and mf.actual_finding\n",
    "        ]\n",
    "        unique_actuals = len(set(actual_findings_used))\n",
    "        \n",
    "        if matches > unique_actuals:\n",
    "            print(f\"ERROR: {matches} matches but only {unique_actuals} unique actual findings used!\")\n",
    "            print(\"The judge matched the same actual finding multiple times.\")\n",
    "        \n",
    "        if matches > self.total_actual:\n",
    "            print(f\"WARNING: Matches ({matches}) > Total Actual ({self.total_actual})\")\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_report(report: str, ground_truth_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixed evaluation function with proper counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        name=\"Evaluation Judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"gpt-5.1\",\n",
    "        output_type=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "GROUND TRUTH (expected findings):\n",
    "{ground_truth_content}\n",
    "\n",
    "ACTUAL REPORT (what the system found):\n",
    "{report}\n",
    "\n",
    "For each expected finding, determine if it matches any actual finding.\n",
    "Output matched_findings list, total_expected, and total_actual.\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(judge_agent, prompt)\n",
    "    eval_result = result.final_output\n",
    "    \n",
    "    # Calculate matches from the actual data - don't trust LLM counting\n",
    "    matches = sum(1 for mf in eval_result.matched_findings if mf.matched)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = matches / eval_result.total_expected if eval_result.total_expected > 0 else 0\n",
    "    precision = matches / eval_result.total_actual if eval_result.total_actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"matches\": matches,\n",
    "        \"total_expected\": eval_result.total_expected,\n",
    "        \"total_actual\": eval_result.total_actual,\n",
    "        \"details\": eval_result.matched_findings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c9aa656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a high-severity SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to bypass authentication or access sensitive data; it must be addressed immediately using parameterized queries. Additional bugs include missing exception handling that risks application crashes, while best practices highlight resource leaks from unclosed cursors and lack of documentation. Test coverage for the `authenticate` function is severely lacking, with no tests for normal, edge, security, and error scenarios, requiring comprehensive unit and integration tests.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability | user_auth.py | 7-12 | 9 | Security | Use parameterized queries or prepared statements to safely pass user inputs to the SQL query, e.g., replace query construction with `query = \"SELECT * FROM users WHERE username=? AND password=?\"` and call `cursor.execute(query, (username, password))`. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Exception Handling for Database Operations | user_auth.py | 7-11 | 6 | Bug | Wrap database operations inside try-except blocks to catch potential sqlite3.DatabaseError or other exceptions and handle them gracefully. | Code Analyzer |\n",
      "| Unclosed Cursor Resource | user_auth.py | 9-10 | 5 | Performance | Use a context manager (with statement) for the cursor or explicitly close the cursor after use to ensure resources are properly released. | Best Practices |\n",
      "| Missing Docstring for Method | user_auth.py | 7,12 | 3 | Style | Add a docstring to the authenticate method explaining what it does, its input parameters, and what it returns. | Best Practices |\n",
      "| Test Coverage Gap in authenticate | user_auth.py | 7-11 | 9 | Test Gap | Add unit tests for function behavior including mocking database cursor (covering normal case: valid username and password returns True; edge case: empty username and/or password; edge case: SQL injection attempt in username or password; error handling: database connection failure; error handling: exceptions from malformed SQL query) and integration tests for full authentication flow with real database and user data. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 5\n",
      "total_actual: 5\n",
      "matches: 5\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: SQL injection vulnerability in authenticate method due to string concatenation of user inputs\n",
      "  Matched: True\n",
      "  Actual: SQL Injection Vulnerability | user_auth.py | 7-12 | 9 | Security | Use parameterized queries or prep...\n",
      "\n",
      "  Expected: Missing docstring for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing Docstring for Method | user_auth.py | 7,12 | 3 | Style | Add a docstring to the authenticate...\n",
      "\n",
      "  Expected: No error handling for database query failures\n",
      "  Matched: True\n",
      "  Actual: Missing Exception Handling for Database Operations | user_auth.py | 7-11 | 6 | Bug | Wrap database o...\n",
      "\n",
      "  Expected: Database cursor not explicitly closed (best practice)\n",
      "  Matched: True\n",
      "  Actual: Unclosed Cursor Resource | user_auth.py | 9-10 | 5 | Performance | Use a context manager (with state...\n",
      "\n",
      "  Expected: Missing test coverage for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Test Coverage Gap in authenticate | user_auth.py | 7-11 | 9 | Test Gap | Add unit tests for function...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Run test 2\n",
    "test_dir = Path(\"test-cases\")\n",
    "diff_file = test_dir / \"01_sql_injection.diff\"\n",
    "\n",
    "# Load files\n",
    "diff_content = diff_file.read_text()\n",
    "expected_file = diff_file.with_name(\"01_sql_injection_expected.json\")\n",
    "ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# Run review WITH saving\n",
    "report = await review_code(diff_content, save_output=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_result = await evaluate_report(report, ground_truth_content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JUDGE OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "print(f\"matches: {eval_result['matches']}\")\n",
    "print(f\"\\nmatched_findings:\")\n",
    "for mf in eval_result['details']:\n",
    "    print(f\"\\n  Expected: {mf.expected}\")\n",
    "    print(f\"  Matched: {mf.matched}\")\n",
    "    if mf.actual_finding:\n",
    "        print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATED METRICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a2694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the same test twice to check consistency\n",
    "# test_dir = Path(\"test-cases\")\n",
    "# diff_file = test_dir / \"04_multi_file_security.diff\"\n",
    "\n",
    "# # Load files\n",
    "# diff_content = diff_file.read_text()\n",
    "# expected_file = test_dir / \"04_multi_file_security_expected.json\"\n",
    "# ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# # Run twice\n",
    "# for i in range(2):\n",
    "#     print(f\"\\n=== RUN {i+1} ===\")\n",
    "    \n",
    "#     # Run review\n",
    "#     report = await review_code(diff_content, save_output=False)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     eval_result = await evaluate_report(report, ground_truth_content)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"JUDGE OUTPUT:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "#     print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "#     print(f\"matches: {eval_result['matches']}\")\n",
    "#     print(f\"\\nmatched_findings:\")\n",
    "#     for mf in eval_result['details']:\n",
    "#         print(f\"\\n  Expected: {mf.expected}\")\n",
    "#         print(f\"  Matched: {mf.matched}\")\n",
    "#         if mf.actual_finding:\n",
    "#             print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"CALCULATED METRICS:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "#     print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "#     print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b57fa37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING: 01_sql_injection\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical finding is a high-severity SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to execute arbitrary database commands and compromise the system. Additional issues include a resource leak from an unclosed database cursor and missing documentation via docstring, both of which impact maintainability and reliability. Test coverage for the `authenticate` method is severely lacking, with no tests for key scenarios like valid authentication, edge cases, error handling, and SQL injection attempts, necessitating comprehensive unit and integration tests.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate method | user_auth.py | [7,8,9,10,11,12] | 9 | Security | Use parameterized queries or prepared statements to safely include user inputs in the SQL query. For example, use: query = 'SELECT * FROM users WHERE username=? AND password=?' and then execute the query with parameters cursor.execute(query, (username, password)) to prevent SQL injection. | Code Analyzer, Security, Best Practices |\n",
      "| Missing docstring for authenticate method | user_auth.py | [7,12] | 4 | Style | Add a docstring to the 'authenticate' method explaining its function, input parameters, and return value. | Best Practices |\n",
      "| Unclosed cursor resource in authenticate method | user_auth.py | [9,10] | 5 | Performance | Use a context manager (with statement) to ensure the cursor is properly closed after use, e.g., 'with self.db.cursor() as cursor:'. | Best Practices |\n",
      "| Missing tests for authenticate function: normal case (valid username/password), edge cases (empty/special characters), error handling (SQL injection, DB failure, malformed query), integration with DB entries | user_auth.py | [7,11] | 9 | Test Gap | Unit tests with mocking for database interactions; integration tests with a test database. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 5\n",
      "total_actual: 4\n",
      "matches: 4\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: SQL injection vulnerability in authenticate method due to string concatenation of user inputs\n",
      "  Matched: True\n",
      "  Actual: SQL Injection Vulnerability in authenticate method...\n",
      "\n",
      "  Expected: Missing docstring for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing docstring for authenticate method...\n",
      "\n",
      "  Expected: No error handling for database query failures\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Database cursor not explicitly closed (best practice)\n",
      "  Matched: True\n",
      "  Actual: Unclosed cursor resource in authenticate method...\n",
      "\n",
      "  Expected: Missing test coverage for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing tests for authenticate function: normal case (valid username/password), edge cases (empty/sp...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.80\n",
      "Precision: 1.00\n",
      "F1 Score: 0.89\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 02_logic_bug\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is an off-by-one error in the `find_item` method of `inventory.py`, which can cause an IndexError crash and potentially lead to a denial of service, flagged at severity 8 by multiple agents. Additional performance inefficiencies exist in item search and retrieval methods, while missing docstrings reduce maintainability. High-priority test gaps are present for both `get_last_n_items` (priority 7) and `find_item` (priority 9), requiring comprehensive unit tests to cover edge cases and errors.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Off-by-one IndexError in find_item method | inventory.py | [12-18] | 8 | Bug | Change the loop to iterate over range(len(self.items)) or, preferably, directly over the items: `for item in self.items: if item.id == item_id: return item`. This prevents IndexError crashes that could lead to denial of service if unhandled. | Code Analyzer, Security, Best Practices |\n",
      "| Inefficient item search in find_item method | inventory.py | [12-18] | 4 | Performance | Replace index-based search with direct iteration: `for item in self.items: if item.id == item_id: return item`. This is more Pythonic and avoids unnecessary index calculations. | Best Practices |\n",
      "| Inefficient method to get last n items | inventory.py | [4-11] | 3 | Performance | Replace the entire `get_last_n_items` method with: `def get_last_n_items(self, n): return self.items[-n:]`. This uses efficient list slicing instead of manual looping. | Best Practices |\n",
      "| Missing docstrings for methods | inventory.py | [4,11,12] | 3 | Style | Add docstrings to `__init__`, `get_last_n_items`, and `find_item` methods per PEP-257 guidelines, describing purpose, parameters, and return values to improve readability and maintainability. | Best Practices |\n",
      "| Missing tests for get_last_n_items | inventory.py | [6,14] | 7 | Test Gap | Implement unit tests covering: normal case (n less than number of items), edge case (n equals zero), edge case (n greater than number of items), edge case (n is negative), and edge case (empty items list). | Test Coverage |\n",
      "| Missing tests for find_item | inventory.py | [15,22] | 9 | Test Gap | Implement unit tests covering: normal case (item_id exists), edge case (item_id does not exist), error case (empty items list), error case (index out of range due to loop), and error case (None or malformed items). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 6\n",
      "total_actual: 6\n",
      "matches: 5\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Off-by-one error in find_item causing IndexError - loop iterates beyond list bounds\n",
      "  Matched: True\n",
      "  Actual: Off-by-one IndexError in find_item method...\n",
      "\n",
      "  Expected: Inefficient implementation of get_last_n_items - should use list slicing\n",
      "  Matched: True\n",
      "  Actual: Inefficient method to get last n items...\n",
      "\n",
      "  Expected: No error handling for edge cases in get_last_n_items (n=0, n>length, negative n, empty list)\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for both methods\n",
      "  Matched: True\n",
      "  Actual: Missing docstrings for methods...\n",
      "\n",
      "  Expected: Missing test coverage for get_last_n_items\n",
      "  Matched: True\n",
      "  Actual: Missing tests for get_last_n_items...\n",
      "\n",
      "  Expected: Missing test coverage for find_item\n",
      "  Matched: True\n",
      "  Actual: Missing tests for find_item...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.83\n",
      "Precision: 0.83\n",
      "F1 Score: 0.83\n",
      "Status: ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: 03_code_quality\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is the excessive nested if-statements in the `calculate` function, which introduce high complexity (severity 7), potential logic errors from incomplete parameter checks (severity 5), poor readability, and code duplication, requiring immediate refactoring. Additional concerns include magic numbers without explanation and missing docstrings, both impacting maintainability. Test coverage gaps are significant for the `calculate` function (priority 8), with lesser gaps for `process` (priority 3), necessitating comprehensive unit and integration tests for edge cases and error handling.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Deeply nested if-statements causing poor readability, incomplete positive value checks, and redundant else branches | calculator.py | [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] | 7 | Bug | Refactor the nested if conditions into a single combined condition using logical operators, for example: if all(x > 0 for x in [a,b,c,d,e]) to improve clarity, avoid logic errors from incomplete checks on parameters like d and e, and eliminate multiple redundant else returns of 0 by using a single return after the check. | Code Analyzer, Best Practices |\n",
      "| Magic Numbers Without Explanation | calculator.py | [10, 11, 12, 13] | 5 | Style | Define these constants as clearly named variables or class attributes with descriptive names and add comments explaining their purpose. | Best Practices |\n",
      "| Missing Docstrings | calculator.py | [1, 4, 22] | 4 | Style | Add appropriate docstrings to the class and methods following standard conventions to improve code documentation. | Best Practices |\n",
      "| Test coverage gap for calculate function | calculator.py | [2, 24] | 8 | Test Gap | Implement unit tests covering normal input cases with all positive values, edge cases where one or more inputs are zero or negative, error conditions such as non-numeric inputs or None values, boundary values like inputs equal to zero or very large numbers, and integration with other components expecting correct numeric output. | Test Coverage |\n",
      "| Test coverage gap for process function | calculator.py | [25, 26] | 3 | Test Gap | Implement unit or integration tests covering normal input case with typical data input, edge case with empty input or None, error conditions like unsupported data types or malformed input, and integration test if process is expected to interact with other system parts. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 8\n",
      "total_actual: 5\n",
      "matches: 5\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Excessive nesting depth in calculate method (5 levels deep)\n",
      "  Matched: True\n",
      "  Actual: Deeply nested if-statements causing poor readability, incomplete positive value checks, and redundan...\n",
      "\n",
      "  Expected: Magic numbers without explanation (1.15, 0.95, 1.08, 1.12, 0.88)\n",
      "  Matched: True\n",
      "  Actual: Magic Numbers Without Explanation...\n",
      "\n",
      "  Expected: Unclear variable names (a, b, c, d, e, x, y, z)\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for class and both methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings...\n",
      "\n",
      "  Expected: calculate method returns 0 for negative inputs without clear justification and has repeated return statements\n",
      "  Matched: False\n",
      "\n",
      "  Expected: process method does nothing useful and should be removed or implemented\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for calculate method\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for calculate function...\n",
      "\n",
      "  Expected: Missing test coverage for process method\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for process function...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.62\n",
      "Precision: 1.00\n",
      "F1 Score: 0.77\n",
      "Status: ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: 04_multi_file_security\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The codebase contains several critical security vulnerabilities, including hardcoded sensitive credentials, command injection risks, insecure deserialization via pickle, and path traversal issues in file handling, all rated severity 9-10 and requiring immediate fixes to prevent exploitation. Style and best practice issues like missing docstrings, magic file paths, and improper import placement reduce maintainability but are lower priority. Test coverage is incomplete for key functions, with gaps in handling normal, edge, and error scenarios for methods like execute_command, load_data, read_user_file, and delete_file, necessitating additional unit tests.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Hardcoded Sensitive Credentials | api_handler.py | [3,4,5,6,7,8] | 9 | Security | Use environment variables or secure vaults to store sensitive credentials instead of hardcoding them in the code. Remove hardcoded credentials and load them securely from environment variables or secure vaults. | Code Analyzer, Security, Best Practices |\n",
      "| Command Injection in execute_command | api_handler.py | [8,9,10,11] | 10 | Security | Avoid using os.system with user input. Use subprocess.run with argument list or sanitize user_cmd to prevent shell injection. Avoid using os.system with unsanitized user input. Use safer alternatives like subprocess.run with argument lists and sanitize or validate input. Avoid using os.system for executing commands with user input. Use the subprocess module with proper argument handling. | Code Analyzer, Security, Best Practices |\n",
      "| Insecure Deserialization via pickle.load | api_handler.py | [11,12,13,14] | 9 | Security | Validate or sanitize the pickle file source or replace pickle with a safer serialization format such as JSON. Alternatively, restrict which files can be loaded. Avoid using pickle.load on untrusted data. Use safer serialization formats like JSON or ensure files are from trusted sources with validation. | Code Analyzer, Security |\n",
      "| Missing Docstrings in Classes and Methods | api_handler.py | [3,6,8,11] | 4 | Style | Add descriptive docstrings to classes and all methods explaining their purpose and parameters. | Best Practices |\n",
      "| Test Gap: execute_command | api_handler.py | [7,10] | 9 | Test Gap | unit test with mocking of os.system | Test Coverage |\n",
      "| Test Gap: load_data | api_handler.py | [11,14] | 8 | Test Gap | unit test with file mocks and exception handling tests | Test Coverage |\n",
      "| Path Traversal Vulnerability in read_user_file | file_handler.py | [3,4,5,6,7] | 10 | Security | Sanitize filename to prevent '..' or absolute paths. Use os.path.join with validation to ensure the final path is within allowed directories. Validate and sanitize filename inputs to prevent '../' sequences or use a safe method to construct paths, such as pathlib with strict checks. | Code Analyzer, Security |\n",
      "| Path Traversal Vulnerability in delete_file | file_handler.py | [8,9,10,11] | 10 | Security | Sanitize or validate filename input to prevent directory traversal attacks. Use secure methods to build file paths. Sanitize and validate filename inputs to prevent path traversal, or restrict deletions to allowed filenames only. | Code Analyzer, Security |\n",
      "| Hardcoded Magic File Paths Without Explanation | file_handler.py | [3,5,10] | 5 | Style | Define file paths as configurable class attributes or parameters and explain their usage. | Best Practices |\n",
      "| Import Statement Inside Method | file_handler.py | [9] | 3 | Style | Move all imports to the top of the file for readability and best practice. | Best Practices |\n",
      "| Test Gap: read_user_file | file_handler.py | [2,7] | 8 | Test Gap | unit test with filesystem mocks | Test Coverage |\n",
      "| Test Gap: delete_file | file_handler.py | [8,11] | 7 | Test Gap | unit test with os.remove and filesystem mocks | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 12**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 11\n",
      "total_actual: 12\n",
      "matches: 10\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Hardcoded secrets (API key and password) in api_handler.py\n",
      "  Matched: True\n",
      "  Actual: Hardcoded Sensitive Credentials | api_handler.py...\n",
      "\n",
      "  Expected: Command injection vulnerability in execute_command via os.system with unsanitized user input\n",
      "  Matched: True\n",
      "  Actual: Command Injection in execute_command | api_handler.py...\n",
      "\n",
      "  Expected: Insecure deserialization using pickle.load in load_data which can execute arbitrary code\n",
      "  Matched: True\n",
      "  Actual: Insecure Deserialization via pickle.load | api_handler.py...\n",
      "\n",
      "  Expected: Path traversal vulnerability in read_user_file allowing access to arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in read_user_file | file_handler.py...\n",
      "\n",
      "  Expected: Path traversal vulnerability in delete_file allowing deletion of arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in delete_file | file_handler.py...\n",
      "\n",
      "  Expected: Missing error handling for file operations and database operations\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for classes and methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings in Classes and Methods | api_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for execute_command including command injection attempts\n",
      "  Matched: True\n",
      "  Actual: Test Gap: execute_command | api_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for load_data including malicious pickle payloads\n",
      "  Matched: True\n",
      "  Actual: Test Gap: load_data | api_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for read_user_file including path traversal attempts and error conditions\n",
      "  Matched: True\n",
      "  Actual: Test Gap: read_user_file | file_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for delete_file including path traversal attempts and error conditions\n",
      "  Matched: True\n",
      "  Actual: Test Gap: delete_file | file_handler.py...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.91\n",
      "Precision: 0.83\n",
      "F1 Score: 0.87\n",
      "Status: ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: 05_multi_file_mixed\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The codebase exhibits critical security vulnerabilities, particularly in password hashing using the insecure MD5 algorithm in user_manager.py and low-entropy session token generation in session_manager.py, which could lead to credential compromise or session hijacking. Additional bugs include undefined methods and ineffective validation functions that may cause runtime errors or accept invalid inputs. Test coverage is insufficient across key functions, with multiple missing scenarios that need unit tests to ensure robustness.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Use of MD5 for Password Hashing | user_manager.py | [3,6,7,8] | 9 | Security | Replace hashlib.md5 with a secure password hashing library such as bcrypt. Include salt and proper iteration to protect stored passwords. | Code Analyzer, Security, Best Practices |\n",
      "| Undefined fetch_user Method | user_manager.py | [11,12] | 8 | Bug | Implement the fetch_user method in UserManager or ensure it is inherited from a superclass. Alternatively, adjust get_users to retrieve users from an existing data source. | Code Analyzer, Best Practices |\n",
      "| validate_email Always Returns True | user_manager.py | [15,16,17] | 5 | Bug | Implement actual email validation logic, for example by using regex or external libraries to check email format correctness. | Code Analyzer, Security, Best Practices |\n",
      "| Missing docstrings in classes and methods | user_manager.py | [2,4,6,12,16] | 4 | Style | Add descriptive docstrings to classes and all methods explaining their purpose, parameters, and return values. | Best Practices |\n",
      "| Test Gap for create_user | user_manager.py | [3,9] | 8 | Test Gap | unit test | Test Coverage |\n",
      "| Test Gap for get_users | user_manager.py | [10,16] | 7 | Test Gap | unit test with mocks | Test Coverage |\n",
      "| Test Gap for validate_email | user_manager.py | [17,19] | 5 | Test Gap | unit test | Test Coverage |\n",
      "| Session Token Generation Using Random Digits Only | session_manager.py | [4,5,6,7,8,9,10] | 9 | Security | Use a secure source of randomness and include alphanumeric characters or hex digits to increase token entropy. Consider using secrets.token_hex or os.urandom. | Code Analyzer, Security, Best Practices |\n",
      "| validate_session Method Too Permissive | session_manager.py | [11,12,13,14] | 6 | Bug | Implement actual session validation logic by checking token existence, expiration, and integrity against stored sessions or a database. | Code Analyzer |\n",
      "| Inefficient string concatenation in generate_token | session_manager.py | [7,8,9,10,11] | 3 | Performance | Use a list to collect characters and join them at the end or use built-in token generation functions from the secrets module. | Best Practices |\n",
      "| Test Gap for generate_token | session_manager.py | [3,11] | 7 | Test Gap | unit test | Test Coverage |\n",
      "| Test Gap for validate_session | session_manager.py | [12,16] | 7 | Test Gap | unit test | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 12**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 12\n",
      "total_actual: 12\n",
      "matches: 11\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Insecure password hashing using MD5 without salt in create_user\n",
      "  Matched: True\n",
      "  Actual: Use of MD5 for Password Hashing...\n",
      "\n",
      "  Expected: Insecure token generation using random.randint with weak digit-only token space\n",
      "  Matched: True\n",
      "  Actual: Session Token Generation Using Random Digits Only...\n",
      "\n",
      "  Expected: Weak session validation in validate_session - only checks if token exists without verifying authenticity or expiry\n",
      "  Matched: True\n",
      "  Actual: validate_session Method Too Permissive...\n",
      "\n",
      "  Expected: validate_email always returns True without any actual email validation logic\n",
      "  Matched: True\n",
      "  Actual: validate_email Always Returns True...\n",
      "\n",
      "  Expected: get_users calls undefined method fetch_user which will cause AttributeError at runtime\n",
      "  Matched: True\n",
      "  Actual: Undefined fetch_user Method...\n",
      "\n",
      "  Expected: Missing docstrings for classes and methods\n",
      "  Matched: True\n",
      "  Actual: Missing docstrings in classes and methods...\n",
      "\n",
      "  Expected: Missing error handling in get_users and other methods\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for create_user\n",
      "  Matched: True\n",
      "  Actual: Test Gap for create_user...\n",
      "\n",
      "  Expected: Missing test coverage for generate_token\n",
      "  Matched: True\n",
      "  Actual: Test Gap for generate_token...\n",
      "\n",
      "  Expected: Missing test coverage for validate_email\n",
      "  Matched: True\n",
      "  Actual: Test Gap for validate_email...\n",
      "\n",
      "  Expected: Missing test coverage for get_users\n",
      "  Matched: True\n",
      "  Actual: Test Gap for get_users...\n",
      "\n",
      "  Expected: Missing test coverage for validate_session\n",
      "  Matched: True\n",
      "  Actual: Test Gap for validate_session...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.92\n",
      "Precision: 0.92\n",
      "F1 Score: 0.92\n",
      "Status: ✓ PASSED\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "✓ 01_sql_injection: R=0.80 P=1.00 F1=0.89\n",
      "✗ 02_logic_bug: R=0.83 P=0.83 F1=0.83\n",
      "✗ 03_code_quality: R=0.62 P=1.00 F1=0.77\n",
      "✗ 04_multi_file_security: R=0.91 P=0.83 F1=0.87\n",
      "✓ 05_multi_file_mixed: R=0.92 P=0.92 F1=0.92\n",
      "\n",
      "Passed: 2/5\n"
     ]
    }
   ],
   "source": [
    "# test all test cases\n",
    "\n",
    "test_cases = [\n",
    "    \"01_sql_injection\",\n",
    "    \"02_logic_bug\",\n",
    "    \"03_code_quality\",\n",
    "    \"04_multi_file_security\",\n",
    "    \"05_multi_file_mixed\"\n",
    "]\n",
    "\n",
    "async def run_all_tests():\n",
    "    test_dir = Path(\"test-cases\")\n",
    "    results = []\n",
    "    \n",
    "    for test_name in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {test_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load files\n",
    "        diff_file = test_dir / f\"{test_name}.diff\"\n",
    "        diff_content = diff_file.read_text()\n",
    "        expected_file = test_dir / f\"{test_name}_expected.json\"\n",
    "        ground_truth_content = expected_file.read_text()\n",
    "        \n",
    "        # Run review\n",
    "        report = await review_code(diff_content, save_output=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = await evaluate_report(report, ground_truth_content)\n",
    "        \n",
    "        # Print detailed judge output\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"JUDGE OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "        print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "        print(f\"matches: {eval_result['matches']}\")\n",
    "        print(f\"\\nmatched_findings:\")\n",
    "        for mf in eval_result['details']:\n",
    "            print(f\"\\n  Expected: {mf.expected}\")\n",
    "            print(f\"  Matched: {mf.matched}\")\n",
    "            if mf.actual_finding:\n",
    "                print(f\"  Actual: {mf.actual_finding[:100]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'recall': eval_result['recall'],\n",
    "            'precision': eval_result['precision'],\n",
    "            'f1': eval_result['f1'],\n",
    "            'passed': eval_result['recall'] >= 0.80 and \n",
    "                     eval_result['precision'] >= 0.85 and \n",
    "                     eval_result['f1'] >= 0.82\n",
    "        })\n",
    "        \n",
    "        # Print calculated metrics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATED METRICS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "        print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "        print(f\"F1 Score: {eval_result['f1']:.2f}\")\n",
    "        print(f\"Status: {'✓ PASSED' if results[-1]['passed'] else '✗ FAILED'}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        status = '✓' if result['passed'] else '✗'\n",
    "        print(f\"{status} {result['test_name']}: R={result['recall']:.2f} P={result['precision']:.2f} F1={result['f1']:.2f}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r['passed'])\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run all tests\n",
    "results = await run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
