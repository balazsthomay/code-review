{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_1_fast=LitellmModel(model=\"openrouter/x-ai/grok-4.1-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers (max 20 lines per finding)\", max_length=20)\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code (max 20 lines)\", max_length=20)\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added deletion analysis, chain-of-thought, and better semantic understanding\n",
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed: What code was added? What was removed? What was modified?\n",
    "2. Then, identify potential issues in the changes\n",
    "3. Consider the inverse: What functionality might be LOST from deletions?\n",
    "\n",
    "DELETION ANALYSIS (CRITICAL):\n",
    "- When you see removed code (lines starting with -), pay special attention to:\n",
    "  * Entire functions/classes being deleted - flag if they're called elsewhere\n",
    "  * Helper functions removed - check if remaining code still works without them\n",
    "  * Error handling removed - flag if this makes code less safe\n",
    "  * Imports removed - verify they're truly unused\n",
    "- If 10+ consecutive lines are deleted, describe what functionality is being removed\n",
    "\n",
    "BUG PATTERNS TO IDENTIFY:\n",
    "- Logic errors, unhandled edge cases, null/undefined access, type mismatches\n",
    "- Off-by-one errors, resource leaks (unclosed files/cursors/connections)\n",
    "- Infinite loops, missing error handling (no try-except blocks)\n",
    "- Code duplication, overly complex functions\n",
    "- Removed functionality that breaks remaining code\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines where the issue occurs (max 20 lines per finding). \n",
    "Do NOT list entire files or large ranges. Be precise and focused.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. First, describe what changed from a security perspective\n",
    "2. Identify what security controls or validations were added or removed\n",
    "3. Consider: Does this change introduce new attack surface?\n",
    "\n",
    "SECURITY PATTERNS:\n",
    "- SQL injection, command injection, XSS vulnerabilities\n",
    "- Hardcoded secrets/credentials, insecure authentication\n",
    "- Path traversal, insecure deserialization\n",
    "- Improper input validation\n",
    "- Missing error handling that could expose sensitive information\n",
    "- Removed security checks or validation code\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If security-related code is removed (validation, sanitization, auth checks), flag it as HIGH severity\n",
    "- Consider what protections are LOST, not just what bugs are added\n",
    "\n",
    "IMPORTANT: For each vulnerability, specify ONLY the specific lines where the vulnerability exists (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Focus on the exact vulnerable code location.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Describe what changed in terms of code quality\n",
    "2. Identify violations of best practices in the new/modified code\n",
    "3. Consider: Does this change make the code harder to maintain?\n",
    "\n",
    "CODE QUALITY ISSUES:\n",
    "- Unclear variable names, functions exceeding 50 lines\n",
    "- Nested complexity over 3 levels, missing docstrings\n",
    "- Inconsistent formatting, magic numbers without explanation\n",
    "- Violations of DRY principle\n",
    "- Unclosed resources (files, database cursors, connections)\n",
    "- Missing try-except blocks for error-prone operations\n",
    "\n",
    "DELETION AWARENESS:\n",
    "- If helpful comments, docstrings, or error handling are removed, flag it\n",
    "- If code is simplified but loses clarity, mention it\n",
    "\n",
    "IMPORTANT: For each issue, specify ONLY the specific lines with the violation (max 20 lines per finding).\n",
    "Do NOT list entire files or large ranges. Be specific and targeted.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff.\n",
    "\n",
    "ANALYSIS APPROACH:\n",
    "1. Identify what functions/methods are new or modified\n",
    "2. For each, list what test scenarios are needed\n",
    "3. Consider edge cases and error conditions\n",
    "\n",
    "For each new or modified function, suggest test cases covering:\n",
    "- Normal input cases\n",
    "- Edge cases (empty, null, boundary values)\n",
    "- Error conditions (exceptions, failures, timeouts)\n",
    "- Integration scenarios\n",
    "\n",
    "IMPORTANT: For each gap, specify ONLY the specific lines of the function needing tests (max 20 lines per gap).\n",
    "Do NOT list entire files. Focus on the specific untested function location.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb32706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_python_gotchas(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant Python gotchas patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    python_gotchas_collection = chroma_client.get_collection(name=\"python_gotchas_patterns\")\n",
    "    results = python_gotchas_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_code_review_patterns(code_diff: str, n_results: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant code review patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    code_review_collection = chroma_client.get_collection(name=\"code_review_patterns\")\n",
    "    results = code_review_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_refactoring_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant refactoring patterns from ChromaDB (multi-file changes, shotgun surgery, etc.)\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    refactoring_collection = chroma_client.get_collection(name=\"refactoring_patterns\")\n",
    "    results = refactoring_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_all_agents(diff):\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer, diff),\n",
    "#         Runner.run(security_agent, diff),\n",
    "#         Runner.run(best_practices_agent, diff),\n",
    "#         Runner.run(test_coverage_agent, diff)\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "async def run_all_agents(diff):\n",
    "    # Get RAG context for all agents\n",
    "    # INCREASED n_results from 5 to 15 for security patterns to capture more injection patterns\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=15)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    python_gotchas = get_relevant_python_gotchas(diff, n_results=3)\n",
    "    code_review_patterns = get_relevant_code_review_patterns(diff, n_results=3)\n",
    "    refactoring_patterns = get_relevant_refactoring_patterns(diff, n_results=5)  # NEW: Multi-file refactoring patterns\n",
    "    \n",
    "    # Create RAG-enhanced Code Analyzer agent (UPDATED: with all patterns including refactoring)\n",
    "    enhanced_code_analyzer_instructions = f\"\"\"{code_analyzer_instructions}\n",
    "\n",
    "RELEVANT PYTHON GOTCHAS TO CHECK:\n",
    "{python_gotchas}\n",
    "\n",
    "RELEVANT CODE REVIEW PATTERNS TO CHECK:\n",
    "{code_review_patterns}\n",
    "\n",
    "RELEVANT REFACTORING PATTERNS TO CHECK (Multi-File Changes):\n",
    "{refactoring_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    code_analyzer_rag = Agent(\n",
    "        name=\"Code Analyzer\",\n",
    "        instructions=enhanced_code_analyzer_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=CodeAnalyzerOutput\n",
    "    )\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            max_tokens=4000,\n",
    "        ),\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer_rag, diff),  # Now uses RAG with refactoring patterns!\n",
    "        Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(best_practices_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(test_coverage_agent, diff)  # No RAG needed for test coverage\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Added multi-file awareness and cross-file dependency detection\n",
    "\n",
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "CRITICAL: Output your report as plain text/markdown. Do NOT wrap your response in JSON or code fences.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "AGGREGATION GUIDELINES:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. MULTI-FILE AWARENESS (CRITICAL):\n",
    "   - If findings span multiple files, check for cross-file dependencies\n",
    "   - Flag if changes in one file might break APIs/contracts in another file\n",
    "   - Look for patterns like: \"File A removes function X, but does File B call it?\"\n",
    "   - Consider the bigger picture: Do these changes work together?\n",
    "\n",
    "3. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "4. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "5. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "6. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings. If multi-file change, mention cross-file implications]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_1_fast,\n",
    "    model_settings=ModelSettings(\n",
    "            temperature=0.6,\n",
    "            extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    with trace(\"Multi-Agent Code Review\"):\n",
    "        results = await run_all_agents(diff)\n",
    "        code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "        \n",
    "        organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALLING AGGREGATOR...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report = await aggregator_agent(organized)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AGGREGATOR OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(report)\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        if save_output:\n",
    "            os.makedirs(\"user-data\", exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {filepath}\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Utilities\n",
    "import re\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Strip brackets like [82-85] -> 82-85\n",
    "                    lines_str = lines_str.strip('[]')\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13030d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 17 CVEs\n",
      "\n",
      "CWE Coverage:\n",
      "  Code Injection: 1\n",
      "  Command Injection: 1\n",
      "  Cross-Site Scripting: 1\n",
      "  Improper Certificate Validation: 1\n",
      "  Improper Input Validation: 1\n",
      "  Information Exposure: 4\n",
      "  Insecure Deserialization: 1\n",
      "  NULL Pointer Dereference: 1\n",
      "  OS Command Injection: 1\n",
      "  Path Traversal: 1\n",
      "  SQL Injection: 2\n",
      "  Uncontrolled Resource Consumption: 2\n"
     ]
    }
   ],
   "source": [
    "# CVE Dataset Loading Functions\n",
    "\n",
    "def load_cve_dataset(json_path: str = \"cve_dataset.json\") -> list[dict]:\n",
    "    \"\"\"Load curated CVE dataset\"\"\"\n",
    "    with open(json_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_cve_patch(cve_id: str, patches_dir: str = \"cve_patches\") -> str:\n",
    "    \"\"\"Load patch file for specific CVE\"\"\"\n",
    "    patch_path = Path(patches_dir) / f\"{cve_id}.patch\"\n",
    "    return patch_path.read_text()\n",
    "\n",
    "# Test loading\n",
    "cve_dataset = load_cve_dataset()\n",
    "print(f\"‚úì Loaded {len(cve_dataset)} CVEs\")\n",
    "print(f\"\\nCWE Coverage:\")\n",
    "cwe_counts = {}\n",
    "for cve in cve_dataset:\n",
    "    cwe = cve['cwe_name']\n",
    "    cwe_counts[cwe] = cwe_counts.get(cwe, 0) + 1\n",
    "\n",
    "for cwe, count in sorted(cwe_counts.items()):\n",
    "    print(f\"  {cwe}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "h14kceo638o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: Helper Functions\n",
    "\n",
    "def check_security_agent_flagged(report: str) -> bool:\n",
    "    \"\"\"Check if Security Agent found anything\"\"\"\n",
    "    return \"Security\" in report and \"Found By\" in report\n",
    "\n",
    "def extract_max_severity(report: str) -> int:\n",
    "    \"\"\"Extract highest severity from report (1-10 scale)\"\"\"\n",
    "    import re\n",
    "    # Parse markdown table for severity column (finds Security findings)\n",
    "    severities = re.findall(r'\\|\\s*(\\d+)\\s*\\|.*\\|\\s*Security\\s*\\|', report, re.IGNORECASE)\n",
    "    return max(map(int, severities)) if severities else 0\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nxhejx209k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE-Specific Evaluation\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid_cve(report: str, patch: str, \n",
    "                               cve_id: str, cwe_id: str, cwe_name: str,\n",
    "                               cvss_score: float, severity: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation for CVEs: Location metrics + LLM relevance + Security detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance with CVE context\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_cve_instructions = f\"\"\"You are evaluating code review findings against a real CVE.\n",
    "\n",
    "CRITICAL: Output ONLY valid JSON matching the specified schema. Do NOT wrap your response in markdown code fences or backticks.\n",
    "\n",
    "Given:\n",
    "1. CVE ID: {cve_id}\n",
    "2. CWE Type: {cwe_name} ({cwe_id})\n",
    "3. CVSS Score: {cvss_score} ({severity})\n",
    "4. ACTUAL FIX PATCH: The changes that fixed the vulnerability\n",
    "5. CODE REVIEW REPORT: What our system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the CVE vulnerability type\n",
    "- 0.7-0.9: Findings flag related security issues that would lead to discovery\n",
    "- 0.4-0.6: Findings flag the general area but miss specific vulnerability\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Special attention:\n",
    "- Did the Security Agent flag this as a security issue?\n",
    "- Is the severity appropriate for the CVE?\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"CVE Relevance Judge\",\n",
    "            instructions=llm_judge_cve_instructions,\n",
    "            model=grok_4_1_fast,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=0.6,\n",
    "                extra_args={\"reasoning\": {\"enabled\": True}}\n",
    "            ),\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to this CVE.\n",
    "\"\"\"\n",
    "        with trace(\"CVE LLM Judge\"):\n",
    "            result = await Runner.run(llm_judge, prompt)\n",
    "            llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Stage 3: Security detection check\n",
    "    security_flagged = check_security_agent_flagged(report)\n",
    "    severity_from_report = extract_max_severity(report)\n",
    "    severity_appropriate = abs(severity_from_report - cvss_score) <= 3 if severity_from_report > 0 else False\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score,\n",
    "        'severity_appropriate': severity_appropriate,\n",
    "        'security_finding_present': security_flagged\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "npvow3atsm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CVE testing framework loaded\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: CVE Testing Framework\n",
    "\n",
    "async def test_cve_benchmark(cve_dataset: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test code review system on CVE dataset.\n",
    "    Reuses existing hybrid evaluation with CVE enhancements.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cve in cve_dataset:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {cve['cve_id']} - {cve['cwe_name']}\")\n",
    "        print(f\"CVSS: {cve['cvss_score']} | Project: {cve['project']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load patch\n",
    "            patch = load_cve_patch(cve['cve_id'])\n",
    "            \n",
    "            # Reverse diff (show vulnerability introduction)\n",
    "            reversed_diff = reverse_diff(patch)\n",
    "            \n",
    "            # Run code review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation with CVE context\n",
    "            eval_result = await evaluate_hybrid_cve(\n",
    "                report, patch,\n",
    "                cve['cve_id'], cve['cwe_id'], cve['cwe_name'],\n",
    "                cve['cvss_score'], cve['severity']\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'cwe_id': cve['cwe_id'],\n",
    "                'cwe_name': cve['cwe_name'],\n",
    "                'cvss_score': cve['cvss_score'],\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'security_flagged': eval_result['security_finding_present'],\n",
    "                'severity_appropriate': eval_result['severity_appropriate'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìç Location: FileRec={result['file_recall']:.0%}, LineRec={result['line_recall']:.0%}\")\n",
    "            print(f\"ü§ñ LLM Relevance: {result['llm_relevance']:.0%}\")\n",
    "            print(f\"üõ°Ô∏è  Security Agent: {'‚úì FLAGGED' if result['security_flagged'] else '‚úó MISSED'}\")\n",
    "            print(f\"üìä Composite: {result['composite_score']:.0%} - {'‚úì PASSED' if result['passed'] else '‚úó FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'cve_id': cve['cve_id'],\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"CVE BENCHMARK SUMMARY\")\n",
    "    print('='*60)\n",
    "    \n",
    "    valid_results = [r for r in results if 'error' not in r]\n",
    "    passed = sum(r['passed'] for r in valid_results)\n",
    "    security_detected = sum(r['security_flagged'] for r in valid_results)\n",
    "    \n",
    "    print(f\"Overall Pass Rate: {passed}/{len(valid_results)} ({passed/len(valid_results):.0%})\")\n",
    "    print(f\"Security Agent Detection: {security_detected}/{len(valid_results)} ({security_detected/len(valid_results):.0%})\")\n",
    "    \n",
    "    # By CWE type\n",
    "    print(f\"\\nüìã Results by CWE Type:\")\n",
    "    cwe_results = {}\n",
    "    for r in valid_results:\n",
    "        cwe = r['cwe_name']\n",
    "        if cwe not in cwe_results:\n",
    "            cwe_results[cwe] = {'total': 0, 'passed': 0}\n",
    "        cwe_results[cwe]['total'] += 1\n",
    "        cwe_results[cwe]['passed'] += r['passed']\n",
    "    \n",
    "    for cwe, stats in sorted(cwe_results.items()):\n",
    "        print(f\"  {cwe}: {stats['passed']}/{stats['total']} passed\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"user-data\", exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"user-data/cve_benchmark_{timestamp}.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nüíæ Results saved to {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì CVE testing framework loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "k3ikvcg60kl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 6: Run CVE Benchmark\n",
    "\n",
    "# # Load CVE dataset\n",
    "# cve_dataset = load_cve_dataset()\n",
    "# print(f\"Loaded {len(cve_dataset)} CVEs\\n\")\n",
    "\n",
    "# # Run benchmark on all CVEs\n",
    "# results = await test_cve_benchmark(cve_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "g4sphvor2cv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 2 problematic CVEs with fixed schema (max 20 lines per finding)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING: CVE-2024-53908 - SQL Injection\n",
      "CVSS: 7.5 | Project: Django\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "This patch fixes a critical SQL injection vulnerability (CVE-2024-53908) in Django's JSONField HasKeyLookup on Oracle by restructuring SQL generation to safely embed JSON paths, but merges overlapping reports into one high-severity Security issue. Best practices issues in the new `as_sql` method include missing docstrings, exception handling, and comments, potentially affecting maintainability. Test coverage gaps are prominent, including untested scenarios for the new methods and a removed test in `test_jsonfield.py` that risks regressions in HasKey lookup coverage; cross-file implications exist as the test removal may leave the json.py fixes unverified.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Potential SQL Injection in Oracle JSON Path Handling (CVE-2024-53908) | django/db/models/fields/json.py | 193-245 | 9 | Security | Ensure JSON path components are strictly serialized/validated; use safe custom delimiters for Oracle (q'[... ]'); refactor as_sql to separate SQL/params; add tests for malicious inputs. | Code Analyzer, Security |\n",
      "| Missing Docstring on Public Method as_sql | django/db/models/fields/json.py | 194-212 | 4 | Style | Add docstring describing purpose, parameters (compiler, connection, template), and return values. | Best Practices |\n",
      "| Bare Except Clause Not Present but Ensure Exception Handling on Database Calls | django/db/models/fields/json.py | 194,215,229,234,240 | 3 | Style | Add specific try-except blocks for expected exceptions from compiler/connection methods. | Best Practices |\n",
      "| Redundant Complex Logic and Lack of Comments in as_sql Method | django/db/models/fields/json.py | 194-245 | 3 | Style | Add detailed comments on SQL construction, custom Oracle delimiters, and lhs/rhs processing flow. | Best Practices |\n",
      "| Test Gap: as_sql missing scenarios | django/db/models/fields/json.py | 193-212 | 9 | Test Gap | Unit tests with mocked compiler/connection: normal KeyTransform/single rhs, non-KeyTransform/multiple rhs, empty rhs, error handling, SQL injection prevention on Oracle. | Test Coverage |\n",
      "| Test Gap: as_oracle missing scenarios | django/db/models/fields/json.py | 236-255 | 10 | Test Gap | Unit tests for Oracle SQL: injection prevention via direct embedding, malicious lhs/rhs, empty/complex paths, empty params validation. | Test Coverage |\n",
      "| Removed test_has_key_literal_lookup reduces coverage | tests/model_fields/test_jsonfield.py | 580-595 | 6 | Test Gap | Add/update tests for HasKey with Value JSON lhs to cover new as_sql changes and prevent regressions. | Code Analyzer |\n",
      "\n",
      "**Total Distinct Issues: 7**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç Location: FileRec=40%, LineRec=55%\n",
      "ü§ñ LLM Relevance: 100%\n",
      "üõ°Ô∏è  Security Agent: ‚úì FLAGGED\n",
      "üìä Composite: 78% - ‚úì PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: CVE-2024-23346 - Command Injection\n",
      "CVSS: 7.8 | Project: Pymatgen\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a high-severity security vulnerability in `pymatgen/symmetry/settings.py` where `eval()` is used insecurely on transformation string-derived expressions (lines 108-116), risking code injection despite restricted builtins. A related best practice violation involves removing safer sympy parsing (lines 100-126), degrading clarity and maintainability. Comprehensive test coverage is missing for `parse_transformation_string` (lines 99-118), including error handling for invalid and malicious inputs.\n",
      "\n",
      "## Summary of Actions\n",
      "\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Insecure use of eval() with limited builtins/context on basis_change expressions | pymatgen/symmetry/settings.py | 108-116 | 8 | Security | Replace eval with safer parser like sympy's parse_expr with strict allowed symbols={'a','b','c'}; implement rigorous input validation/sanitization or custom math expression parser | Code Analyzer, Security, Best Practices |\n",
      "| Removed sympy parsing without safer replacement, reducing clarity and robustness | pymatgen/symmetry/settings.py | 100-126 | 6 | Style | Retain sympy parsing or equivalent robust expression parser; add documentation/comments for parsing steps and input sanitation | Best Practices |\n",
      "| Missing tests for parse_transformation_string (normal, edge, error cases) | pymatgen/symmetry/settings.py | 99-118 | 8 | Test Gap | Add unit tests for valid/malformed inputs, error handling (invalid formats, malicious expr, eval failures), integration checks on output shapes; mock eval if needed | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìç Location: FileRec=100%, LineRec=78%\n",
      "ü§ñ LLM Relevance: 100%\n",
      "üõ°Ô∏è  Security Agent: ‚úì FLAGGED\n",
      "üìä Composite: 89% - ‚úì PASSED\n",
      "\n",
      "\n",
      "============================================================\n",
      "CVE BENCHMARK SUMMARY\n",
      "============================================================\n",
      "Overall Pass Rate: 2/2 (100%)\n",
      "Security Agent Detection: 2/2 (100%)\n",
      "\n",
      "üìã Results by CWE Type:\n",
      "  Command Injection: 1/1 passed\n",
      "  SQL Injection: 1/1 passed\n",
      "\n",
      "üíæ Results saved to user-data/cve_benchmark_20251121_152601.json\n"
     ]
    }
   ],
   "source": [
    "# Test the two problematic CVEs that got stuck\n",
    "\n",
    "problematic_cves = [\"CVE-2024-53908\", \"CVE-2024-23346\"]  # SQL Injection and Code Injection\n",
    "problematic_dataset = [cve for cve in cve_dataset if cve['cve_id'] in problematic_cves]\n",
    "\n",
    "print(f\"Testing {len(problematic_dataset)} problematic CVEs with fixed schema (max 20 lines per finding)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run quick test on just these two\n",
    "results_test = await test_cve_benchmark(problematic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bmuyx9qdx57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CVE BENCHMARK COMPARISON\n",
      "============================================================\n",
      "\n",
      "Configuration changes:\n",
      "  - Security KB: 13 patterns ‚Üí 43 patterns (OWASP Top 10 2021)\n",
      "  - RAG retrieval: n_results=5 ‚Üí n_results=15\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULTS COMPARISON:\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä No RAG (Baseline):\n",
      "  Pass Rate: 16/17 (94%)\n",
      "  Security Detection: 16/17 (94%)\n",
      "\n",
      "üîç RAG with 13 patterns (n_results=5):\n",
      "  Pass Rate: 15/17 (88%)\n",
      "  Security Detection: 16/17 (94%)\n",
      "\n",
      "üéØ RAG with 43 patterns (n_results=15):\n",
      "  Run cell above to see results...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CVE Benchmark Comparison Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CVE BENCHMARK COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfiguration changes:\")\n",
    "print(\"  - Security KB: 13 patterns ‚Üí 43 patterns (OWASP Top 10 2021)\")\n",
    "print(\"  - RAG retrieval: n_results=5 ‚Üí n_results=15\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RESULTS COMPARISON:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\\nüìä No RAG (Baseline):\")\n",
    "print(\"  Pass Rate: 16/17 (94%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\nüîç RAG with 13 patterns (n_results=5):\")\n",
    "print(\"  Pass Rate: 15/17 (88%)\")\n",
    "print(\"  Security Detection: 16/17 (94%)\")\n",
    "print(\"\\nüéØ RAG with 43 patterns (n_results=15):\")\n",
    "print(\"  Run cell above to see results...\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6552020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
