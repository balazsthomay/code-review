{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_fast=LitellmModel(model=\"openrouter/x-ai/grok-4-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff. \n",
    "Identify bugs and anti-patterns including: logic errors, unhandled edge cases, null/undefined access, type mismatches, off-by-one errors, resource leaks (unclosed files/cursors/connections), infinite loops, missing error handling (no try-except blocks), code duplication, and overly complex functions. \n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff. \n",
    "Identify security vulnerabilities including: SQL injection, command injection, XSS vulnerabilities, hardcoded secrets/credentials, insecure authentication, path traversal, insecure deserialization, improper input validation, and missing error handling that could expose sensitive information.\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff. \n",
    "Identify code quality issues including: unclear variable names, functions exceeding 50 lines, nested complexity over 3 levels, missing docstrings, inconsistent formatting, magic numbers without explanation, violations of DRY principle, unclosed resources (files, database cursors, connections), and missing try-except blocks for error-prone operations.\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff. \n",
    "For each new or modified function, suggest test cases covering: normal input cases, edge cases (empty, null, boundary values), error conditions (exceptions, failures, timeouts), and integration scenarios.\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc44cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_security_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    security_collection = chroma_client.get_collection(name=\"security_patterns\")\n",
    "    results = security_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\"\n",
    "\n",
    "def get_relevant_best_practices_patterns(code_diff: str, n_results: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant best practices patterns from ChromaDB\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    best_practices_collection = chroma_client.get_collection(name=\"best_practices_patterns\")\n",
    "    results = best_practices_collection.query(query_texts=[code_diff], n_results=n_results)\n",
    "    return \"\\n\\n\".join(results['documents'][0]) if results['documents'][0] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_all_agents(diff):\n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(code_analyzer, diff),\n",
    "#         Runner.run(security_agent, diff),\n",
    "#         Runner.run(best_practices_agent, diff),\n",
    "#         Runner.run(test_coverage_agent, diff)\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "async def run_all_agents(diff):\n",
    "    # Get RAG context for both security and best practices agents\n",
    "    security_patterns = get_relevant_security_patterns(diff, n_results=5)\n",
    "    best_practices_patterns = get_relevant_best_practices_patterns(diff, n_results=5)\n",
    "    \n",
    "    # Create RAG-enhanced security agent\n",
    "    enhanced_security_instructions = f\"\"\"{security_instructions}\n",
    "\n",
    "RELEVANT SECURITY PATTERNS TO CHECK:\n",
    "{security_patterns}\"\"\"\n",
    "    \n",
    "    # Create RAG-enhanced best practices agent\n",
    "    enhanced_best_practices_instructions = f\"\"\"{best_practices_instructions}\n",
    "\n",
    "RELEVANT BEST PRACTICES PATTERNS TO CHECK:\n",
    "{best_practices_patterns}\"\"\"\n",
    "    \n",
    "    security_agent_rag = Agent(\n",
    "        name=\"Security Agent\",\n",
    "        instructions=enhanced_security_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        output_type=SecurityOutput\n",
    "    )\n",
    "    \n",
    "    best_practices_agent_rag = Agent(\n",
    "        name=\"Best Practices Agent\",\n",
    "        instructions=enhanced_best_practices_instructions,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        output_type=BestPracticesOutput\n",
    "    )\n",
    "    \n",
    "    # Run all agents in parallel\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent_rag, diff),  # Uses RAG\n",
    "        Runner.run(best_practices_agent_rag, diff),  # Now uses RAG too!\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "When creating the report, follow these guidelines:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "3. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "4. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "5. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_fast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    results = await run_all_agents(diff)\n",
    "    code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "    \n",
    "    organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALLING AGGREGATOR...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = await aggregator_agent(organized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATOR OUTPUT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if save_output:\n",
    "        os.makedirs(\"user-data\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8v51k78wlmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation (Option 4): Models and Utilities\n",
    "import re\n",
    "\n",
    "class HybridEvaluation(BaseModel):\n",
    "    \"\"\"Hybrid evaluation combining location metrics + LLM relevance.\"\"\"\n",
    "    file_recall: float = Field(description=\"Recall at file level (0.0-1.0)\")\n",
    "    line_precision: float = Field(description=\"Precision at line level (0.0-1.0)\")\n",
    "    line_recall: float = Field(description=\"Recall at line level (0.0-1.0)\")\n",
    "    llm_relevance: float = Field(description=\"LLM-judged relevance score (0.0-1.0)\")\n",
    "    composite_score: float = Field(description=\"Combined score: (line_recall + llm_relevance) / 2\")\n",
    "\n",
    "def reverse_diff(bug_patch: str) -> str:\n",
    "    \"\"\"Reverses a bug patch to show bug introduction instead of fix.\"\"\"\n",
    "    lines = bug_patch.split('\\n')\n",
    "    reversed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            reversed_lines.append(line)\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            reversed_lines.append('+' + line[1:])\n",
    "        elif line.startswith('+') and not line.startswith('+++'):\n",
    "            reversed_lines.append('-' + line[1:])\n",
    "        else:\n",
    "            reversed_lines.append(line)\n",
    "    return '\\n'.join(reversed_lines)\n",
    "\n",
    "def parse_changed_locations(bug_patch: str) -> dict:\n",
    "    \"\"\"Extract files and lines changed in the patch.\"\"\"\n",
    "    changed_files = set()\n",
    "    changed_lines = {}\n",
    "    \n",
    "    current_file = None\n",
    "    for line in bug_patch.split('\\n'):\n",
    "        # Extract filename from +++ line\n",
    "        if line.startswith('+++'):\n",
    "            match = re.search(r'\\+\\+\\+ b/(.+)', line)\n",
    "            if match:\n",
    "                current_file = match.group(1)\n",
    "                changed_files.add(current_file)\n",
    "                changed_lines[current_file] = set()\n",
    "        \n",
    "        # Extract line numbers from @@ hunk headers\n",
    "        elif line.startswith('@@') and current_file:\n",
    "            match = re.search(r'@@ -\\d+,?\\d* \\+(\\d+),?(\\d*)', line)\n",
    "            if match:\n",
    "                start = int(match.group(1))\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                changed_lines[current_file].update(range(start, start + count))\n",
    "    \n",
    "    return {'files': changed_files, 'lines': changed_lines}\n",
    "\n",
    "def parse_flagged_locations(report: str) -> dict:\n",
    "    \"\"\"Extract files and lines flagged in the report.\"\"\"\n",
    "    flagged_files = set()\n",
    "    flagged_lines = {}\n",
    "    \n",
    "    # Parse markdown table from report\n",
    "    in_table = False\n",
    "    for line in report.split('\\n'):\n",
    "        if '| Issue | File | Lines |' in line:\n",
    "            in_table = True\n",
    "            continue\n",
    "        if in_table and line.strip().startswith('|') and not line.strip().startswith('|---'):\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) > 3:\n",
    "                file_path = parts[2]\n",
    "                lines_str = parts[3]\n",
    "                \n",
    "                if file_path and file_path != 'File':\n",
    "                    flagged_files.add(file_path)\n",
    "                    if file_path not in flagged_lines:\n",
    "                        flagged_lines[file_path] = set()\n",
    "                    \n",
    "                    # Parse line numbers (e.g., \"7-10\", \"24-25\", \"9\")\n",
    "                    for line_range in lines_str.split(','):\n",
    "                        line_range = line_range.strip()\n",
    "                        if '-' in line_range:\n",
    "                            start, end = map(int, line_range.split('-'))\n",
    "                            flagged_lines[file_path].update(range(start, end + 1))\n",
    "                        elif line_range.isdigit():\n",
    "                            flagged_lines[file_path].add(int(line_range))\n",
    "    \n",
    "    return {'files': flagged_files, 'lines': flagged_lines}\n",
    "\n",
    "def calculate_location_metrics(actual: dict, flagged: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate location-based overlap metrics.\n",
    "    \n",
    "    Recall: Of all actual changed lines, how many did we flag (within 5 line tolerance)?\n",
    "    Precision: Of all flagged lines, how many correspond to actual changes (within 5 line tolerance)?\n",
    "    \"\"\"\n",
    "    # File-level recall\n",
    "    file_recall = len(flagged['files'] & actual['files']) / len(actual['files']) if actual['files'] else 0.0\n",
    "    \n",
    "    # Line-level metrics\n",
    "    total_actual_lines = 0\n",
    "    total_flagged_lines = 0\n",
    "    actual_lines_matched = 0  # For recall: how many actual lines have a nearby flagged line\n",
    "    flagged_lines_matched = 0  # For precision: how many flagged lines have a nearby actual line\n",
    "    \n",
    "    for file in actual['files']:\n",
    "        actual_lines = actual['lines'].get(file, set())\n",
    "        flagged_lines_in_file = flagged['lines'].get(file, set())\n",
    "        \n",
    "        total_actual_lines += len(actual_lines)\n",
    "        total_flagged_lines += len(flagged_lines_in_file)\n",
    "        \n",
    "        # Count actual lines that have at least one flagged line within 5 lines (for recall)\n",
    "        for actual_line in actual_lines:\n",
    "            if any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                actual_lines_matched += 1\n",
    "        \n",
    "        # Count flagged lines that have at least one actual line within 5 lines (for precision)\n",
    "        for flagged_line in flagged_lines_in_file:\n",
    "            if any(abs(flagged_line - actual_line) <= 5 for actual_line in actual_lines):\n",
    "                flagged_lines_matched += 1\n",
    "    \n",
    "    line_recall = actual_lines_matched / total_actual_lines if total_actual_lines > 0 else 0.0\n",
    "    line_precision = flagged_lines_matched / total_flagged_lines if total_flagged_lines > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'file_recall': file_recall,\n",
    "        'line_recall': line_recall,\n",
    "        'line_precision': line_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ri2gmqdlr5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Evaluation Function\n",
    "\n",
    "class LLMRelevance(BaseModel):\n",
    "    \"\"\"LLM's assessment of how relevant the review findings are to the actual fix.\"\"\"\n",
    "    relevance_score: float = Field(description=\"0.0-1.0: How well the review findings align with the actual fix\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "async def evaluate_hybrid(report: str, bug_patch: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid evaluation: Location metrics (automated) + LLM relevance (semantic).\n",
    "    \n",
    "    Stage 1: Calculate automated location overlap\n",
    "    Stage 2: If file_recall > 0, use LLM to judge semantic relevance\n",
    "    \n",
    "    Returns:\n",
    "        dict with file_recall, line_precision, line_recall, llm_relevance, composite_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stage 1: Automated location metrics\n",
    "    actual_locations = parse_changed_locations(bug_patch)\n",
    "    flagged_locations = parse_flagged_locations(report)\n",
    "    location_metrics = calculate_location_metrics(actual_locations, flagged_locations)\n",
    "    \n",
    "    # Stage 2: LLM relevance (only if there's file overlap)\n",
    "    llm_relevance = 0.0\n",
    "    if location_metrics['file_recall'] > 0:\n",
    "        llm_judge_instructions = \"\"\"You are evaluating the semantic relevance of code review findings to an actual bug fix.\n",
    "\n",
    "Given:\n",
    "1. ACTUAL FIX PATCH: The changes that were made to fix bugs\n",
    "2. CODE REVIEW REPORT: What the review system found\n",
    "\n",
    "Rate the relevance (0.0 to 1.0) of the review findings:\n",
    "- 1.0: Findings directly identify the bugs that were fixed\n",
    "- 0.7-0.9: Findings flag related issues that would lead to discovering the bugs\n",
    "- 0.4-0.6: Findings flag the general area but miss specific bugs\n",
    "- 0.1-0.3: Findings are tangentially related\n",
    "- 0.0: No relevant findings\n",
    "\n",
    "Be objective and strict in your assessment.\"\"\"\n",
    "\n",
    "        llm_judge = Agent(\n",
    "            name=\"Relevance Judge\",\n",
    "            instructions=llm_judge_instructions,\n",
    "            model=\"gpt-5.1\",\n",
    "            output_type=LLMRelevance\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ACTUAL FIX PATCH:\n",
    "{bug_patch}\n",
    "\n",
    "CODE REVIEW REPORT:\n",
    "{report}\n",
    "\n",
    "Rate the semantic relevance of the review findings to the actual fix.\n",
    "\"\"\"\n",
    "        \n",
    "        result = await Runner.run(llm_judge, prompt)\n",
    "        llm_relevance = result.final_output.relevance_score\n",
    "    \n",
    "    # Composite score: average of line recall and LLM relevance\n",
    "    composite_score = (location_metrics['line_recall'] + llm_relevance) / 2\n",
    "    \n",
    "    return {\n",
    "        'file_recall': location_metrics['file_recall'],\n",
    "        'line_precision': location_metrics['line_precision'],\n",
    "        'line_recall': location_metrics['line_recall'],\n",
    "        'llm_relevance': llm_relevance,\n",
    "        'composite_score': composite_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e63ligp8p4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Batch Testing\n",
    "\n",
    "async def test_bugsinpy_hybrid(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with hybrid evaluation.\n",
    "    \n",
    "    Args:\n",
    "        bugs_to_test: List of (project, bug_id) tuples\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation results with hybrid metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            # Reverse diff to show bug introduction\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run code review on buggy code\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60  # 60% threshold for hybrid\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nðŸ“ LOCATION METRICS (Automated):\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.2%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.2%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.2%}\")\n",
    "            print(f\"\\nðŸ¤– LLM RELEVANCE (Semantic):\")\n",
    "            print(f\"  Relevance Score: {eval_result['llm_relevance']:.2%}\")\n",
    "            print(f\"\\nðŸŽ¯ COMPOSITE SCORE:\")\n",
    "            print(f\"  Score: {eval_result['composite_score']:.2%}\")\n",
    "            print(f\"  Status: {'âœ“ PASSED' if result['passed'] else 'âœ— FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"âœ— {result['project']}/{result['bug_id']}: ERROR - {result['error']}\")\n",
    "        else:\n",
    "            status = 'âœ“' if result['passed'] else 'âœ—'\n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"FileRec={result['file_recall']:.0%}, \"\n",
    "                  f\"LineRec={result['line_recall']:.0%}, \"\n",
    "                  f\"LLM={result['llm_relevance']:.0%}, \"\n",
    "                  f\"Composite={result['composite_score']:.0%}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    print(f\"Success Rate: {passed/len(results):.0%}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "wu6y1lr2hy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 4 bugs (commented out - see 10-bug test below)\n",
    "# bugs_to_test = [\n",
    "#     (\"luigi\", 2),\n",
    "#     (\"black\", 4),\n",
    "#     (\"keras\", 1),\n",
    "#     (\"pandas\", 1),\n",
    "# ]\n",
    "# \n",
    "# results = await test_bugsinpy_hybrid(bugs_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bgahnfexbho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test with 10 bugs - showing what agents missed\n",
    "\n",
    "async def test_bugsinpy_with_miss_analysis(bugs_to_test: list[tuple[str, int]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Test multiple BugsInPy bugs with detailed miss analysis.\n",
    "    Shows what the agents caught vs. what they missed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for project, bug_id in bugs_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {project} bug {bug_id}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Load bug patch\n",
    "            bug_patch_path = Path(f\"BugsInPy/projects/{project}/bugs/{bug_id}/bug_patch.txt\")\n",
    "            bug_patch = bug_patch_path.read_text()\n",
    "            \n",
    "            print(\"\\nACTUAL FIX (first 500 chars):\")\n",
    "            print(bug_patch[:500])\n",
    "            print(\"...\" if len(bug_patch) > 500 else \"\")\n",
    "            \n",
    "            # Reverse diff\n",
    "            reversed_diff = reverse_diff(bug_patch)\n",
    "            \n",
    "            # Run review\n",
    "            report = await review_code(reversed_diff, save_output=False)\n",
    "            \n",
    "            # Hybrid evaluation\n",
    "            eval_result = await evaluate_hybrid(report, bug_patch)\n",
    "            \n",
    "            # Parse locations to show what was missed\n",
    "            actual_locations = parse_changed_locations(bug_patch)\n",
    "            flagged_locations = parse_flagged_locations(report)\n",
    "            \n",
    "            # Find missed files\n",
    "            missed_files = actual_locations['files'] - flagged_locations['files']\n",
    "            \n",
    "            # Find missed line ranges\n",
    "            missed_lines = {}\n",
    "            for file in actual_locations['files']:\n",
    "                actual_lines = actual_locations['lines'].get(file, set())\n",
    "                flagged_lines_in_file = flagged_locations['lines'].get(file, set())\n",
    "                \n",
    "                # Lines that weren't caught (no flagged line within 5 lines)\n",
    "                uncaught = []\n",
    "                for actual_line in actual_lines:\n",
    "                    if not any(abs(actual_line - flagged_line) <= 5 for flagged_line in flagged_lines_in_file):\n",
    "                        uncaught.append(actual_line)\n",
    "                \n",
    "                if uncaught:\n",
    "                    missed_lines[file] = sorted(uncaught)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'file_recall': eval_result['file_recall'],\n",
    "                'line_precision': eval_result['line_precision'],\n",
    "                'line_recall': eval_result['line_recall'],\n",
    "                'llm_relevance': eval_result['llm_relevance'],\n",
    "                'composite_score': eval_result['composite_score'],\n",
    "                'passed': eval_result['composite_score'] >= 0.60,\n",
    "                'missed_files': list(missed_files),\n",
    "                'missed_lines': missed_lines\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nðŸ“ LOCATION METRICS:\")\n",
    "            print(f\"  File Recall: {eval_result['file_recall']:.0%}\")\n",
    "            print(f\"  Line Precision: {eval_result['line_precision']:.0%}\")\n",
    "            print(f\"  Line Recall: {eval_result['line_recall']:.0%}\")\n",
    "            print(f\"\\nðŸ¤– LLM RELEVANCE: {eval_result['llm_relevance']:.0%}\")\n",
    "            print(f\"ðŸŽ¯ COMPOSITE: {eval_result['composite_score']:.0%}\")\n",
    "            \n",
    "            # Show what was missed\n",
    "            if missed_files:\n",
    "                print(f\"\\nâŒ MISSED FILES: {', '.join(missed_files)}\")\n",
    "            \n",
    "            if missed_lines:\n",
    "                print(f\"\\nâŒ MISSED LINES:\")\n",
    "                for file, lines in missed_lines.items():\n",
    "                    line_ranges = []\n",
    "                    start = lines[0]\n",
    "                    end = start\n",
    "                    for i in range(1, len(lines)):\n",
    "                        if lines[i] == end + 1:\n",
    "                            end = lines[i]\n",
    "                        else:\n",
    "                            line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                            start = lines[i]\n",
    "                            end = start\n",
    "                    line_ranges.append(f\"{start}-{end}\" if start != end else str(start))\n",
    "                    print(f\"  {file}: lines {', '.join(line_ranges)}\")\n",
    "            \n",
    "            print(f\"\\n{'âœ“ PASSED' if result['passed'] else 'âœ— FAILED'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'project': project,\n",
    "                'bug_id': bug_id,\n",
    "                'error': str(e),\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            print(f\"âœ— {result['project']}/{result['bug_id']}: ERROR\")\n",
    "        else:\n",
    "            status = 'âœ“' if result['passed'] else 'âœ—'\n",
    "            missed_info = \"\"\n",
    "            if result['missed_files']:\n",
    "                missed_info += f\" | Missed files: {len(result['missed_files'])}\"\n",
    "            if result['missed_lines']:\n",
    "                total_missed = sum(len(lines) for lines in result['missed_lines'].values())\n",
    "                missed_info += f\" | Missed lines: {total_missed}\"\n",
    "            \n",
    "            print(f\"{status} {result['project']}/{result['bug_id']}: \"\n",
    "                  f\"Composite={result['composite_score']:.0%} \"\n",
    "                  f\"(LineRec={result['line_recall']:.0%}, LLM={result['llm_relevance']:.0%})\"\n",
    "                  f\"{missed_info}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r.get('passed', False))\n",
    "    print(f\"\\nPassed: {passed}/{len(results)} ({passed/len(results):.0%})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "il199vb2y1h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING: luigi bug 2\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/luigi/contrib/beam_dataflow.py b/luigi/contrib/beam_dataflow.py\n",
      "index dd510786..42cdc742 100644\n",
      "--- a/luigi/contrib/beam_dataflow.py\n",
      "+++ b/luigi/contrib/beam_dataflow.py\n",
      "@@ -219,6 +219,7 @@ class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n",
      "     def __init__(self):\n",
      "         if not isinstance(self.dataflow_params, DataflowParamKeys):\n",
      "             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n",
      "+        super(BeamDataflowJobTask, self).__init__()\n",
      " \n",
      "  \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code in luigi/contrib/beam_dataflow.py has critical bugs related to incomplete class initialization and incorrect handling in the get_target_path method, which could lead to runtime errors or unexpected behavior. Documentation and style issues, such as missing docstrings and inconsistent formatting, reduce code maintainability. Additionally, significant test coverage gaps exist for key methods, requiring unit tests to validate normal and edge cases.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Missing call to superclass __init__ in BeamDataflowJobTask | luigi/contrib/beam_dataflow.py | 220,221,222 | 7 | Bug | Uncomment or add the call to super(BeamDataflowJobTask, self).__init__() in the __init__ method after validating dataflow_params to ensure proper initialization of the base class luigi.Task and any mixins, preventing incomplete setup or validation. | Code Analyzer, Security, Best Practices |\n",
      "| Incorrect handling of bigquery.BigQueryTarget in get_target_path | luigi/contrib/beam_dataflow.py | 474,475,477,478,479,480 | 8 | Bug | Add a return statement for the BigQueryTarget case, using consistent attribute access (verify if attributes are on target or target.table, e.g., return \"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id) or equivalent for target.table); replace the unused string expression with this return to avoid returning None. | Code Analyzer, Security, Best Practices |\n",
      "| Inconsistent target attribute usage in bigquery.BigQueryTarget formatting | luigi/contrib/beam_dataflow.py | 474,479 | 6 | Bug | Verify and standardize attribute access for BigQueryTarget in get_target_path (e.g., use target.project_id if direct or target.table.project_id if nested) to prevent AttributeError; ensure consistency with original code patterns. | Code Analyzer |\n",
      "| Lost docstring in get_target_path method | luigi/contrib/beam_dataflow.py | 471,472,473,474,477 | 3 | Style | Restore the original docstring or add a complete one explaining the method's purpose, parameters (target: luigi.Target), return value (str path), and exceptions (ValueError for unsupported targets) to improve documentation and clarity. | Code Analyzer, Best Practices |\n",
      "| Improper exception message formatting in get_target_path for unsupported targets | luigi/contrib/beam_dataflow.py | 481,482 | 2 | Style | Use consistent modern string formatting for the ValueError in get_target_path, e.g., raise ValueError(f\"Target {target} not supported\") instead of plain strings or old-style formatting, to provide more informative error messages. | Code Analyzer, Best Practices |\n",
      "| Missing tests for __init__ method | luigi/contrib/beam_dataflow.py | 220,221 | 7 | Test Gap | Implement unit tests covering normal initialization with valid DataflowParamKeys instance and initialization with invalid dataflow_params type raising ValueError. | Test Coverage |\n",
      "| Missing tests for get_target_path method | luigi/contrib/beam_dataflow.py | 473,483 | 8 | Test Gap | Implement unit tests for inputs including luigi.LocalTarget (returns local path), gcs.GCSTarget (returns GCS path), bigquery.BigQueryTarget (returns formatted string), unsupported target (raises ValueError), None input (raises ValueError or handled), and boundary cases with minimal attributes. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 7**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: black bug 4\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/black.py b/black.py\n",
      "index f7022d8..05edf1a 100644\n",
      "--- a/black.py\n",
      "+++ b/black.py\n",
      "@@ -1480,7 +1480,13 @@ class EmptyLineTracker:\n",
      "         lines (two on module-level).\n",
      "         \"\"\"\n",
      "         before, after = self._maybe_empty_lines(current_line)\n",
      "-        before -= self.previous_after\n",
      "+        before = (\n",
      "+            # Black should not insert empty lines at the beginning\n",
      "+            # of the file\n",
      "+            0\n",
      "+            if self.previous_line is None\n",
      "+            else before - self.pr\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary critical finding is a bug in the `_maybe_empty_lines` function of `black.py` that incorrectly handles empty line calculations at the file start by removing a conditional check for `previous_line` being None, potentially leading to negative values and miscounting; this is compounded by reduced code clarity and lost comments from the original implementation. Additionally, there is a high-priority test coverage gap for the same function, missing scenarios for edge cases like file beginnings and invalid inputs, which could allow undetected issues in formatting behavior. No security or performance issues were identified.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect handling of empty line calculation at file start | black.py | 1481,1483,1484,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497 | 6 | Bug | Restore the check for the beginning of the file by using the original conditional: if self.previous_line is None: before = 0 else: before -= self.previous_after. Preserve or improve explanatory comments for clarity and PEP 8 compliance, avoiding redundant reassignments. | Code Analyzer, Best Practices |\n",
      "| Missing test coverage for _maybe_empty_lines | black.py | 1480,1488 | 7 | Test Gap | Implement unit tests covering normal operations with various empty lines, edge cases (first line, previous_line None, previous_after zero/non-zero), error handling (None/invalid inputs), consecutive empties, and integration with formatting flow. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 50%\n",
      "ðŸŽ¯ COMPOSITE: 75%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: pandas bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/pandas/core/dtypes/common.py b/pandas/core/dtypes/common.py\n",
      "index 5b20b8e1b..a4a5ae1bf 100644\n",
      "--- a/pandas/core/dtypes/common.py\n",
      "+++ b/pandas/core/dtypes/common.py\n",
      "@@ -599,7 +599,7 @@ def is_string_dtype(arr_or_dtype) -> bool:\n",
      "         \"\"\"\n",
      "         These have kind = \"O\" but aren't string dtypes so need to be explicitly excluded\n",
      "         \"\"\"\n",
      "-        is_excluded_checks = (is_period_dtype, is_interval_dtype)\n",
      "+        is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categor\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue identified is an incorrect modification to the `is_string_dtype` function in `pandas/core/dtypes/common.py`, where `is_categorical_dtype` was removed from the exclusion checks, potentially leading to logic errors by misclassifying categorical dtypes as strings; this change lacks explanation and could introduce downstream bugs. Additionally, there is a significant test coverage gap for this function, missing scenarios for normal inputs, edge cases, error handling, and integrations. No security or performance issues were found, but addressing the bug and test gaps is critical to maintain dtype identification accuracy.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect Exclusion Checks in is_string_dtype Function | pandas/core/dtypes/common.py | [601, 602, 603] | 7 | Bug | Restore 'is_categorical_dtype' to the 'is_excluded_checks' tuple to match original behavior (is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)); add comments explaining any intentional changes if applicable, update docstrings, and include validation tests to ensure no downstream breakage from dtype misidentification. | Code Analyzer, Best Practices |\n",
      "| Missing Test Coverage for is_string_dtype Function | pandas/core/dtypes/common.py | [599, 600, 601, 602, 603, 604, 605, 606] | 7 | Test Gap | Implement unit tests covering: normal inputs (string vs. non-string dtype arrays), edge cases (None/empty arrays, Period/Interval/Categorical dtypes for exclusion), error handling (invalid/unsupported types), and integration with related functions (is_period_dtype, is_interval_dtype); use pytest for comprehensive scenario validation. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: scrapy bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py\n",
      "index 232e96cb..36f80969 100644\n",
      "--- a/scrapy/spidermiddlewares/offsite.py\n",
      "+++ b/scrapy/spidermiddlewares/offsite.py\n",
      "@@ -54,12 +54,16 @@ class OffsiteMiddleware(object):\n",
      "         if not allowed_domains:\n",
      "             return re.compile('')  # allow all by default\n",
      "         url_pattern = re.compile(\"^https?://.*$\")\n",
      "+        domains = []\n",
      "         for domain in allowed_domains:\n",
      "-            if url_pattern.match(domai\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code in `scrapy/spidermiddlewares/offsite.py` has a critical bug involving improper handling of URL entries and indentation issues in the `allowed_domains` processing logic, leading to unfiltered domains and potential regex mismatches. Additionally, the method lacks a docstring, reducing maintainability, and the `_domains_regex` function has significant test coverage gaps across multiple scenarios like empty inputs, URLs, and special characters. Addressing these will improve reliability, clarity, and robustness.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Improper handling of URL entries in allowed_domains | scrapy/spidermiddlewares/offsite.py | 55,58,59,62,63,66,67,68,69,70,71,72,73,74,75 | 6 | Bug | Filter out entries matching the URL pattern from allowed_domains before escaping and including them in the domains list. For example, change domains assignment to: domains = [re.escape(d) for d in allowed_domains if d is not None and not url_pattern.match(d)] to exclude URLs, consistent with the warning logic. Fix the indentation of the for loop block to properly check each domain for being a URL and emit warnings appropriately, then build the 'domains' list by escaping domains that are not URLs or None. Ensure the code path for appending escaped domains is reachable and correctly placed. | Code Analyzer, Best Practices |\n",
      "| Missing docstring for class method | scrapy/spidermiddlewares/offsite.py | 55,56,57,58 | 3 | Style | Add a docstring to explain the method's behavior, input, and output. | Best Practices |\n",
      "| Test coverage gap for _domains_regex | scrapy/spidermiddlewares/offsite.py | 54,69 | 7 | Test Gap | Implement unit tests covering: normal input with valid domain names, allowed_domains list empty or None, allowed_domains containing None entries, allowed_domains containing URLs instead of domains, allowed_domains with special regex characters (verify proper escaping), allowed_domains with mixed None, valid domains, and URLs, handling of non-string domain entries (type robustness). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 3**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 94%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 80%\n",
      "ðŸŽ¯ COMPOSITE: 90%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: thefuck bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/thefuck/rules/pip_unknown_command.py b/thefuck/rules/pip_unknown_command.py\n",
      "index 75fcc7c..2720cda 100644\n",
      "--- a/thefuck/rules/pip_unknown_command.py\n",
      "+++ b/thefuck/rules/pip_unknown_command.py\n",
      "@@ -12,8 +12,8 @@ def match(command):\n",
      " \n",
      " \n",
      " def get_new_command(command):\n",
      "-    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n",
      "+    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n",
      "                             command.output)[0]\n",
      "-    new_cmd = re.findall(r'maybe you m\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code in `thefuck/rules/pip_unknown_command.py` has critical bugs related to unguarded regex operations that can cause IndexError crashes due to assumptions about matching patterns, which should be addressed immediately to prevent runtime failures. Additionally, there's a moderate security risk from potential ReDoS on unsanitized input, and minor best practices issues like inconsistent quoting that impact readability. Test coverage is insufficient for the `get_new_command` function, missing scenarios for normal, edge, and error cases, requiring new unit tests to ensure robustness.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Potential IndexError from unguarded list indexing and missing error handling | thefuck/rules/pip_unknown_command.py | [13, 14, 16, 17] | 7 | Bug | Add checks to ensure re.findall returns a non-empty list before accessing the first element (e.g., assign to a variable and check `if broken_cmd:` before using `[0]`). Alternatively, use try-except blocks around regex operations to handle IndexError gracefully, or switch to re.search and check for None before extracting groups. | Code Analyzer, Best Practices |\n",
      "| Regular Expression Injection via Unsanitized Input | thefuck/rules/pip_unknown_command.py | [14, 16] | 4 | Security | Validate or sanitize command.output before using it in regular expression matching. Use compiled regex with timeouts or safely limit the length and content of command.output to mitigate risks of ReDoS. Consider adding error handling if regex matching fails to prevent exceptions. | Security |\n",
      "| Inconsistent Quotation Usage in Regex Patterns | thefuck/rules/pip_unknown_command.py | [13, 14, 16, 17] | 3 | Style | Use a consistent style for quotes in regex patterns, preferably raw string literals with single quotes outside and unescaped double quotes inside or vice versa for readability. For example, standardize all patterns like `broken_cmd = re.findall(r'ERROR: unknown command \"([a-z]+)\"', command.output)[0]`. | Best Practices |\n",
      "| Test coverage gap for get_new_command function | thefuck/rules/pip_unknown_command.py | [14, 15, 17, 18] | 7 | Test Gap | Add unit tests covering: normal case (output contains unknown command and suggestion with lowercase ASCII commands), edge case (output missing 'unknown command' pattern, handling IndexError), edge case (output missing 'maybe you meant' pattern, handling IndexError), error case (command.output is None or empty string), error case (command.output contains uppercase or non-ASCII characters in patterns), and integration case (verify replace_argument is called with correct arguments from regex groups). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 30%\n",
      "ðŸŽ¯ COMPOSITE: 65%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: matplotlib bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py\n",
      "index 71eb153f2..8009207dd 100644\n",
      "--- a/lib/matplotlib/backend_bases.py\n",
      "+++ b/lib/matplotlib/backend_bases.py\n",
      "@@ -46,6 +46,7 @@ from matplotlib._pylab_helpers import Gcf\n",
      " from matplotlib.backend_managers import ToolManager\n",
      " from matplotlib.transforms import Affine2D\n",
      " from matplotlib.path import Path\n",
      "+from matplotlib.cbook import _setattr_cm\n",
      " \n",
      " \n",
      " _log = logging.getLogger(__name__)\n",
      "@@ -1502,15 +1503,14 @@ class KeyEvent\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a lambda late binding bug in backend_bases.py's _get_renderer function (severity 7), which causes all replaced draw methods to incorrectly reference the last loop variable, breaking method disabling functionality. Additional high-priority concerns include incomplete removal of the _setattr_cm context manager across files, leading to potential side effects and lingering references (severities 5), and multiple test coverage gaps for key functions like _get_renderer and tight_layout (priorities 6-7), which lack scenarios for edge cases, error handling, and integration with draw_disabled behavior. No security or performance issues were identified, but style and best practice violations around imports and exception handling should be addressed to prevent subtle bugs.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Lambda late binding issue when disabling draw methods | lib/matplotlib/backend_bases.py | 1531-1545 | 7 | Bug | Define the lambda with meth_name as a default argument to capture its current value: replace setattr(renderer, meth_name, lambda *args, **kwargs: None) with setattr(renderer, meth_name, lambda *args, **kwargs, meth_name=meth_name: None). Alternatively, use functools.partial or a function factory to create distinct no-op functions for each method. | Code Analyzer, Best Practices |\n",
      "| Unused Import Removed Without Replacement | lib/matplotlib/backend_bases.py | 44, 1508, 1510, 1532-1538, 1583 | 5 | Best Practices | Consider preserving the context manager _setattr_cm approach to patch methods safely, or implement a safer equivalent that avoids lambda late-binding issues when replacing draw methods with no-ops. Reintroduce context manager usage for safe temporary patching or ensure modifications are reverted after use to maintain original behavior. | Best Practices |\n",
      "| Missing Try-Except Around File or I/O Operations | lib/matplotlib/backend_bases.py | 1506, 1525, 1590 | 5 | Best Practices | Wrap error-prone operations such as figure drawing and file writing in try-except blocks to catch and handle potential exceptions gracefully, and to release resources properly. | Best Practices |\n",
      "| Removed _setattr_cm import but references commented out without full replacement | lib/matplotlib/figure.py | 2394-2429 | 5 | Bug | Remove all _setattr_cm imports and commented-out usage to fully embrace the new approach. Ensure no other code relies on _setattr_cm for renderer patching, or reintroduce usage consistently. | Code Analyzer |\n",
      "| Incorrect or Missing Import Order and Grouping | lib/matplotlib/figure.py | 2394-2408 | 3 | Style | Group and order imports correctly: standard library imports first, third party imports next, and local application imports last, separated by blank lines for clarity. | Best Practices |\n",
      "| _get_renderer invoked inconsistently regarding draw_disabled parameter | lib/matplotlib/tight_layout.py | 174 | 4 | Bug | Audit usage of draw_disabled parameter to confirm that setting drawing methods to no-ops does not prevent necessary drawing side effects. Consider adding comments or tests to verify behavior consistency across all usages. | Code Analyzer |\n",
      "| Test gap for _get_renderer | lib/matplotlib/backend_bases.py | 1503, 1540 | 7 | Test Gap | unit test | Test Coverage |\n",
      "| Test gap for FigureCanvasBase.print_figure (modified section at line ~2090) | lib/matplotlib/backend_bases.py | 2087, 2105 | 6 | Test Gap | integration test | Test Coverage |\n",
      "| Test gap for Figure.tight_layout (modified section at line ~2392) | lib/matplotlib/figure.py | 2392, 2416 | 6 | Test Gap | integration test | Test Coverage |\n",
      "| Test gap for get_renderer (in lib/matplotlib/tight_layout.py) | lib/matplotlib/tight_layout.py | 170, 177 | 5 | Test Gap | unit and integration test | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 10**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 82%\n",
      "  Line Recall: 86%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 20%\n",
      "ðŸŽ¯ COMPOSITE: 53%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  lib/matplotlib/backend_bases.py: lines 50-52, 1516, 2093-2099\n",
      "\n",
      "âœ— FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: tqdm bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tqdm/contrib/__init__.py b/tqdm/contrib/__init__.py\n",
      "index 1dddacf..935ab63 100644\n",
      "--- a/tqdm/contrib/__init__.py\n",
      "+++ b/tqdm/contrib/__init__.py\n",
      "@@ -38,7 +38,7 @@ def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n",
      "         if isinstance(iterable, np.ndarray):\n",
      "             return tqdm_class(np.ndenumerate(iterable),\n",
      "                               total=total or len(iterable), **tqdm_kwargs)\n",
      "-    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n",
      "+    return \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary issue identified is a critical bug in the `tenumerate` function implementation in `tqdm/contrib/__init__.py`, where the `start` parameter is incorrectly passed to `tqdm_class` as a positional argument instead of to `enumerate`, leading to incorrect indexing starting from 0 regardless of the intended start value. This is compounded by a related best practice violation in parameter handling. Additionally, there are significant test coverage gaps for the `tenumerate` function, missing coverage for normal cases, edge cases, error handling, and integration scenarios.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Incorrect use of enumerate with tqdm_class causing wrong start index | tqdm/contrib/__init__.py | [41, 42] | 7 | Bug | Replace 'return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))' with 'return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)' to correctly set the enumeration start index. Avoid passing 'start' as a positional argument to tqdm_class unless confirmed supported; explicitly handle 'start' in enumerate (e.g., enumerate(..., start=start)). | Code Analyzer, Best Practices |\n",
      "| Missing tests for tenumerate function | tqdm/contrib/__init__.py | [38, 44] | 7 | Test Gap | Implement unit tests covering: normal case (iterating over list with default start, numpy ndarray), edge cases (empty iterable, start at boundary values like negative/zero/large positive), error handling (non-iterable input, unexpected kwargs), integration (different tqdm_class implementations, with total parameter). | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 0%\n",
      "  Line Recall: 0%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 50%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  tqdm/contrib/__init__.py: lines 38-44\n",
      "\n",
      "âœ— FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: tornado bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/tornado/websocket.py b/tornado/websocket.py\n",
      "index 00d08bab..d991fee5 100644\n",
      "--- a/tornado/websocket.py\n",
      "+++ b/tornado/websocket.py\n",
      "@@ -558,8 +558,8 @@ class WebSocketHandler(tornado.web.RequestHandler):\n",
      " \n",
      "         .. versionadded:: 3.1\n",
      "         \"\"\"\n",
      "-        assert self.stream is not None\n",
      "-        self.stream.set_nodelay(value)\n",
      "+        assert self.ws_connection is not None\n",
      "+        self.ws_connection.set_nodelay(value)\n",
      " \n",
      "     def on_connection_close(self) -> None:\n",
      "         if self.ws\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical findings revolve around inconsistencies in the set_nodelay method implementation across WebSocketProtocol, its subclasses like WebSocketProtocol13, and WebSocketHandler, which could lead to runtime errors (severity 6). A related bug in WebSocketHandler's set_nodelay method involves incorrect handling of the connection object, potentially causing performance issues (severity 5). Minor style concerns include missing docstrings and vague parameter names, while a high-priority test gap exists for verifying the WebSocketHandler.set_nodelay method's behavior.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Inconsistent set_nodelay method implementation in WebSocketProtocol classes | tornado/websocket.py | [559, 560, 714, 1345, 1346, 1347] | 6 | Bug | Uncomment and keep set_nodelay as an abstractmethod in WebSocketProtocol. Implement set_nodelay properly in all subclasses such as WebSocketProtocol13. Ensure all subclasses of WebSocketProtocol implement the set_nodelay method fully and consistently to respect the abstract interface contract and avoid runtime errors. | Code Analyzer, Best Practices |\n",
      "| Incorrect object used in set_nodelay in WebSocketHandler | tornado/websocket.py | [559, 560, 561, 562] | 5 | Bug | Restore setting nodelay on self.ws_connection after asserting it's not None, or verify that setting it only on self.stream suffices for all protocols to avoid partial configuration. | Code Analyzer |\n",
      "| Inconsistent or Missing Docstring | tornado/websocket.py | [715, 716, 717, 718] | 3 | Style | Add a proper docstring to the set_nodelay method explaining its role as an abstract method for setting Nagle's algorithm (TCP_NODELAY) on connections. | Best Practices |\n",
      "| Unnumbered Magic Argument 'value' and 'x' Naming Inconsistency | tornado/websocket.py | [559, 560, 1345, 1346] | 4 | Style | Use descriptive and consistent parameter names such as 'enable_nodelay' or 'nodelay' (boolean) across all set_nodelay methods for improved clarity. | Best Practices |\n",
      "| Missing tests for WebSocketHandler.set_nodelay | tornado/websocket.py | [561, 562] | 7 | Test Gap | Unit test with mock stream, integration test in WebSocket connection environment. Cover scenarios: normal case: set nodelay to True; normal case: set nodelay to False; edge case: self.stream is None (should assert fail); edge case: invalid value for set_nodelay (e.g., non-bool); integration: verify nodelay behavior affects underlying TCP stream as expected. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 74%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 87%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  tornado/websocket.py: lines 723, 1352-1357\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: httpie bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/httpie/downloads.py b/httpie/downloads.py\n",
      "index b49e335..972151e 100644\n",
      "--- a/httpie/downloads.py\n",
      "+++ b/httpie/downloads.py\n",
      "@@ -7,6 +7,7 @@ from __future__ import division\n",
      " import os\n",
      " import re\n",
      " import sys\n",
      "+import errno\n",
      " import mimetypes\n",
      " import threading\n",
      " from time import sleep, time\n",
      "@@ -135,12 +136,43 @@ def filename_from_url(url, content_type):\n",
      "     return fn\n",
      " \n",
      " \n",
      "+def trim_filename(filename, max_len):\n",
      "+    if len(filename) > max_len:\n",
      "+        trim_by = len(filename) - max_len\n",
      "+  \n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The primary concern in httpie/downloads.py is the removal of filename trimming logic in the get_unique_filename function and related utilities, which introduces a high-severity bug potentially causing infinite loops or OS errors due to unhandled filename length limits. This change also creates a medium-severity security vulnerability by enabling possible path traversal or denial-of-service through unsanitized long or malicious filenames, alongside best practice violations including missing error handling for os.pathconf calls and loss of code reuse principles. Additionally, there is a high-priority test coverage gap for get_unique_filename, lacking tests for normal operations, conflict resolution, edge cases, error handling, and filesystem integration.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Removed filename trimming leads to potential infinite loop | httpie/downloads.py | 135, 146-156 | 7 | Bug | Restore the trimming logic by reintroducing the trim_filename_if_needed function and calls within get_unique_filename. Before appending suffixes, trim the base filename so the total length does not exceed filesystem limits. Additionally, add error handling for cases where filename length limits cannot be satisfied. | Code Analyzer |\n",
      "| Insecure Filename Handling Leading to Possible Path Traversal | httpie/downloads.py | 133-144 | 6 | Security | Reintroduce proper filename trimming and validation to ensure filenames do not exceed filesystem limits and do not contain invalid or dangerous characters. Sanitize inputs and continue preventing path traversal by validating and normalizing file paths before using them. | Security |\n",
      "| Missing try-except for os.pathconf call | httpie/downloads.py | 34, 42 | 7 | Bug | Reintroduce robust error handling around os.pathconf calls to handle OSError specifically, such as catching EINVAL and using a safe default max length. Alternatively, ensure that any calls that replaced this functionality also handle errors properly. | Best Practices |\n",
      "| Removed resource management and potential code reuse (DRY) issues | httpie/downloads.py | 18, 56 | 5 | Style | Reintroduce or preserve filename trimming logic considering filesystem limits to avoid invalid filenames. If simplification is desired, clearly document that filename length limits are no longer checked. | Best Practices |\n",
      "| Test Coverage Gap for get_unique_filename | httpie/downloads.py | 127, 140 | 8 | Test Gap | unit test with mock for 'exists' function; integration test with temp files in filesystem to ensure uniqueness logic | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 86%\n",
      "  Line Recall: 54%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 20%\n",
      "ðŸŽ¯ COMPOSITE: 37%\n",
      "\n",
      "âŒ MISSED LINES:\n",
      "  httpie/downloads.py: lines 7-12, 162-178\n",
      "\n",
      "âœ— FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: cookiecutter bug 1\n",
      "============================================================\n",
      "\n",
      "ACTUAL FIX (first 500 chars):\n",
      "diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py\n",
      "index 37365a4..c526b97 100644\n",
      "--- a/cookiecutter/generate.py\n",
      "+++ b/cookiecutter/generate.py\n",
      "@@ -82,7 +82,7 @@ def generate_context(\n",
      "     context = OrderedDict([])\n",
      " \n",
      "     try:\n",
      "-        with open(context_file) as file_handle:\n",
      "+        with open(context_file, encoding='utf-8') as file_handle:\n",
      "             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n",
      "     except ValueError as e:\n",
      "         # JSON decoding error.  Let's thr\n",
      "...\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is the missing encoding parameter in file open operations within the `generate_context` function, which can lead to UnicodeDecodeError, potential data corruption, or denial of service implications when handling files with non-ASCII characters; this is merged from findings by Code Analyzer, Security, and Best Practices agents with a highest severity of 7. Comprehensive testing is lacking for this function, covering normal, error, edge, and integration scenarios. Addressing these will improve reliability, security, and maintainability of file handling in cookiecutter/generate.py.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Missing file encoding in open() leading to potential UnicodeDecodeError, data corruption, or security implications from improper handling | cookiecutter/generate.py | [83,84,85] | 7 | Bug | Restore the encoding='utf-8' argument to the open() call, e.g., with open(context_file, encoding='utf-8') as file_handle | Code Analyzer, Security, Best Practices |\n",
      "| Test coverage gap in generate_context function (missing scenarios: normal case valid JSON, JSON decode error, file not found, empty JSON, unexpected keys, permission denied, directory as file path) | cookiecutter/generate.py | [82,83,84,85,86,87,88,89] | 7 | Test Gap | unit test | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 2**\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ LOCATION METRICS:\n",
      "  File Recall: 100%\n",
      "  Line Precision: 100%\n",
      "  Line Recall: 100%\n",
      "\n",
      "ðŸ¤– LLM RELEVANCE: 100%\n",
      "ðŸŽ¯ COMPOSITE: 100%\n",
      "\n",
      "âœ“ PASSED\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "âœ“ luigi/2: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ black/4: Composite=75% (LineRec=100%, LLM=50%)\n",
      "âœ“ pandas/1: Composite=100% (LineRec=100%, LLM=100%)\n",
      "âœ“ scrapy/1: Composite=90% (LineRec=100%, LLM=80%)\n",
      "âœ“ thefuck/1: Composite=65% (LineRec=100%, LLM=30%)\n",
      "âœ— matplotlib/1: Composite=53% (LineRec=86%, LLM=20%) | Missed lines: 11\n",
      "âœ— tqdm/1: Composite=50% (LineRec=0%, LLM=100%) | Missed lines: 7\n",
      "âœ“ tornado/1: Composite=87% (LineRec=74%, LLM=100%) | Missed lines: 7\n",
      "âœ— httpie/1: Composite=37% (LineRec=54%, LLM=20%) | Missed lines: 23\n",
      "âœ“ cookiecutter/1: Composite=100% (LineRec=100%, LLM=100%)\n",
      "\n",
      "Passed: 7/10 (70%)\n"
     ]
    }
   ],
   "source": [
    "# Test 10 bugs from different projects\n",
    "bugs_to_test_10 = [\n",
    "    (\"luigi\", 2),\n",
    "    (\"black\", 4),\n",
    "    (\"pandas\", 1),\n",
    "    (\"scrapy\", 1),\n",
    "    (\"thefuck\", 1),\n",
    "    (\"matplotlib\", 1),\n",
    "    (\"tqdm\", 1),\n",
    "    (\"tornado\", 1),\n",
    "    (\"httpie\", 1),\n",
    "    (\"cookiecutter\", 1),\n",
    "]\n",
    "\n",
    "results_10 = await test_bugsinpy_with_miss_analysis(bugs_to_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
