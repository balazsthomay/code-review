{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5822e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "OpenRouter API Key exists and begins sk-or-v1\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c333c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_code_fast_1=LitellmModel(model=\"openrouter/x-ai/grok-code-fast-1\", api_key=openrouter_api_key)\n",
    "grok_4_fast=LitellmModel(model=\"openrouter/x-ai/grok-4-fast\", api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the bug\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "\n",
    "class VulnerabilityFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the vulnerability\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    cve_reference: str | None = Field(default=None, description=\"CVE ID if applicable\")\n",
    "\n",
    "class BestPracticeFinding(BaseModel):\n",
    "    title: str = Field(description=\"Brief name for the best practice violation\")\n",
    "    description: str = Field(description=\"Detailed explanation\")\n",
    "    severity: int = Field(description=\"Severity 1-10\")\n",
    "    file: str = Field(description=\"File path\")\n",
    "    relevant_lines: list[int] = Field(description=\"Line numbers\")\n",
    "    suggested_fix: str = Field(description=\"Recommended solution\")\n",
    "    \n",
    "class TestGap(BaseModel):\n",
    "    function_name: str = Field(description=\"Name of the function/method lacking tests\")\n",
    "    file: str = Field(description=\"File containing the untested code\")\n",
    "    lines: list[int] = Field(description=\"Line numbers of the untested code\")\n",
    "    missing_scenarios: list[str] = Field(description=\"Specific test cases that should be added, e.g., ['edge case: empty input', 'error handling: invalid type']\")\n",
    "    priority: int = Field(description=\"Priority 1-10, based on code criticality\")\n",
    "    suggested_test_approach: str = Field(description=\"How to test this (unit test, integration test, etc.)\")\n",
    "    \n",
    "class CodeAnalyzerOutput(BaseModel):\n",
    "    findings: list[BugFinding] = Field(description=\"Bugs and anti-patterns found\")\n",
    "\n",
    "class SecurityOutput(BaseModel):\n",
    "    findings: list[VulnerabilityFinding] = Field(description=\"Security vulnerabilities found\")\n",
    "\n",
    "class BestPracticesOutput(BaseModel):\n",
    "    findings: list[BestPracticeFinding] = Field(description=\"Style and best practice violations\")\n",
    "\n",
    "class TestCoverageOutput(BaseModel):\n",
    "    findings: list[TestGap] = Field(description=\"Testing gaps found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc14d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_analyzer_instructions = \"\"\"You are a Code Analyzer agent reviewing a pull request diff. \n",
    "Identify bugs and anti-patterns including: logic errors, unhandled edge cases, null/undefined access, type mismatches, off-by-one errors, resource leaks (unclosed files/cursors/connections), infinite loops, missing error handling (no try-except blocks), code duplication, and overly complex functions. \n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "security_instructions = \"\"\"You are a Security agent reviewing a pull request diff. \n",
    "Identify security vulnerabilities including: SQL injection, command injection, XSS vulnerabilities, hardcoded secrets/credentials, insecure authentication, path traversal, insecure deserialization, improper input validation, and missing error handling that could expose sensitive information.\n",
    "For each issue found, specify the exact lines, severity (1-10), clear fix, and CVE reference if applicable.\"\"\"\n",
    "\n",
    "best_practices_instructions = \"\"\"You are a Best Practices agent reviewing a pull request diff. \n",
    "Identify code quality issues including: unclear variable names, functions exceeding 50 lines, nested complexity over 3 levels, missing docstrings, inconsistent formatting, magic numbers without explanation, violations of DRY principle, unclosed resources (files, database cursors, connections), and missing try-except blocks for error-prone operations.\n",
    "For each issue found, specify the exact lines, severity (1-10), and a clear fix.\"\"\"\n",
    "\n",
    "test_coverage_instructions = \"\"\"You are a Test Coverage agent reviewing a pull request diff. \n",
    "For each new or modified function, suggest test cases covering: normal input cases, edge cases (empty, null, boundary values), error conditions (exceptions, failures, timeouts), and integration scenarios.\n",
    "For each gap found, specify the function name, lines, missing test scenarios, priority (1-10), and whether unit or integration tests are needed.\"\"\"\n",
    "\n",
    "code_analyzer = Agent(\n",
    "    name=\"Code Analyzer\",\n",
    "    instructions=code_analyzer_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=CodeAnalyzerOutput\n",
    ")\n",
    "\n",
    "security_agent = Agent(\n",
    "    name=\"Security Agent\",\n",
    "    instructions=security_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=SecurityOutput\n",
    ")\n",
    "\n",
    "best_practices_agent = Agent(\n",
    "    name=\"Best Practices Agent\",\n",
    "    instructions=best_practices_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=BestPracticesOutput\n",
    ")\n",
    "\n",
    "test_coverage_agent = Agent(\n",
    "    name=\"Test Coverage Agent\",\n",
    "    instructions=test_coverage_instructions,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=TestCoverageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all_agents(diff):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(code_analyzer, diff),\n",
    "        Runner.run(security_agent, diff),\n",
    "        Runner.run(best_practices_agent, diff),\n",
    "        Runner.run(test_coverage_agent, diff)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c1bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_findings(\n",
    "    code_result,\n",
    "    security_result, \n",
    "    best_practices_result,\n",
    "    test_coverage_result\n",
    "):\n",
    "    \"\"\"\n",
    "    Organizes all findings by file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"file.py\": [Finding, Finding, TestGap, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    organized = {}\n",
    "    for result in [code_result, security_result,  best_practices_result, test_coverage_result]:\n",
    "        for finding in result.final_output.findings:\n",
    "            file = finding.file\n",
    "            if file not in organized:\n",
    "                organized[file] = []\n",
    "            organized[file].append(finding)\n",
    "        \n",
    "    return organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "421bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_instructions = \"\"\"You are a Code Review Aggregator tasked with creating a deduplicated summary report. Your goal is to merge duplicate findings from multiple agents into a clear, actionable report.\n",
    "\n",
    "You will be provided with findings from multiple agents:\n",
    "<findings>\n",
    "{organized}\n",
    "</findings>\n",
    "\n",
    "When creating the report, follow these guidelines:\n",
    "\n",
    "1. IDENTIFY DUPLICATES: Group findings that describe the same root issue\n",
    "   - Look for overlapping line numbers and similar descriptions\n",
    "   - When multiple agents flag the same problem, merge into one issue\n",
    "   - Use the HIGHEST severity when merging\n",
    "\n",
    "2. PRESERVE INFORMATION: \n",
    "   - Keep agent names: Code Analyzer, Security, Best Practices, Test Coverage\n",
    "   - Include file paths and line numbers\n",
    "   - Maintain the most comprehensive description from merged findings\n",
    "\n",
    "3. CATEGORIZE each issue as:\n",
    "   - Bug: Logic errors, crashes, incorrect behavior  \n",
    "   - Security: Vulnerabilities, unsafe code\n",
    "   - Performance: Inefficient algorithms, resource issues\n",
    "   - Style: Naming, formatting, documentation\n",
    "   - Test Gap: Missing test coverage\n",
    "\n",
    "4. CREATE SUMMARY TABLE with these columns:\n",
    "   | Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "\n",
    "5. SEPARATE CONCERNS: Test coverage gaps are distinct from code issues\n",
    "\n",
    "Present your report in this format:\n",
    "\n",
    "# Code Review Report\n",
    "\n",
    "## Executive Summary\n",
    "[2-3 sentences highlighting the most critical findings]\n",
    "\n",
    "## Summary of Actions\n",
    "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
    "|-------|------|-------|----------|----------|-----|----------|\n",
    "[One row per unique issue]\n",
    "\n",
    "**Total Distinct Issues: [count]**\n",
    "\n",
    "CRITICAL REQUIREMENT: \n",
    "- EVERY finding from EVERY agent must appear in the summary table\n",
    "- This includes ALL test coverage gaps reported by the Test Coverage agent\n",
    "- Test gaps should be listed as separate rows (one per function needing tests)\n",
    "- Do NOT omit any findings, especially test coverage gaps\n",
    "- The Total Distinct Issues count must match the number of rows in the table.\"\"\"\n",
    "\n",
    "aggregator = Agent(\n",
    "    name=\"Aggregator\",\n",
    "    instructions=aggregator_instructions,\n",
    "    model=grok_4_fast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "069ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aggregator_agent(organized):\n",
    "    result = await Runner.run(aggregator, f\"Aggregate these findings into a structured report:\\n\\n{organized}\")\n",
    "    return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ce4a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "#     \"\"\"\n",
    "#     Complete code review pipeline.\n",
    "    \n",
    "#     Args:\n",
    "#         diff: The code diff to review\n",
    "        \n",
    "#     Returns:\n",
    "#         Markdown-formatted code review report\n",
    "#     \"\"\"\n",
    "#     results = await run_all_agents(diff)\n",
    "#     code_result, security_result, best_practices_result, test_coverage_result = results    \n",
    "#     organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "#     report = await aggregator_agent(organized)\n",
    "    \n",
    "#     if save_output:\n",
    "#         os.makedirs(\"user-data\", exist_ok=True)\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "#         with open(filepath, \"w\") as f:\n",
    "#             f.write(report)\n",
    "#         print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "#     return report\n",
    "\n",
    "\n",
    "async def review_code(diff: str, save_output: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete code review pipeline.\n",
    "    \n",
    "    Args:\n",
    "        diff: The code diff to review\n",
    "        \n",
    "    Returns:\n",
    "        Markdown-formatted code review report\n",
    "    \"\"\"\n",
    "    results = await run_all_agents(diff)\n",
    "    code_result, security_result, best_practices_result, test_coverage_result = results\n",
    "    \n",
    "    # # DEBUG: Print all agent outputs\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"CODE ANALYZER RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in code_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"SECURITY AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in security_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"BEST PRACTICES AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for finding in best_practices_result.final_output.findings:\n",
    "    #     print(f\"\\nTitle: {finding.title}\")\n",
    "    #     print(f\"Severity: {finding.severity}\")\n",
    "    #     print(f\"Lines: {finding.relevant_lines}\")\n",
    "    #     print(f\"Description: {finding.description[:150]}...\")\n",
    "    # print(\"=\"*60)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"TEST COVERAGE AGENT RAW OUTPUT:\")\n",
    "    # print(\"=\"*60)\n",
    "    # for gap in test_coverage_result.final_output.findings:\n",
    "    #     print(f\"\\nFunction: {gap.function_name}\")\n",
    "    #     print(f\"Priority: {gap.priority}\")\n",
    "    #     print(f\"Lines: {gap.lines}\")\n",
    "    #     print(f\"Missing scenarios: {gap.missing_scenarios}\")\n",
    "    # print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    organized = organize_findings(code_result, security_result, best_practices_result, test_coverage_result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALLING AGGREGATOR...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report = await aggregator_agent(organized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATOR OUTPUT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if save_output:\n",
    "        os.makedirs(\"user-data\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = f\"user-data/code_review_{timestamp}.md\"\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved to {filepath}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "211922ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CODE ANALYZER RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [8, 9, 10]\n",
      "Description: The authenticate method constructs SQL queries by directly concatenating user inputs (username and password) into the query string. This allows an att...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SECURITY AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection in authenticate method\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method constructs an SQL query by directly concatenating user inputs (username and password) into the query string. This allows an at...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BEST PRACTICES AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Title: SQL Injection Vulnerability\n",
      "Severity: 9\n",
      "Lines: [7, 8, 9]\n",
      "Description: The authenticate method builds the SQL query by concatenating user inputs directly into the query string. This approach is vulnerable to SQL injection...\n",
      "\n",
      "Title: Missing Cursor Closure\n",
      "Severity: 5\n",
      "Lines: [8, 9]\n",
      "Description: The database cursor created in the authenticate method is not explicitly closed, which can lead to resource leaks....\n",
      "\n",
      "Title: No Exception Handling for Database Operations\n",
      "Severity: 6\n",
      "Lines: [7, 8, 9, 10]\n",
      "Description: The authenticate method does not include try-except blocks around database operations, making it prone to unhandled exceptions that could crash the ap...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST COVERAGE AGENT RAW OUTPUT:\n",
      "============================================================\n",
      "\n",
      "Function: authenticate\n",
      "Priority: 9\n",
      "Lines: [7, 8, 9, 10, 11, 12]\n",
      "Missing scenarios: ['normal case: valid username and password', 'edge case: empty username and/or password', 'edge case: SQL injection attempt in username or password', 'error handling: database connection failure or query execution error', 'integration: interaction with actual database and user records']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue identified is a severe SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to compromise the database or bypass authentication. Additional concerns include missing cursor closure leading to potential resource leaks and lack of exception handling around database operations, both increasing the risk of application instability. Test coverage is notably deficient for the `authenticate` function, lacking scenarios for normal use, edges, security attempts, errors, and integration.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability in authenticate method | user_auth.py | 7-10 | 9 | Security | Use parameterized queries with placeholders to safely insert user input values into the SQL query, preventing injection attacks. For example, use `query = \"SELECT * FROM users WHERE username=? AND password=?\"` and pass `(username, password)` as parameters to `cursor.execute()`. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Cursor Closure | user_auth.py | 8-9 | 5 | Performance | Use a context manager (with statement) when creating the cursor to ensure it is closed automatically, e.g., `with self.db.cursor() as cursor: cursor.execute(query, (username, password)) ...`. | Best Practices |\n",
      "| No Exception Handling for Database Operations | user_auth.py | 7-10 | 6 | Bug | Add try-except blocks to catch database exceptions and handle them gracefully, for example: `try: ... except sqlite3.DatabaseError as e: # Handle or log the error return False`. | Best Practices |\n",
      "| Missing tests for authenticate function | user_auth.py | 7-12 | 9 | Test Gap | Unit tests with mocking for query execution and integration tests with a test database, covering: normal case (valid username and password), edge cases (empty username/password, SQL injection attempts), error handling (database connection failure or query execution error), and integration with actual database and user records. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_file = Path(\"test-cases/01_sql_injection.diff\")\n",
    "diff_content = diff_file.read_text()\n",
    "\n",
    "report = await review_code(diff_content, save_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a73ded2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"You are an evaluation judge for code review systems comparing expected findings (ground truth) against actual findings.\n",
    "\n",
    "CRITICAL MATCHING RULES:\n",
    "1. Each actual finding can match AT MOST ONE expected finding\n",
    "2. Each expected finding can match AT MOST ONE actual finding\n",
    "3. Once an actual finding is matched, it CANNOT be used again\n",
    "4. Only match within same category (bugs ≠ test gaps)\n",
    "\n",
    "PROCESS:\n",
    "1. Count total_actual from \"Total Distinct Issues: X\" in report\n",
    "2. For EACH expected finding:\n",
    "   - Find the BEST matching actual finding that hasn't been used yet\n",
    "   - If good match exists: mark as matched=True, record which actual finding\n",
    "   - If no match: mark as matched=False\n",
    "   - NEVER reuse an actual finding for multiple expected findings\n",
    "\n",
    "A match means the same type of issue was identified, even if worded differently.\n",
    "\"\"\"\n",
    "\n",
    "class MatchedFinding(BaseModel):\n",
    "    expected: str = Field(description=\"the expected finding text\")\n",
    "    matched: bool = Field(description=\"true if the expected finding is present, else false\")\n",
    "    actual_finding: Optional[str] = Field(default=None, description=\"the matching text from report (if matched)\")\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    matched_findings: list[MatchedFinding]\n",
    "    total_expected: int = Field(description=\"Total number of expected findings from ground truth\")\n",
    "    total_actual: int = Field(description=\"Count of distinct issues in the report's summary section\")\n",
    "    # matches: int = Field(description=\"Number of expected findings successfully matched\")\n",
    "    \n",
    "    def model_post_init(self, __context):\n",
    "        # Calculate matches from the list\n",
    "        matches = sum(1 for mf in self.matched_findings if mf.matched)\n",
    "        \n",
    "        # Check for duplicate actual findings\n",
    "        actual_findings_used = [\n",
    "            mf.actual_finding for mf in self.matched_findings \n",
    "            if mf.matched and mf.actual_finding\n",
    "        ]\n",
    "        unique_actuals = len(set(actual_findings_used))\n",
    "        \n",
    "        if matches > unique_actuals:\n",
    "            print(f\"ERROR: {matches} matches but only {unique_actuals} unique actual findings used!\")\n",
    "            print(\"The judge matched the same actual finding multiple times.\")\n",
    "        \n",
    "        if matches > self.total_actual:\n",
    "            print(f\"WARNING: Matches ({matches}) > Total Actual ({self.total_actual})\")\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_report(report: str, ground_truth_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixed evaluation function with proper counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        name=\"Evaluation Judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"gpt-5.1\",\n",
    "        output_type=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "GROUND TRUTH (expected findings):\n",
    "{ground_truth_content}\n",
    "\n",
    "ACTUAL REPORT (what the system found):\n",
    "{report}\n",
    "\n",
    "For each expected finding, determine if it matches any actual finding.\n",
    "Output matched_findings list, total_expected, and total_actual.\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(judge_agent, prompt)\n",
    "    eval_result = result.final_output\n",
    "    \n",
    "    # Calculate matches from the actual data - don't trust LLM counting\n",
    "    matches = sum(1 for mf in eval_result.matched_findings if mf.matched)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = matches / eval_result.total_expected if eval_result.total_expected > 0 else 0\n",
    "    precision = matches / eval_result.total_actual if eval_result.total_actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"matches\": matches,\n",
    "        \"total_expected\": eval_result.total_expected,\n",
    "        \"total_actual\": eval_result.total_actual,\n",
    "        \"details\": eval_result.matched_findings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c9aa656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue is a high-severity SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to bypass authentication or access sensitive data; it must be addressed immediately using parameterized queries. Additional bugs include missing exception handling that risks application crashes, while best practices highlight resource leaks from unclosed cursors and lack of documentation. Test coverage for the `authenticate` function is severely lacking, with no tests for normal, edge, security, and error scenarios, requiring comprehensive unit and integration tests.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability | user_auth.py | 7-12 | 9 | Security | Use parameterized queries or prepared statements to safely pass user inputs to the SQL query, e.g., replace query construction with `query = \"SELECT * FROM users WHERE username=? AND password=?\"` and call `cursor.execute(query, (username, password))`. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Exception Handling for Database Operations | user_auth.py | 7-11 | 6 | Bug | Wrap database operations inside try-except blocks to catch potential sqlite3.DatabaseError or other exceptions and handle them gracefully. | Code Analyzer |\n",
      "| Unclosed Cursor Resource | user_auth.py | 9-10 | 5 | Performance | Use a context manager (with statement) for the cursor or explicitly close the cursor after use to ensure resources are properly released. | Best Practices |\n",
      "| Missing Docstring for Method | user_auth.py | 7,12 | 3 | Style | Add a docstring to the authenticate method explaining what it does, its input parameters, and what it returns. | Best Practices |\n",
      "| Test Coverage Gap in authenticate | user_auth.py | 7-11 | 9 | Test Gap | Add unit tests for function behavior including mocking database cursor (covering normal case: valid username and password returns True; edge case: empty username and/or password; edge case: SQL injection attempt in username or password; error handling: database connection failure; error handling: exceptions from malformed SQL query) and integration tests for full authentication flow with real database and user data. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 5\n",
      "total_actual: 5\n",
      "matches: 5\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: SQL injection vulnerability in authenticate method due to string concatenation of user inputs\n",
      "  Matched: True\n",
      "  Actual: SQL Injection Vulnerability | user_auth.py | 7-12 | 9 | Security | Use parameterized queries or prep...\n",
      "\n",
      "  Expected: Missing docstring for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing Docstring for Method | user_auth.py | 7,12 | 3 | Style | Add a docstring to the authenticate...\n",
      "\n",
      "  Expected: No error handling for database query failures\n",
      "  Matched: True\n",
      "  Actual: Missing Exception Handling for Database Operations | user_auth.py | 7-11 | 6 | Bug | Wrap database o...\n",
      "\n",
      "  Expected: Database cursor not explicitly closed (best practice)\n",
      "  Matched: True\n",
      "  Actual: Unclosed Cursor Resource | user_auth.py | 9-10 | 5 | Performance | Use a context manager (with state...\n",
      "\n",
      "  Expected: Missing test coverage for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Test Coverage Gap in authenticate | user_auth.py | 7-11 | 9 | Test Gap | Add unit tests for function...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Run test 2\n",
    "test_dir = Path(\"test-cases\")\n",
    "diff_file = test_dir / \"01_sql_injection.diff\"\n",
    "\n",
    "# Load files\n",
    "diff_content = diff_file.read_text()\n",
    "expected_file = diff_file.with_name(\"01_sql_injection_expected.json\")\n",
    "ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# Run review WITH saving\n",
    "report = await review_code(diff_content, save_output=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_result = await evaluate_report(report, ground_truth_content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JUDGE OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "print(f\"matches: {eval_result['matches']}\")\n",
    "print(f\"\\nmatched_findings:\")\n",
    "for mf in eval_result['details']:\n",
    "    print(f\"\\n  Expected: {mf.expected}\")\n",
    "    print(f\"  Matched: {mf.matched}\")\n",
    "    if mf.actual_finding:\n",
    "        print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATED METRICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a2694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the same test twice to check consistency\n",
    "# test_dir = Path(\"test-cases\")\n",
    "# diff_file = test_dir / \"04_multi_file_security.diff\"\n",
    "\n",
    "# # Load files\n",
    "# diff_content = diff_file.read_text()\n",
    "# expected_file = test_dir / \"04_multi_file_security_expected.json\"\n",
    "# ground_truth_content = expected_file.read_text()\n",
    "\n",
    "# # Run twice\n",
    "# for i in range(2):\n",
    "#     print(f\"\\n=== RUN {i+1} ===\")\n",
    "    \n",
    "#     # Run review\n",
    "#     report = await review_code(diff_content, save_output=False)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     eval_result = await evaluate_report(report, ground_truth_content)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"JUDGE OUTPUT:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "#     print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "#     print(f\"matches: {eval_result['matches']}\")\n",
    "#     print(f\"\\nmatched_findings:\")\n",
    "#     for mf in eval_result['details']:\n",
    "#         print(f\"\\n  Expected: {mf.expected}\")\n",
    "#         print(f\"  Matched: {mf.matched}\")\n",
    "#         if mf.actual_finding:\n",
    "#             print(f\"  Actual: {mf.actual_finding[:100]}...\")  # truncate if long\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"CALCULATED METRICS:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "#     print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "#     print(f\"F1 Score: {eval_result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fa37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING: 01_sql_injection\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issue identified is a severe SQL injection vulnerability in the `authenticate` method of `user_auth.py`, flagged by multiple agents, which could allow attackers to bypass authentication or execute arbitrary database commands. Additional concerns include resource leaks from unclosed cursors and missing documentation, both of which impact maintainability and reliability. Test coverage for the `authenticate` function is inadequate, lacking scenarios for normal, edge, error, and integration cases.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| SQL Injection Vulnerability | user_auth.py | [7, 8, 9, 10, 11] | 9 | Security | Use parameterized queries to safely pass user inputs into the SQL statement, e.g., use 'cursor.execute(\"SELECT * FROM users WHERE username=? AND password=?\", (username, password))' instead of string concatenation. | Code Analyzer, Security, Best Practices |\n",
      "| Unclosed cursor resource | user_auth.py | [9, 11] | 5 | Bug | Use a context manager (with statement) for the cursor to ensure it is automatically closed after execution. | Best Practices |\n",
      "| Missing docstring in authenticate method | user_auth.py | [7, 13] | 3 | Style | Add a docstring to the 'authenticate' method explaining what it does, its parameters, and return value. | Best Practices |\n",
      "| Test coverage gap for authenticate function | user_auth.py | [7, 13] | 9 | Test Gap | Unit tests for input variations and error conditions (normal case: valid username and password; edge case: empty username and/or password; edge case: very long username and/or password; error case: SQL injection attempt via username or password; error case: database connection failure or cursor execution error); integration tests for end-to-end authentication flow with user session creation. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 4**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 5\n",
      "total_actual: 4\n",
      "matches: 4\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: SQL injection vulnerability in authenticate method due to string concatenation of user inputs\n",
      "  Matched: True\n",
      "  Actual: SQL Injection Vulnerability | user_auth.py | [7, 8, 9, 10, 11] | 9 | Security | Use parameterized qu...\n",
      "\n",
      "  Expected: Missing docstring for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Missing docstring in authenticate method | user_auth.py | [7, 13] | 3 | Style | Add a docstring to t...\n",
      "\n",
      "  Expected: No error handling for database query failures\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Database cursor not explicitly closed (best practice)\n",
      "  Matched: True\n",
      "  Actual: Unclosed cursor resource | user_auth.py | [9, 11] | 5 | Bug | Use a context manager (with statement)...\n",
      "\n",
      "  Expected: Missing test coverage for authenticate method\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for authenticate function | user_auth.py | [7, 13] | 9 | Test Gap | Unit tests for...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.80\n",
      "Precision: 1.00\n",
      "F1 Score: 0.89\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 02_logic_bug\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The codebase in inventory.py contains a critical off-by-one error in the find_item method, which can cause an IndexError and potential application crash or denial of service, flagged at severity 8 by multiple agents. Additional issues include an inefficient implementation of get_last_n_items and missing docstrings, impacting performance and maintainability. Test coverage gaps are significant, particularly for find_item with high-priority missing scenarios like edge cases and error conditions.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Off-by-one error in find_item method causing IndexError | inventory.py | [13, 14, 15, 16, 17, 18] | 8 | Bug | Change the loop to range(len(self.items)) or better, use 'for item in self.items:' to avoid index issues; add try-except if necessary for error handling | Code Analyzer, Security, Best Practices |\n",
      "| Inefficient get_last_n_items method | inventory.py | [5, 7, 8, 9, 10, 11, 12] | 4 | Performance | Replace the loop with Python list slicing: return self.items[-n:] | Best Practices |\n",
      "| Missing docstrings for methods | inventory.py | [5, 13] | 3 | Style | Add appropriate docstrings following PEP-257 conventions to both get_last_n_items and find_item methods | Best Practices |\n",
      "| Test coverage gap for get_last_n_items | inventory.py | [6, 14] | 6 | Test Gap | Add unit tests covering normal case (n less than list length), edge cases (n equals zero, n greater than list length, n negative, empty list), and error conditions (n not integer, n None) | Test Coverage |\n",
      "| Test coverage gap for find_item | inventory.py | [15, 23] | 9 | Test Gap | Add unit tests covering normal case (item_id exists), edge cases (item_id does not exist, empty list), and error conditions (item_id None, different data type, index out of range causing exception) | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 5**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 6\n",
      "total_actual: 5\n",
      "matches: 5\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Off-by-one error in find_item causing IndexError - loop iterates beyond list bounds\n",
      "  Matched: True\n",
      "  Actual: Off-by-one error in find_item method causing IndexError...\n",
      "\n",
      "  Expected: Inefficient implementation of get_last_n_items - should use list slicing\n",
      "  Matched: True\n",
      "  Actual: Inefficient get_last_n_items method...\n",
      "\n",
      "  Expected: No error handling for edge cases in get_last_n_items (n=0, n>length, negative n, empty list)\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for both methods\n",
      "  Matched: True\n",
      "  Actual: Missing docstrings for methods...\n",
      "\n",
      "  Expected: Missing test coverage for get_last_n_items\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for get_last_n_items...\n",
      "\n",
      "  Expected: Missing test coverage for find_item\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for find_item...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.83\n",
      "Precision: 1.00\n",
      "F1 Score: 0.91\n",
      "Status: ✓ PASSED\n",
      "\n",
      "============================================================\n",
      "TESTING: 03_code_quality\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The most critical issues in calculator.py revolve around the deeply nested and repetitive conditional logic in the `calculate` method, which severely impacts readability and maintainability (severity 6), alongside inadequate handling of zero and negative input values that may mask errors (severity 5). Additionally, the `calculate` function lacks comprehensive test coverage for boundary and error scenarios (priority 8), posing risks to reliability. Other concerns include magic numbers and missing docstrings, which hinder code clarity, while the `process` method requires basic unit tests for input handling.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Deeply Nested and Repetitive Conditional Logic in calculate | calculator.py | 2-25 | 6 | Style | Refactor the method to reduce nesting by using a single combined condition with logical AND (e.g., if a > 0 and b > 0 and c > 0 and d > 0 and e > 0) or early returns. Check if any parameter is less than or equal to zero at the beginning and return 0 early to eliminate repetitive else branches and improve readability/maintainability. | Code Analyzer, Best Practices |\n",
      "| Inconsistent Handling of Zero and Negative Values in calculate | calculator.py | 3,6,9,12,15,18,21,24 | 5 | Bug | Consider raising an exception or handling edge cases explicitly instead of returning 0 silently. Document expected input ranges to avoid masking input errors. | Code Analyzer |\n",
      "| Magic Numbers Without Explanation in calculate | calculator.py | 10,12,13,14 | 4 | Style | Define the numeric factors (1.15, 0.95, 1.08, 1.12, 0.88) as named constants at the class or module level with descriptive names to clarify their meaning and facilitate changes. | Best Practices |\n",
      "| Missing Docstrings in calculate and process Methods | calculator.py | 3,26,27 | 3 | Style | Add docstrings to both methods explaining their functionality, input parameters, expected ranges, and return values. | Best Practices |\n",
      "| Test Coverage Gap for calculate (missing scenarios: normal inputs with all positive values, each parameter exactly zero (boundary), each parameter negative (error case), non-integer and non-float types (e.g., strings, None), very large input values (boundary), interaction of parameter conditions (e.g., some positive, some zero, some negative)) | calculator.py | 4,24 | 8 | Test Gap | Add unit tests covering all listed missing scenarios to ensure robust handling of inputs, boundaries, and errors. | Test Coverage |\n",
      "| Test Coverage Gap for process (missing scenarios: normal input cases with typical data types, empty input, null/None input, test with complex data types (e.g., dict, list)) | calculator.py | 25,27 | 3 | Test Gap | Add unit tests covering all listed missing scenarios to validate input handling and edge cases. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 6**\n",
      "============================================================\n",
      "\n",
      "ERROR: 7 matches but only 6 unique actual findings used!\n",
      "The judge matched the same actual finding multiple times.\n",
      "WARNING: Matches (7) > Total Actual (6)\n",
      "WARNING: Invalid logic - Matches (7) > Total Actual (6)\n",
      "This indicates the judge is still matching incorrectly.\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 9\n",
      "total_actual: 6\n",
      "matches: 7\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Excessive nesting depth in calculate method (5 levels deep)\n",
      "  Matched: True\n",
      "  Actual: Deeply Nested and Repetitive Conditional Logic in calculate...\n",
      "\n",
      "  Expected: Magic numbers without explanation (1.15, 0.95, 1.08, 1.12, 0.88)\n",
      "  Matched: True\n",
      "  Actual: Magic Numbers Without Explanation in calculate...\n",
      "\n",
      "  Expected: Unclear variable names (a, b, c, d, e, x, y, z)\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for class and both methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings in calculate and process Methods...\n",
      "\n",
      "  Expected: calculate method returns 0 for any negative input without clear business logic justification\n",
      "  Matched: True\n",
      "  Actual: Inconsistent Handling of Zero and Negative Values in calculate...\n",
      "\n",
      "  Expected: Repeated return 0 statements could be simplified\n",
      "  Matched: True\n",
      "  Actual: Deeply Nested and Repetitive Conditional Logic in calculate...\n",
      "\n",
      "  Expected: process method does nothing useful and should be removed or implemented\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for calculate method including positive inputs, negative inputs, edge cases, and validation of magic number calculations\n",
      "  Matched: True\n",
      "  Actual: Test Coverage Gap for calculate (missing scenarios: normal inputs with all positive values, each par...\n",
      "\n",
      "  Expected: Missing test coverage or clarification for process method purpose\n",
      "  Matched: True\n",
      "  Actual: Test Coverage Gap for process (missing scenarios: normal input cases with typical data types, empty ...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.78\n",
      "Precision: 1.17\n",
      "F1 Score: 0.93\n",
      "Status: ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: 04_multi_file_security\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The codebase exhibits critical security vulnerabilities, including hardcoded sensitive keys, command injection risks via os.system, insecure pickle deserialization, and path traversal attacks in file handling operations across both files. These issues pose high risks of credential leakage, arbitrary code execution, and unauthorized file access, with severities reaching 10. Additionally, there are multiple gaps in test coverage for key functions, alongside best practice violations like missing docstrings, inconsistent imports, and inadequate error handling that could lead to runtime failures.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Hardcoded Sensitive Keys/Credentials | api_handler.py | [3,4,5,6,7] | 9 | Security | Remove hardcoded sensitive keys and credentials from the source code. Load them from secure environment variables using os.environ or a secure configuration management system like vaults to enable key rotation and prevent leakage if code is exposed. | Code Analyzer, Security, Best Practices |\n",
      "| Command Injection Vulnerability via os.system | api_handler.py | [7,8,9,10] | 10 | Security | Avoid using os.system with unsanitized user input directly. Use subprocess.run with a list of arguments (shell=False) or properly sanitize and validate user_cmd before execution to prevent arbitrary shell command injection. | Code Analyzer, Security, Best Practices |\n",
      "| Insecure Deserialization with Pickle Load | api_handler.py | [11,12,13] | 9 | Security | Avoid using pickle.load on untrusted or unauthenticated data due to remote code execution risks. Replace with safe serialization formats like JSON, implement strict validation before deserialization, and add try-except blocks to catch exceptions for corrupted or malicious files. | Code Analyzer, Security, Best Practices |\n",
      "| Missing Docstrings in Classes and Methods | api_handler.py | [3,5,8,11] | 4 | Style | Add descriptive docstrings to the APIHandler and FileHandler classes and their methods, explaining their purpose, responsibilities, parameters, and usage to improve code maintainability and readability. | Best Practices |\n",
      "| Test Gap: execute_command | api_handler.py | [8,9,10] | 9 | Test Gap | Implement unit tests with mocking of os.system to cover normal input execution, empty string command, command with special shell characters, handling of potentially unsafe input to avoid injection, and simulated os.system failure or exception scenarios. | Test Coverage |\n",
      "| Test Gap: load_data | api_handler.py | [11,12,13] | 8 | Test Gap | Implement unit tests with fixture files and exception simulation to cover loading valid pickle file, file not found error handling, corrupted pickle file handling (unpickle exceptions), loading large data, and permissions error opening file. | Test Coverage |\n",
      "| Path Traversal Vulnerability in read_user_file | file_handler.py | [3,4,5,6] | 9 | Security | Validate and sanitize the filename input to prevent directory traversal (e.g., '..' or absolute paths), using os.path.basename, whitelisting, or pathlib.resolve to restrict paths within allowed directories before concatenating to the base path. | Code Analyzer, Security |\n",
      "| Missing File Mode Specification in open() | file_handler.py | [6] | 3 | Bug | Specify the file open mode explicitly (e.g., 'r') and the encoding if needed (e.g., open(path, 'r', encoding='utf-8')) in read_user_file to ensure consistent behavior and avoid default assumptions. | Code Analyzer |\n",
      "| Path Traversal Vulnerability in delete_file | file_handler.py | [8,9,10,11,12] | 8 | Security | Validate and sanitize the filename parameter to prevent path traversal attacks, restrict deletions to intended directories (e.g., '/tmp/'), verify resolved file paths before deletion, and add try-except around os.remove to handle missing or locked files gracefully. | Code Analyzer, Security |\n",
      "| Inconsistent Import Placement | file_handler.py | [10] | 3 | Style | Move all import statements, such as os, to the top of the file for clarity, conformity to PEP8, and to avoid runtime import issues. | Best Practices |\n",
      "| Hardcoded File Paths | file_handler.py | [4,10,13] | 5 | Style | Make base directories configurable via arguments, environment variables, or configuration files instead of hardcoding them to improve flexibility across different environments. | Best Practices |\n",
      "| Missing Error Handling on File Operations | file_handler.py | [5,11,13] | 7 | Bug | Add try-except blocks around file operations in read_user_file and delete_file to handle and log exceptions (e.g., IO errors, FileNotFoundError) gracefully, preventing program crashes. | Best Practices |\n",
      "| Test Gap: read_user_file | file_handler.py | [3,4,5,6] | 7 | Test Gap | Implement unit tests with mocked open and file contents to cover reading existing file with normal content, reading empty file, file does not exist, file path traversal or invalid filename, permissions denied for file, and very large file scenarios. | Test Coverage |\n",
      "| Test Gap: delete_file | file_handler.py | [7,8,9] | 8 | Test Gap | Implement unit tests with mocking os.remove and exception scenarios to cover deleting existing file, deleting non-existent file (FileNotFoundError), permission denied to delete file, deleting file with special characters in name, and handling unexpected exceptions from os.remove. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 14**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 15\n",
      "total_actual: 14\n",
      "matches: 11\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Hardcoded API key in api_handler.py\n",
      "  Matched: True\n",
      "  Actual: Hardcoded Sensitive Keys/Credentials | api_handler.py...\n",
      "\n",
      "  Expected: Hardcoded secret password in api_handler.py\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Command injection vulnerability in execute_command via os.system with unsanitized user input\n",
      "  Matched: True\n",
      "  Actual: Command Injection Vulnerability via os.system | api_handler.py...\n",
      "\n",
      "  Expected: Insecure deserialization using pickle.load in load_data which can execute arbitrary code\n",
      "  Matched: True\n",
      "  Actual: Insecure Deserialization with Pickle Load | api_handler.py...\n",
      "\n",
      "  Expected: Path traversal vulnerability in read_user_file allowing access to arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in read_user_file | file_handler.py...\n",
      "\n",
      "  Expected: Path traversal vulnerability in delete_file allowing deletion of arbitrary files\n",
      "  Matched: True\n",
      "  Actual: Path Traversal Vulnerability in delete_file | file_handler.py...\n",
      "\n",
      "  Expected: No input validation in execute_command\n",
      "  Matched: False\n",
      "\n",
      "  Expected: No path sanitization in read_user_file and delete_file\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing error handling in all methods\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing docstrings for both classes and all methods\n",
      "  Matched: True\n",
      "  Actual: Missing Docstrings in Classes and Methods | api_handler.py...\n",
      "\n",
      "  Expected: Missing error handling in delete_file for file not found or permission errors\n",
      "  Matched: True\n",
      "  Actual: Missing Error Handling on File Operations | file_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for execute_command including command injection attempts\n",
      "  Matched: True\n",
      "  Actual: Test Gap: execute_command | api_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for load_data including malicious pickle payloads\n",
      "  Matched: True\n",
      "  Actual: Test Gap: load_data | api_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for read_user_file including path traversal attempts, file not found, permission errors\n",
      "  Matched: True\n",
      "  Actual: Test Gap: read_user_file | file_handler.py...\n",
      "\n",
      "  Expected: Missing test coverage for delete_file including path traversal attempts, file not found, permission errors\n",
      "  Matched: True\n",
      "  Actual: Test Gap: delete_file | file_handler.py...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.73\n",
      "Precision: 0.79\n",
      "F1 Score: 0.76\n",
      "Status: ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "TESTING: 05_multi_file_mixed\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CALLING AGGREGATOR...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AGGREGATOR OUTPUT:\n",
      "============================================================\n",
      "# Code Review Report\n",
      "\n",
      "## Executive Summary\n",
      "The code exhibits critical security vulnerabilities, particularly in password hashing using the insecure MD5 algorithm in user_manager.py and session token generation with non-cryptographic random functions in session_manager.py, which could lead to brute-force attacks or session hijacking. Additional bugs include undefined method calls and ineffective email validation in user_manager.py, alongside style issues like missing docstrings and inefficient practices across both files. Test coverage is insufficient, with multiple functions lacking scenarios for edge cases, errors, and integration testing, requiring comprehensive unit and integration tests to ensure robustness.\n",
      "\n",
      "## Summary of Actions\n",
      "| Issue | File | Lines | Severity | Category | Fix | Found By |\n",
      "|-------|------|-------|----------|----------|-----|----------|\n",
      "| Use of MD5 for password hashing | user_manager.py | 3,5,6,7 | 9 | Security | Replace hashlib.md5 with a secure password hashing library such as bcrypt or passlib. Use a stronger password hashing algorithm designed for security such as bcrypt, Argon2, or PBKDF2 with salt to securely hash passwords before storing them. Avoid using MD5 for password hashing. | Code Analyzer, Security, Best Practices |\n",
      "| Undefined method fetch_user called in get_users | user_manager.py | 11,12,14 | 8 | Bug | Implement the fetch_user method within UserManager class or replace the call with appropriate user retrieval logic. Define the 'fetch_user' method or import it appropriately, or replace the call with correct logic to retrieve users. | Code Analyzer, Best Practices |\n",
      "| validate_email method always returns True without validation | user_manager.py | 15,17,18,19,20,21 | 6 | Bug | Implement proper email validation using regex or a validation library to ensure email addresses are valid. Implement proper email validation using regular expressions or specialized libraries like email-validator to ensure that email addresses are syntactically correct and safe. | Code Analyzer, Security, Best Practices |\n",
      "| Unclear variable name 'hash' shadowing built-in | user_manager.py | 5 | 3 | Style | Rename the variable 'hash' to 'password_hash' or 'hashed_password' to improve clarity. | Best Practices |\n",
      "| Missing docstrings in UserManager methods | user_manager.py | 4,9,14 | 4 | Style | Add docstrings to all methods explaining their functionality, parameters, and return types. | Best Practices |\n",
      "| Test coverage gap for create_user | user_manager.py | 3,10 | 7 | Test Gap | Unit test for: normal case with valid username, password, and email; edge case with empty username, password, or email; edge case with very long strings for username, password, and email; test password hashing correctness; error handling when password is None or not a string. | Test Coverage |\n",
      "| Test coverage gap for get_users | user_manager.py | 11,17 | 6 | Test Gap | Unit and integration tests for: normal case with count > 0; edge case with count = 0; edge case with count as negative number; error handling when fetch_user raises exception; integration test to verify fetch_user interaction. | Test Coverage |\n",
      "| Test coverage gap for validate_email | user_manager.py | 18,20 | 7 | Test Gap | Unit and integration tests for: normal case with valid emails; edge case with invalid email formats (missing '@', missing domain); edge case with empty string; error handling when input is None or non-string type; integration with create_user to ensure only valid emails accepted. | Test Coverage |\n",
      "| Insecure session token generation using random.randint | session_manager.py | 4,5,6,7,8,9 | 9 | Security | Use the secrets module (e.g., secrets.token_hex or secrets.token_urlsafe) for generating cryptographically secure tokens. Use a cryptographically secure random token generator, such as secrets.token_hex or os.urandom, to generate session tokens with sufficient entropy and unpredictability. | Code Analyzer, Security |\n",
      "| Magic number used for token length in generate_token | session_manager.py | 5 | 3 | Style | Replace the magic number 16 with a named constant like TOKEN_LENGTH to improve readability and maintainability. | Best Practices |\n",
      "| Inefficient token generation in generate_token | session_manager.py | 5,6 | 4 | Performance | Use a list comprehension with ''.join to generate the token string efficiently. | Best Practices |\n",
      "| Missing docstrings in SessionManager methods | session_manager.py | 4,9 | 4 | Style | Add docstrings for both 'generate_token' and 'validate_session' methods. | Best Practices |\n",
      "| Unnecessary conditional in validate_session method | session_manager.py | 9,10,11 | 2 | Style | Simplify the method to 'return bool(token)'. | Best Practices |\n",
      "| Test coverage gap for generate_token | session_manager.py | 3,12 | 6 | Test Gap | Unit test for: normal case generating token of length 16; edge case to verify randomness (multiple calls produce different tokens); edge case validating token only contains digits; performance under multiple rapid calls. | Test Coverage |\n",
      "| Test coverage gap for validate_session | session_manager.py | 13,15 | 7 | Test Gap | Unit test for: normal case with valid non-empty token; edge case with empty token string; edge case with None as token; edge case with non-string token input. | Test Coverage |\n",
      "\n",
      "**Total Distinct Issues: 15**\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "JUDGE OUTPUT:\n",
      "============================================================\n",
      "total_expected: 16\n",
      "total_actual: 15\n",
      "matches: 10\n",
      "\n",
      "matched_findings:\n",
      "\n",
      "  Expected: Weak cryptographic hash algorithm (MD5) used for password hashing in create_user\n",
      "  Matched: True\n",
      "  Actual: Use of MD5 for password hashing...\n",
      "\n",
      "  Expected: Cryptographically insecure random number generator (random.randint) used for token generation\n",
      "  Matched: True\n",
      "  Actual: Insecure session token generation using random.randint...\n",
      "\n",
      "  Expected: Weak session validation in validate_session - only checks if token exists, not authenticity or expiry\n",
      "  Matched: False\n",
      "\n",
      "  Expected: validate_email always returns True without any actual email validation logic\n",
      "  Matched: True\n",
      "  Actual: validate_email method always returns True without validation...\n",
      "\n",
      "  Expected: get_users calls undefined method fetch_user which will cause AttributeError at runtime\n",
      "  Matched: True\n",
      "  Actual: Undefined method fetch_user called in get_users...\n",
      "\n",
      "  Expected: Missing salt when hashing passwords in create_user\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Token generation only uses digits (0-9) resulting in weak token space\n",
      "  Matched: False\n",
      "\n",
      "  Expected: validate_session can be simplified to return bool(token)\n",
      "  Matched: True\n",
      "  Actual: Unnecessary conditional in validate_session method...\n",
      "\n",
      "  Expected: Missing docstrings for both classes and all methods\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing error handling in get_users for when fetch_user fails\n",
      "  Matched: False\n",
      "\n",
      "  Expected: No validation that count parameter in get_users is positive\n",
      "  Matched: False\n",
      "\n",
      "  Expected: Missing test coverage for create_user including password hashing verification, edge cases for inputs\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for create_user...\n",
      "\n",
      "  Expected: Missing test coverage for generate_token including uniqueness, randomness quality, token strength\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for generate_token...\n",
      "\n",
      "  Expected: Missing test coverage for validate_email including valid/invalid email formats, edge cases\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for validate_email...\n",
      "\n",
      "  Expected: Missing test coverage for get_users including undefined fetch_user error, negative count, empty results\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for get_users...\n",
      "\n",
      "  Expected: Missing test coverage for validate_session including empty strings, None, expired tokens, forged tokens\n",
      "  Matched: True\n",
      "  Actual: Test coverage gap for validate_session...\n",
      "\n",
      "============================================================\n",
      "CALCULATED METRICS:\n",
      "============================================================\n",
      "Recall: 0.62\n",
      "Precision: 0.67\n",
      "F1 Score: 0.65\n",
      "Status: ✗ FAILED\n",
      "\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "✓ 01_sql_injection: R=0.80 P=1.00 F1=0.89\n",
      "✓ 02_logic_bug: R=0.83 P=1.00 F1=0.91\n",
      "✗ 03_code_quality: R=0.78 P=1.17 F1=0.93\n",
      "✗ 04_multi_file_security: R=0.73 P=0.79 F1=0.76\n",
      "✗ 05_multi_file_mixed: R=0.62 P=0.67 F1=0.65\n",
      "\n",
      "Passed: 2/5\n"
     ]
    }
   ],
   "source": [
    "# test all test cases\n",
    "\n",
    "test_cases = [\n",
    "    \"01_sql_injection\",\n",
    "    \"02_logic_bug\",\n",
    "    \"03_code_quality\",\n",
    "    \"04_multi_file_security\",\n",
    "    \"05_multi_file_mixed\"\n",
    "]\n",
    "\n",
    "async def run_all_tests():\n",
    "    test_dir = Path(\"test-cases\")\n",
    "    results = []\n",
    "    \n",
    "    for test_name in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING: {test_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load files\n",
    "        diff_file = test_dir / f\"{test_name}.diff\"\n",
    "        diff_content = diff_file.read_text()\n",
    "        expected_file = test_dir / f\"{test_name}_expected.json\"\n",
    "        ground_truth_content = expected_file.read_text()\n",
    "        \n",
    "        # Run review\n",
    "        report = await review_code(diff_content, save_output=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = await evaluate_report(report, ground_truth_content)\n",
    "        \n",
    "        # Print detailed judge output\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"JUDGE OUTPUT:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"total_expected: {eval_result['total_expected']}\")\n",
    "        print(f\"total_actual: {eval_result['total_actual']}\")\n",
    "        print(f\"matches: {eval_result['matches']}\")\n",
    "        print(f\"\\nmatched_findings:\")\n",
    "        for mf in eval_result['details']:\n",
    "            print(f\"\\n  Expected: {mf.expected}\")\n",
    "            print(f\"  Matched: {mf.matched}\")\n",
    "            if mf.actual_finding:\n",
    "                print(f\"  Actual: {mf.actual_finding[:100]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'recall': eval_result['recall'],\n",
    "            'precision': eval_result['precision'],\n",
    "            'f1': eval_result['f1'],\n",
    "            'passed': eval_result['recall'] >= 0.80 and \n",
    "                     eval_result['precision'] >= 0.85 and \n",
    "                     eval_result['f1'] >= 0.82\n",
    "        })\n",
    "        \n",
    "        # Print calculated metrics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATED METRICS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Recall: {eval_result['recall']:.2f}\")\n",
    "        print(f\"Precision: {eval_result['precision']:.2f}\")\n",
    "        print(f\"F1 Score: {eval_result['f1']:.2f}\")\n",
    "        print(f\"Status: {'✓ PASSED' if results[-1]['passed'] else '✗ FAILED'}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print('='*60)\n",
    "    for result in results:\n",
    "        status = '✓' if result['passed'] else '✗'\n",
    "        print(f\"{status} {result['test_name']}: R={result['recall']:.2f} P={result['precision']:.2f} F1={result['f1']:.2f}\")\n",
    "    \n",
    "    passed = sum(1 for r in results if r['passed'])\n",
    "    print(f\"\\nPassed: {passed}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run all tests\n",
    "results = await run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
